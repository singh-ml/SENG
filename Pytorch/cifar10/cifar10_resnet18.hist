Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '65', '--arch', 'resnet18', '--lr-decay-epoch', '70', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/singh/data/', '--lr', '0.05', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /data/singh/data/cifar-10-python.tar.gz
Extracting /data/singh/data/cifar-10-python.tar.gz to /data/singh/data/
Files already downloaded and verified
Memory peak: 7648076288 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.1807     58.320  1.4454    46.924  27.98
   2   0.8066     71.810  0.9345    66.786  50.21
   3   0.7438     74.540  0.7122    75.072  72.37
   4   0.6717     78.090  0.5976    79.034  94.56
   5   0.5556     81.140  0.5317    81.512  116.82
   6   0.5343     81.700  0.4670    83.896  139.00
   7   0.4435     84.980  0.4278    84.954  161.24
   8   0.4577     84.450  0.3954    86.302  183.42
   9   0.4380     85.370  0.3647    87.264  205.67
  10   0.4311     85.090  0.3327    88.394  227.82
  11   0.4249     85.820  0.3142    89.040  249.98
  12   0.4515     85.760  0.2938    89.836  272.15
  13   0.4060     87.140  0.2741    90.550  294.36
  14   0.3979     86.750  0.2634    90.790  316.51
  15   0.4165     85.980  0.2451    91.384  338.74
  16   0.4240     86.020  0.2320    91.918  360.97
  17   0.3601     88.610  0.2172    92.514  383.10
  18   0.3853     88.290  0.2076    92.732  405.35
  19   0.3800     88.600  0.1960    93.102  427.54
  20   0.3988     87.320  0.1863    93.584  449.71
  21   0.3147     89.390  0.1789    93.782  471.90
  22   0.3650     88.670  0.1684    94.138  494.14
  23   0.3417     89.050  0.1580    94.498  516.28
  24   0.3841     88.510  0.1545    94.602  538.50
  25   0.3362     89.650  0.1413    95.156  560.72
  26   0.3479     89.220  0.1374    95.178  582.94
  27   0.3250     89.840  0.1307    95.476  605.22
  28   0.3187     90.040  0.1224    95.756  627.47
  29   0.3384     89.910  0.1155    95.992  649.67
  30   0.3500     89.750  0.1095    96.190  671.80
  31   0.3256     90.130  0.1013    96.482  693.99
  32   0.3980     88.870  0.0943    96.696  716.27
  33   0.3817     89.280  0.0948    96.732  738.46
  34   0.3429     90.280  0.0868    96.964  760.66
  35   0.3045     91.130  0.0753    97.464  782.83
  36   0.2953     91.640  0.0728    97.544  805.07
  37   0.3261     90.800  0.0676    97.690  827.32
  38   0.3092     91.660  0.0632    97.898  849.53
  39   0.3452     90.560  0.0543    98.194  871.69
  40   0.3538     90.550  0.0508    98.286  893.85
  41   0.2965     91.980  0.0438    98.540  916.05
  42   0.3408     91.240  0.0417    98.590  938.26
  43   0.3043     91.850  0.0375    98.798  960.44
  44   0.3090     92.160  0.0327    98.982  982.67
  45   0.2898     92.940  0.0218    99.326  1004.85
  46   0.3257     92.280  0.0219    99.322  1027.05
  47   0.2964     92.700  0.0208    99.350  1049.23
  48   0.2907     92.520  0.0143    99.596  1071.47
  49   0.2797     93.160  0.0124    99.672  1093.69
  50   0.2724     93.430  0.0095    99.760  1115.68
  51   0.2723     93.350  0.0076    99.824  1137.86
  52   0.2852     93.300  0.0062    99.894  1160.07
  53   0.2685     93.580  0.0055    99.884  1182.28
  54   0.2609     93.610  0.0041    99.946  1204.49
  55   0.2650     93.840  0.0037    99.946  1226.71
  56   0.2598     93.690  0.0041    99.930  1248.86
  57   0.2582     93.730  0.0031    99.960  1271.15
  58   0.2590     93.820  0.0029    99.966  1293.38
  59   0.2593     93.880  0.0026    99.974  1315.58
  60   0.2548     93.940  0.0025    99.976  1337.80
  61   0.2542     93.980  0.0021    99.990  1359.96
  62   0.2510     94.060  0.0020    99.988  1382.17
  63   0.2515     93.960  0.0022    99.978  1404.46
  64   0.2494     94.050  0.0021    99.978  1426.72
  65   0.2503     93.980  0.0020    99.988  1448.94
