Use GPU: 0 for training
==> Running with ['main_kfac.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 22156040192 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.8638     12.140  4.1955     6.864  27.81
   2   3.3766     19.760  3.5909    15.424  53.16
   3   3.1721     25.300  3.1933    22.566  78.52
   4   2.8736     30.200  2.8935    27.840  103.90
   5   2.6316     34.550  2.6447    33.010  129.28
   6   2.5255     37.520  2.4269    37.122  154.67
   7   2.4097     40.050  2.2239    41.700  180.00
   8   2.3219     41.810  2.0446    45.390  205.37
   9   2.2320     43.890  1.8671    49.292  230.71
  10   2.2407     45.270  1.7274    52.222  256.07
  11   2.1876     47.060  1.5801    55.980  281.49
  12   2.1681     47.730  1.4315    59.734  306.85
  13   2.2212     47.890  1.3086    62.666  332.19
  14   2.1738     48.860  1.1863    65.916  357.54
  15   2.2184     49.490  1.0650    68.756  382.89
  16   2.2079     50.940  0.9460    71.994  408.21
  17   2.2866     50.750  0.8387    74.872  433.57
  18   2.3650     50.020  0.7508    77.554  458.93
  19   2.4048     50.780  0.6691    79.400  484.29
  20   2.4165     52.390  0.5842    81.980  509.64
  21   2.5211     51.890  0.5048    84.286  535.03
  22   2.5383     52.170  0.4370    86.356  560.35
  23   2.5292     52.770  0.3909    87.924  585.71
  24   2.6290     52.370  0.3432    89.320  611.08
  25   2.6147     52.110  0.2946    90.866  636.45
  26   2.6838     52.590  0.2592    91.916  661.80
  27   2.7664     52.120  0.2261    92.976  687.16
  28   2.6765     52.480  0.1971    94.044  712.53
  29   2.7637     52.540  0.1705    94.982  737.93
  30   2.6479     53.630  0.1493    95.638  763.29
  31   2.7748     53.230  0.1288    96.290  788.67
  32   2.7381     53.760  0.1132    96.780  814.06
  33   2.7721     54.640  0.0992    97.324  839.41
  34   2.7315     54.370  0.0855    97.672  864.75
  35   2.7731     54.530  0.0758    97.982  890.12
  36   2.8234     54.320  0.0648    98.416  915.48
  37   2.7668     55.050  0.0566    98.682  940.86
  38   2.8287     54.620  0.0532    98.700  966.22
  39   2.8157     54.180  0.0478    98.892  991.60
  40   2.8608     54.120  0.0409    99.132  1016.96
  41   2.8606     54.040  0.0362    99.262  1042.32
  42   2.7628     54.440  0.0336    99.298  1067.66
  43   2.7938     54.610  0.0307    99.402  1093.03
  44   2.7094     55.490  0.0279    99.476  1118.40
  45   2.7576     55.210  0.0252    99.558  1143.75
  46   2.7150     55.850  0.0226    99.634  1169.12
  47   2.6745     56.340  0.0201    99.670  1194.49
  48   2.7419     55.390  0.0185    99.714  1219.85
  49   2.7445     55.360  0.0176    99.730  1245.23
  50   2.7375     55.710  0.0149    99.810  1269.78
  51   2.6601     56.360  0.0140    99.806  1295.16
  52   2.6985     55.350  0.0149    99.816  1320.50
  53   2.6978     55.920  0.0135    99.836  1345.87
