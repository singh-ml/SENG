Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 19427030016 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.8006      0.780  40.6675     0.840  27.84
   2   4.8104      0.850  40.6662     0.818  53.32
   3   4.7994      0.880  40.6656     0.852  78.83
   4   4.8344      0.870  40.6669     0.804  104.23
   5   4.8339      0.820  40.6662     0.798  129.62
   6   4.8108      0.960  40.6671     0.838  154.95
   7   4.8388      0.820  40.6663     0.770  180.37
   8   4.8509      0.770  40.6651     0.824  205.77
   9   4.8056      0.850  40.6666     0.844  231.06
  10   4.8393      0.820  40.6670     0.794  256.48
  11   4.7743      0.910  40.6671     0.868  281.80
  12   4.8316      0.840  40.6663     0.818  307.08
  13   4.8246      0.890  40.6679     0.802  332.43
  14   4.7527      0.880  40.6674     0.802  357.74
  15   4.7735      0.860  40.6673     0.824  383.04
  16   4.7935      0.810  40.6656     0.836  408.27
  17   4.7868      0.900  40.6664     0.884  433.61
  18   4.8378      0.860  40.6671     0.806  458.97
  19   4.8024      0.790  40.6653     0.840  484.39
  20   4.7787      0.790  40.6670     0.750  509.82
  21   4.7945      0.920  40.6660     0.862  535.23
  22   4.8549      0.770  40.6676     0.804  560.67
  23   4.8112      0.790  40.6673     0.840  586.08
  24   4.8789      0.880  40.6655     0.820  611.49
  25   4.8229      0.780  40.6668     0.762  636.96
  26   4.8342      0.780  40.6672     0.786  662.42
  27   4.7754      0.810  40.6679     0.834  687.83
  28   4.8272      0.930  40.6667     0.826  713.12
  29   4.7840      0.820  40.6653     0.810  738.54
  30   4.8169      0.830  40.6672     0.794  763.86
  31   4.8138      0.870  40.6667     0.802  789.30
  32   4.8185      0.810  40.6667     0.800  814.64
  33   4.8411      0.720  40.6666     0.766  840.09
  34   4.8755      0.860  40.6671     0.798  865.50
  35   4.8309      0.840  40.6657     0.852  890.90
  36   4.8483      0.830  40.6667     0.824  916.40
  37   4.7733      0.860  40.6667     0.816  941.85
  38   4.8770      0.910  40.6663     0.804  967.28
  39   4.7861      0.920  40.6670     0.850  992.72
  40   4.8313      0.910  40.6672     0.762  1018.00
  41   4.8260      0.890  40.6657     0.904  1043.38
  42   4.7754      0.850  40.6665     0.828  1068.77
  43   4.8419      0.680  40.6657     0.858  1094.04
  44   4.8068      0.870  40.6663     0.816  1119.42
  45   4.8244      0.840  40.6692     0.750  1144.70
  46   4.7936      0.840  40.6668     0.798  1170.09
  47   4.8005      0.830  40.6664     0.772  1195.49
  48   4.7887      0.770  40.6664     0.814  1220.92
  49   4.8365      0.900  40.6669     0.870  1246.39
  50   4.7756      0.830  40.6663     0.868  1271.85
  51   4.8355      0.860  40.6656     0.826  1297.24
  52   4.7976      0.840  40.6659     0.804  1322.68
  53   4.8315      0.660  40.6669     0.858  1348.12
  54   4.9028      0.890  40.6658     0.796  1373.58
  55   4.8986      0.840  40.6663     0.828  1398.95
  56   4.8123      0.830  40.6674     0.810  1424.39
  57   4.8732      0.860  40.6669     0.830  1449.85
  58   4.7616      0.850  40.6670     0.854  1475.27
  59   4.8393      0.860  40.6668     0.838  1500.69
  60   4.8501      0.780  40.6661     0.796  1526.09
  61   4.8158      0.870  40.6669     0.840  1551.46
  62   4.7467      0.870  40.6657     0.838  1576.82
  63   4.8281      0.900  40.6674     0.812  1602.27
  64   4.7914      0.840  40.6652     0.796  1627.73
  65   4.7774      0.880  40.6684     0.802  1653.04
  66   4.8543      0.760  40.6647     0.772  1678.42
  67   4.8441      0.880  40.6661     0.818  1703.84
  68   4.8313      0.900  40.6674     0.822  1729.30
  69   4.8234      0.930  40.6674     0.814  1754.72
  70   4.8313      0.910  40.6684     0.854  1780.07
  71   4.8023      0.850  40.6653     0.780  1805.52
  72   4.7950      0.860  40.6663     0.820  1830.95
  73   4.8252      0.850  40.6664     0.778  1856.43
  74   4.8140      0.800  40.6669     0.856  1881.86
  75   4.8267      0.750  40.6657     0.808  1907.30
  76   4.8276      0.900  40.6658     0.800  1932.67
  77   4.8536      0.770  40.6667     0.844  1958.13
  78   4.7917      0.930  40.6672     0.834  1983.45
  79   4.8863      0.830  40.6675     0.768  2008.88
  80   4.8560      0.840  40.6663     0.804  2034.34
  81   4.8738      0.840  40.6659     0.856  2059.56
  82   4.8225      0.800  40.6667     0.804  2085.01
  83   4.8528      0.840  40.6660     0.828  2110.47
  84   4.8209      0.870  40.6663     0.816  2135.80
  85   4.8514      0.850  40.6666     0.790  2161.16
  86   4.8341      0.890  40.6673     0.840  2186.60
  87   4.8072      0.880  40.6662     0.790  2212.03
  88   4.8322      0.850  40.6683     0.768  2237.37
  89   4.8705      0.790  40.6680     0.786  2262.81
  90   4.8326      0.800  40.6677     0.764  2288.07
