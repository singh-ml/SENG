Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 20941931520 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7254      0.770  11.9609     0.892  28.75
   2   4.7311      0.750  11.9616     0.820  55.37
   3   4.7255      0.740  11.9613     0.830  81.97
   4   4.7287      0.770  11.9626     0.740  108.66
   5   4.7376      0.670  11.9620     0.808  135.43
   6   4.7391      0.590  11.9629     0.790  162.14
   7   4.7284      0.690  11.9604     0.756  188.90
   8   4.7293      0.600  11.9606     0.774  215.61
   9   4.7359      0.620  11.9625     0.800  242.45
  10   4.7347      0.640  11.9631     0.742  269.23
  11   4.7262      0.750  11.9618     0.798  296.10
  12   4.7285      0.760  11.9610     0.824  322.96
  13   4.7312      0.670  11.9607     0.804  349.88
  14   4.7219      0.780  11.9609     0.784  376.79
  15   4.7255      0.790  11.9635     0.784  403.68
  16   4.7238      0.710  11.9599     0.830  430.63
  17   4.7241      0.670  11.9616     0.770  457.48
  18   4.7291      0.820  11.9618     0.812  484.35
  19   4.7321      0.780  11.9605     0.768  511.26
  20   4.7297      0.770  11.9622     0.800  538.19
  21   4.7367      0.780  11.9609     0.798  565.00
  22   4.7253      0.690  11.9624     0.800  591.78
  23   4.7382      0.660  11.9641     0.766  618.68
  24   4.7268      0.770  11.9624     0.822  645.59
  25   4.7339      0.710  11.9617     0.798  672.41
  26   4.7298      0.660  11.9616     0.848  699.32
  27   4.7332      0.700  11.9621     0.836  726.15
  28   4.7250      0.690  11.9618     0.796  753.04
  29   4.7249      0.740  11.9618     0.712  779.82
  30   4.7261      0.750  11.9615     0.772  806.62
  31   4.7308      0.740  11.9612     0.792  833.52
  32   4.7283      0.740  11.9616     0.840  860.38
  33   4.7385      0.690  11.9604     0.816  887.21
  34   4.7394      0.660  11.9610     0.814  914.06
  35   4.7313      0.770  11.9615     0.834  940.96
  36   4.7321      0.840  11.9610     0.746  967.86
  37   4.7266      0.690  11.9620     0.798  994.75
  38   4.7321      0.750  11.9619     0.850  1021.68
  39   4.7299      0.650  11.9609     0.798  1048.43
  40   4.7325      0.690  11.9610     0.782  1075.31
  41   4.7380      0.730  11.9617     0.768  1102.08
  42   4.7261      0.750  11.9619     0.804  1129.02
  43   4.7216      0.630  11.9620     0.822  1155.91
  44   4.7310      0.830  11.9620     0.756  1182.80
  45   4.7341      0.730  11.9637     0.776  1209.67
  46   4.7312      0.710  11.9619     0.834  1236.50
  47   4.7320      0.680  11.9617     0.822  1263.34
  48   4.7347      0.890  11.9630     0.770  1290.16
  49   4.7284      0.680  11.9631     0.822  1317.07
  50   4.7291      0.690  11.9608     0.824  1343.91
  51   4.7374      0.640  11.9617     0.788  1370.69
  52   4.7312      0.640  11.9619     0.714  1397.47
  53   4.7306      0.670  11.9619     0.790  1424.40
  54   4.7359      0.710  11.9623     0.784  1451.32
  55   4.7217      0.750  11.9607     0.784  1478.18
  56   4.7343      0.710  11.9629     0.830  1505.11
  57   4.7380      0.810  11.9614     0.804  1532.04
  58   4.7319      0.690  11.9624     0.776  1558.96
  59   4.7284      0.760  11.9616     0.738  1585.77
  60   4.7317      0.700  11.9623     0.780  1612.61
  61   4.7282      0.650  11.9610     0.782  1639.42
  62   4.7350      0.720  11.9612     0.766  1666.35
  63   4.7364      0.700  11.9613     0.796  1693.28
  64   4.7237      0.780  11.9618     0.774  1720.13
  65   4.7277      0.710  11.9617     0.860  1746.99
  66   4.7286      0.730  11.9609     0.856  1773.90
  67   4.7378      0.740  11.9627     0.786  1800.67
  68   4.7302      0.710  11.9603     0.728  1827.59
  69   4.7275      0.710  11.9608     0.820  1854.31
  70   4.7379      0.800  11.9617     0.812  1881.14
  71   4.7239      0.810  11.9617     0.750  1907.96
  72   4.7299      0.730  11.9604     0.806  1934.89
  73   4.7277      0.720  11.9619     0.732  1961.63
  74   4.7379      0.790  11.9606     0.738  1988.49
  75   4.7299      0.720  11.9624     0.794  2015.41
  76   4.7322      0.730  11.9621     0.852  2042.30
  77   4.7278      0.740  11.9619     0.774  2069.21
  78   4.7304      0.720  11.9611     0.744  2096.13
  79   4.7305      0.610  11.9611     0.780  2122.85
  80   4.7253      0.700  11.9616     0.806  2149.65
  81   4.7372      0.700  11.9605     0.842  2176.57
  82   4.7292      0.770  11.9613     0.742  2203.47
  83   4.7226      0.680  11.9629     0.818  2230.25
  84   4.7297      0.640  11.9621     0.834  2257.03
  85   4.7236      0.770  11.9612     0.790  2283.94
  86   4.7281      0.630  11.9618     0.788  2310.82
  87   4.7400      0.720  11.9639     0.778  2337.72
  88   4.7288      0.680  11.9631     0.826  2364.53
  89   4.7249      0.750  11.9616     0.724  2391.45
  90   4.7311      0.650  11.9611     0.758  2418.37
