Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 21890337792 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7729      0.960  11.9616     0.970  30.06
   2   4.7755      1.060  11.9619     0.924  57.76
   3   4.7671      1.000  11.9637     0.898  85.43
   4   4.7595      1.110  11.9637     0.974  113.09
   5   4.7831      0.850  11.9607     0.898  140.55
   6   4.7598      0.940  11.9620     0.926  168.16
   7   4.7561      1.030  11.9625     0.952  195.84
   8   4.7760      0.950  11.9633     0.938  223.48
   9   4.7565      1.010  11.9610     0.946  251.09
  10   4.7709      0.960  11.9630     0.940  278.72
  11   4.7596      1.010  11.9624     0.918  306.35
  12   4.7593      1.040  11.9615     0.958  333.87
  13   4.7737      0.950  11.9625     0.910  361.43
  14   4.7606      1.030  11.9627     0.934  388.92
  15   4.7756      0.880  11.9616     0.928  416.42
  16   4.7884      0.930  11.9623     0.946  444.00
  17   4.7566      1.010  11.9625     0.932  471.57
  18   4.7598      1.120  11.9616     1.030  499.28
  19   4.7710      1.050  11.9637     0.920  526.93
  20   4.7621      0.900  11.9610     0.964  554.58
  21   4.7482      1.090  11.9609     0.964  582.16
  22   4.7769      0.900  11.9609     0.960  609.79
  23   4.7774      0.980  11.9621     0.896  637.45
  24   4.7706      0.980  11.9635     0.936  665.11
  25   4.7746      1.020  11.9616     0.904  692.78
  26   4.7695      1.030  11.9631     0.884  720.25
  27   4.7617      0.890  11.9623     0.924  747.87
  28   4.7729      0.970  11.9630     0.956  775.41
  29   4.7713      0.900  11.9618     0.974  803.04
  30   4.7623      0.920  11.9630     0.968  830.49
  31   4.7676      0.980  11.9621     0.960  858.13
  32   4.7651      0.950  11.9628     0.962  885.72
  33   4.7532      0.890  11.9622     0.958  913.34
  34   4.7697      0.890  11.9613     0.934  941.02
  35   4.7758      0.930  11.9623     0.934  968.68
  36   4.7612      0.930  11.9619     0.984  996.34
  37   4.7604      1.030  11.9621     0.930  1023.92
  38   4.7675      1.060  11.9625     0.936  1051.54
  39   4.7734      0.880  11.9616     0.886  1079.14
  40   4.7569      1.070  11.9634     0.940  1106.78
  41   4.7717      0.980  11.9615     0.946  1134.41
  42   4.7721      0.940  11.9623     0.968  1162.01
  43   4.7757      0.910  11.9614     0.886  1189.62
  44   4.7618      1.040  11.9608     0.984  1217.30
  45   4.7603      0.930  11.9623     0.944  1244.96
  46   4.7544      1.000  11.9624     0.950  1272.62
  47   4.7660      0.940  11.9628     0.942  1300.21
  48   4.7700      1.100  11.9622     1.032  1327.86
  49   4.7700      0.960  11.9622     0.932  1355.41
  50   4.7552      1.070  11.9619     0.974  1382.90
  51   4.7827      1.000  11.9631     0.932  1410.58
  52   4.7512      1.050  11.9637     0.910  1438.23
  53   4.7737      1.010  11.9621     0.918  1465.80
  54   4.7671      1.010  11.9623     0.956  1493.48
  55   4.7758      0.890  11.9622     1.002  1521.01
  56   4.7723      0.980  11.9614     0.946  1548.69
  57   4.7577      1.020  11.9604     0.902  1576.36
  58   4.7594      0.980  11.9622     0.968  1603.98
  59   4.7748      0.860  11.9610     0.916  1631.49
  60   4.7547      0.980  11.9629     0.942  1659.15
  61   4.7784      1.030  11.9615     0.950  1686.76
  62   4.7655      0.900  11.9623     1.028  1714.30
  63   4.7753      0.950  11.9636     0.972  1741.86
  64   4.7564      0.990  11.9636     0.964  1769.37
  65   4.7602      1.050  11.9623     0.936  1796.94
  66   4.7547      0.970  11.9636     0.890  1824.59
  67   4.7827      1.000  11.9620     0.968  1852.21
  68   4.7623      0.980  11.9639     0.980  1879.74
  69   4.7511      1.000  11.9628     0.986  1907.36
  70   4.7511      1.040  11.9624     0.960  1934.86
  71   4.7774      0.900  11.9626     0.966  1962.53
  72   4.7615      0.960  11.9614     0.916  1990.16
  73   4.7801      0.940  11.9638     0.946  2017.77
  74   4.7639      0.880  11.9620     0.972  2045.25
  75   4.7542      0.920  11.9631     0.930  2072.79
  76   4.7558      1.010  11.9627     0.920  2100.40
  77   4.7356      0.970  11.9621     0.980  2127.85
  78   4.7529      1.020  11.9616     0.954  2155.34
  79   4.7762      0.960  11.9629     0.976  2182.89
  80   4.7777      0.980  11.9624     0.908  2210.57
  81   4.7712      0.950  11.9629     0.910  2238.20
  82   4.7669      0.930  11.9615     0.896  2265.88
  83   4.7481      0.940  11.9607     0.920  2293.50
  84   4.7554      1.050  11.9617     0.966  2321.16
  85   4.7906      1.000  11.9616     0.944  2348.86
  86   4.7692      0.960  11.9628     0.952  2376.61
  87   4.7574      0.990  11.9612     0.936  2404.20
  88   4.7571      1.000  11.9622     0.988  2431.81
  89   4.7645      1.000  11.9635     0.948  2459.50
  90   4.7771      1.030  11.9609     0.888  2487.14
