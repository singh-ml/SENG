Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 19048453120 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7606      1.010  11.9741     1.042  27.79
   2   4.7839      1.030  11.9762     0.982  53.05
   3   4.7955      0.990  11.9759     1.004  78.19
   4   4.8055      1.090  11.9761     1.018  103.44
   5   4.8408      1.070  11.9766     0.984  128.56
   6   4.7730      1.000  11.9754     1.058  153.82
   7   4.7937      1.020  11.9745     1.004  178.99
   8   4.7985      1.030  11.9754     1.038  204.16
   9   4.7481      1.060  11.9755     0.998  229.43
  10   4.7679      1.070  11.9761     1.036  254.51
  11   4.7796      1.040  11.9759     1.020  279.59
  12   4.8021      1.020  11.9753     1.028  304.81
  13   4.7583      1.000  11.9760     1.032  330.08
  14   4.7590      1.010  11.9752     1.014  355.10
  15   4.8015      0.990  11.9745     1.002  380.31
  16   4.7947      1.070  11.9754     1.008  405.51
  17   4.7890      1.030  11.9760     1.042  430.77
  18   4.7942      0.990  11.9760     1.012  455.82
  19   4.7948      0.980  11.9758     1.010  481.02
  20   4.7683      1.090  11.9755     0.970  506.28
  21   4.7732      1.020  11.9744     1.010  531.46
  22   4.7743      1.020  11.9756     0.990  556.62
  23   4.7974      1.020  11.9755     1.042  581.88
  24   4.7689      1.050  11.9751     0.976  607.05
  25   4.7853      1.040  11.9764     1.008  632.28
  26   4.8488      1.050  11.9766     0.998  657.45
  27   4.7879      1.000  11.9767     1.012  682.76
  28   4.7967      1.030  11.9760     0.988  707.98
  29   4.7616      1.020  11.9747     1.016  733.21
  30   4.7868      1.000  11.9752     1.062  758.39
  31   4.7987      0.990  11.9761     0.980  783.59
  32   4.7960      1.020  11.9757     1.016  808.80
  33   4.7670      1.030  11.9759     1.014  833.91
  34   4.7819      1.050  11.9753     1.016  859.11
  35   4.8227      1.030  11.9763     1.026  884.21
  36   4.7780      1.010  11.9752     1.002  909.45
  37   4.7692      0.970  11.9766     1.004  934.72
  38   4.7770      1.010  11.9748     1.008  959.92
  39   4.8380      0.990  11.9762     1.004  985.03
  40   4.8393      1.030  11.9751     1.040  1010.25
  41   4.8227      1.040  11.9758     1.006  1035.27
  42   4.7792      1.050  11.9760     1.016  1060.40
  43   4.7791      1.000  11.9750     1.002  1085.65
  44   4.8277      1.020  11.9771     1.014  1110.88
  45   4.8347      1.030  11.9752     1.044  1136.07
  46   4.8173      1.000  11.9754     1.010  1161.32
  47   4.7840      1.030  11.9743     1.008  1186.59
  48   4.7847      1.050  11.9746     1.010  1211.88
  49   4.8022      1.040  11.9753     1.010  1236.96
  50   4.7567      1.040  11.9729     1.028  1262.21
  51   4.7908      1.060  11.9775     1.032  1287.42
  52   4.7925      1.020  11.9740     1.018  1312.68
  53   4.8174      1.050  11.9756     1.008  1337.87
  54   4.8161      1.050  11.9757     1.004  1362.93
  55   4.7858      1.040  11.9767     1.004  1388.16
  56   4.7877      1.010  11.9752     1.014  1413.43
  57   4.8154      1.100  11.9747     1.010  1438.54
  58   4.7651      1.030  11.9745     1.014  1463.65
  59   4.8097      1.020  11.9756     1.022  1488.85
  60   4.8322      1.010  11.9747     1.022  1514.11
  61   4.7936      1.060  11.9748     1.012  1539.36
  62   4.8133      0.990  11.9747     1.014  1564.57
  63   4.7861      1.030  11.9755     1.006  1589.81
  64   4.7928      1.010  11.9757     1.006  1615.10
  65   4.7864      1.060  11.9753     1.022  1640.33
  66   4.8155      1.040  11.9753     1.004  1665.36
  67   4.7581      1.020  11.9739     1.040  1690.55
  68   4.8103      1.050  11.9768     0.982  1715.57
  69   4.7883      1.000  11.9749     1.030  1740.86
  70   4.8062      1.050  11.9746     1.016  1766.11
  71   4.7902      0.980  11.9743     1.032  1791.34
  72   4.7899      1.090  11.9771     1.008  1816.54
  73   4.8459      1.030  11.9748     1.008  1841.75
  74   4.8103      1.020  11.9750     1.012  1867.02
  75   4.8429      0.980  11.9752     1.016  1892.16
  76   4.7934      1.020  11.9743     1.010  1917.42
  77   4.8044      1.030  11.9747     1.040  1942.67
  78   4.8190      1.060  11.9752     1.018  1967.87
  79   4.7772      1.080  11.9767     1.006  1992.92
  80   4.7800      1.090  11.9743     1.000  2018.18
  81   4.7978      1.070  11.9747     1.024  2043.44
  82   4.7860      0.990  11.9749     1.012  2068.60
  83   4.8243      1.030  11.9760     1.026  2093.78
  84   4.7947      1.020  11.9749     0.998  2119.02
  85   4.7850      0.990  11.9750     1.022  2144.22
