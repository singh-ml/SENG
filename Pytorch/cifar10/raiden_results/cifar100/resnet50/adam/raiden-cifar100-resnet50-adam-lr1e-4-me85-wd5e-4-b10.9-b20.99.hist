Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17948521984 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.8690     10.160  4.2369     5.738  25.87
   2   3.5945     15.290  3.6941    13.192  49.31
   3   3.2776     21.150  3.3971    18.206  72.74
   4   3.0273     25.240  3.1361    22.898  96.17
   5   2.9075     27.830  2.9071    27.156  119.61
   6   2.7320     30.900  2.7279    30.716  143.05
   7   2.5827     34.260  2.5577    34.178  166.46
   8   2.4572     36.830  2.4011    37.302  189.91
   9   2.3916     39.250  2.2520    40.792  213.33
  10   2.2452     41.330  2.1066    43.638  236.75
  11   2.2336     42.480  1.9810    46.838  260.17
  12   2.0730     45.420  1.8613    49.484  283.59
  13   2.0144     47.590  1.7591    51.680  307.02
  14   2.0209     47.520  1.6587    54.228  330.41
  15   2.0080     48.300  1.5656    56.494  353.82
  16   1.9309     49.810  1.4781    58.564  377.24
  17   1.9207     50.420  1.3941    60.526  400.64
  18   1.8962     51.620  1.3185    62.740  424.05
  19   1.8614     52.680  1.2447    64.310  447.48
  20   1.9160     51.310  1.1740    66.200  470.90
  21   1.8558     52.710  1.1055    67.938  494.34
  22   1.8876     52.540  1.0363    69.596  517.77
  23   1.8243     54.030  0.9746    71.250  541.16
  24   1.8442     53.790  0.9159    73.068  564.57
  25   1.8261     54.700  0.8500    74.690  588.01
  26   1.8038     55.470  0.8088    76.192  611.42
  27   1.8434     55.480  0.7544    77.706  634.86
  28   1.8066     56.240  0.7099    78.800  658.28
  29   1.8485     56.060  0.6601    80.146  681.68
  30   1.8891     55.490  0.6187    81.290  705.07
  31   1.8483     56.400  0.5776    82.810  728.44
  32   1.9139     55.180  0.5404    83.864  751.81
  33   1.9343     55.960  0.5096    84.646  775.25
  34   1.8779     57.430  0.4771    85.474  798.66
  35   1.9471     56.190  0.4401    87.032  822.02
  36   1.8907     57.480  0.4139    87.436  845.45
  37   1.9267     57.350  0.3913    88.278  868.85
  38   1.9512     57.080  0.3689    88.934  892.23
  39   1.9815     56.370  0.3467    89.774  915.64
  40   2.0221     56.150  0.3310    90.050  939.07
  41   1.9784     56.790  0.3115    90.816  962.43
  42   1.9218     57.980  0.2985    91.070  985.83
