Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--damping', '1.0', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 29698690560 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.5958     14.830  4.2599     7.554  28.49
   2   3.6028     21.860  3.4054    17.966  54.31
   3   2.7744     30.740  2.9163    26.418  80.14
   4   2.3059     38.820  2.4519    35.594  105.95
   5   2.0254     45.980  2.1031    43.238  131.77
   6   1.8445     49.830  1.8579    48.924  157.58
   7   1.7271     53.680  1.6644    53.352  183.42
   8   1.6619     54.560  1.5079    57.176  209.23
   9   1.5844     56.440  1.4017    59.464  235.05
  10   1.4947     58.630  1.2817    62.814  260.88
  11   1.5539     58.660  1.1944    65.028  286.71
  12   1.4281     60.880  1.1107    67.076  312.56
  13   1.3629     63.010  1.0408    68.936  338.36
  14   1.3079     63.910  0.9790    70.632  364.18
  15   1.3339     63.730  0.9205    72.180  390.02
  16   1.3183     64.830  0.8616    73.606  415.83
  17   1.3182     64.740  0.8143    74.702  441.63
  18   1.3656     65.130  0.7614    76.408  467.47
  19   1.3506     65.040  0.7230    77.346  493.28
  20   1.2899     66.000  0.6768    78.836  519.10
  21   1.2828     66.860  0.6347    80.062  544.93
  22   1.3279     66.570  0.5966    80.962  570.73
  23   1.3346     65.980  0.5678    81.872  596.55
  24   1.2894     67.260  0.5322    82.970  622.34
  25   1.3311     66.860  0.4897    84.318  648.18
  26   1.3132     67.210  0.4597    85.202  674.02
  27   1.3443     66.520  0.4245    86.382  699.84
  28   1.3705     66.860  0.4070    86.766  725.66
  29   1.3781     66.850  0.3757    87.868  751.50
  30   1.3198     69.080  0.3496    88.604  777.34
  31   1.3724     68.240  0.3183    89.674  803.20
  32   1.3758     67.960  0.3001    90.338  829.02
  33   1.4041     67.830  0.2882    90.712  854.87
  34   1.3074     69.040  0.2506    92.070  880.70
  35   1.3133     69.140  0.2399    92.206  906.53
  36   1.3584     69.730  0.2163    93.062  932.35
  37   1.3308     69.670  0.2214    92.814  958.16
  38   1.4168     68.560  0.1948    93.806  983.99
  39   1.3430     70.240  0.1772    94.556  1009.81
  40   1.3641     70.330  0.1814    94.254  1035.64
  41   1.3993     69.950  0.1655    94.786  1061.48
  42   1.3775     70.580  0.1311    96.056  1087.29
  43   1.3916     69.860  0.1354    95.864  1113.10
  44   1.3124     71.390  0.1066    96.822  1138.92
  45   1.3407     71.660  0.1068    96.816  1164.75
  46   1.3953     70.390  0.0870    97.516  1190.56
  47   1.3745     71.290  0.0734    97.966  1216.40
  48   1.3454     71.140  0.0612    98.386  1242.23
  49   1.3105     72.830  0.0533    98.568  1268.06
