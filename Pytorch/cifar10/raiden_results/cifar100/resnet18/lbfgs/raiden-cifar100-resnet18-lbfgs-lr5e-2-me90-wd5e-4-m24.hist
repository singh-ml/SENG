Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 5741497856 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7441      0.970  11.0149     0.986  10.51
   2   4.7452      0.980  11.0165     1.000  19.56
   3   4.7444      0.980  11.0162     0.954  28.52
   4   4.7445      0.980  11.0152     0.974  37.46
   5   4.7440      0.980  11.0165     0.948  46.64
   6   4.7445      0.970  11.0163     1.012  55.59
   7   4.7432      0.970  11.0156     0.946  64.55
   8   4.7441      0.980  11.0157     0.990  73.42
   9   4.7434      0.980  11.0155     0.974  82.46
  10   4.7430      0.970  11.0156     0.974  91.88
  11   4.7444      0.980  11.0170     0.964  100.81
  12   4.7412      0.990  11.0159     0.970  109.65
  13   4.7441      0.970  11.0169     0.976  118.43
  14   4.7435      0.970  11.0162     0.968  127.10
  15   4.7425      0.980  11.0158     0.976  135.73
  16   4.7449      0.980  11.0157     0.992  144.44
  17   4.7449      0.980  11.0154     0.988  153.27
  18   4.7452      0.980  11.0166     0.944  161.88
  19   4.7420      0.980  11.0147     0.982  170.44
  20   4.7443      0.980  11.0162     1.028  179.14
  21   4.7459      0.970  11.0168     0.966  187.89
  22   4.7419      0.980  11.0153     1.004  196.78
  23   4.7471      0.980  11.0152     1.004  205.50
  24   4.7445      0.980  11.0168     0.956  214.10
  25   4.7423      0.980  11.0151     1.008  222.97
  26   4.7428      0.970  11.0165     1.006  231.56
  27   4.7429      0.980  11.0159     0.970  240.13
  28   4.7421      0.980  11.0153     1.002  248.92
  29   4.7420      0.960  11.0162     1.002  257.67
  30   4.7417      0.990  11.0154     0.980  266.42
  31   4.7467      0.970  11.0152     0.984  274.96
  32   4.7456      0.970  11.0165     0.954  283.57
  33   4.7423      0.980  11.0167     0.966  292.37
  34   4.7416      0.980  11.0155     0.988  300.93
  35   4.7459      0.980  11.0154     1.004  309.65
  36   4.7451      0.980  11.0165     0.974  318.35
  37   4.7425      0.970  11.0160     0.988  327.07
  38   4.7427      0.980  11.0155     0.972  335.71
  39   4.7448      0.980  11.0165     0.962  344.30
  40   4.7421      0.970  11.0159     0.948  352.94
  41   4.7450      0.980  11.0157     1.028  361.73
  42   4.7413      0.970  11.0148     0.974  370.44
  43   4.7466      0.980  11.0164     0.970  378.95
  44   4.7450      0.980  11.0154     0.978  387.62
  45   4.7423      0.980  11.0161     0.964  396.24
  46   4.7431      0.980  11.0157     0.970  404.94
  47   4.7458      0.960  11.0146     0.972  413.59
  48   4.7438      0.990  11.0157     0.986  422.39
  49   4.7428      0.980  11.0153     0.992  431.13
  50   4.7450      0.960  11.0155     1.012  440.08
  51   4.7428      0.980  11.0156     0.986  448.82
  52   4.7439      0.980  11.0149     0.992  457.51
  53   4.7399      0.980  11.0155     0.978  466.20
  54   4.7406      0.980  11.0158     0.976  474.92
  55   4.7411      0.990  11.0152     0.978  483.45
  56   4.7430      0.980  11.0163     0.986  491.98
  57   4.7446      0.990  11.0162     1.012  500.61
  58   4.7438      0.980  11.0157     0.958  509.38
  59   4.7493      0.990  11.0161     0.970  518.07
  60   4.7437      0.990  11.0155     1.004  526.98
  61   4.7437      0.970  11.0152     1.002  535.62
  62   4.7457      0.980  11.0155     0.978  544.40
  63   4.7436      0.980  11.0142     0.990  553.12
  64   4.7427      0.980  11.0159     0.996  561.70
  65   4.7444      0.970  11.0158     1.028  570.43
  66   4.7426      0.980  11.0162     0.974  579.27
  67   4.7438      0.970  11.0158     1.030  588.19
  68   4.7422      0.960  11.0147     1.000  596.92
  69   4.7424      0.980  11.0162     1.004  605.81
  70   4.7449      0.980  11.0154     0.950  614.61
  71   4.7450      0.980  11.0160     0.976  623.26
  72   4.7432      0.980  11.0161     1.026  631.92
  73   4.7445      0.980  11.0163     0.974  640.62
  74   4.7468      0.990  11.0155     1.008  649.37
  75   4.7449      0.980  11.0161     0.974  658.04
  76   4.7418      0.970  11.0167     0.990  666.71
  77   4.7443      0.990  11.0171     0.960  675.36
  78   4.7422      0.980  11.0158     0.972  684.14
  79   4.7451      0.980  11.0158     0.988  692.65
  80   4.7437      0.980  11.0157     1.006  701.38
  81   4.7454      0.970  11.0161     0.990  710.24
  82   4.7433      0.990  11.0152     1.000  718.89
  83   4.7435      0.960  11.0164     0.938  727.78
  84   4.7435      0.960  11.0156     1.000  736.59
  85   4.7455      0.990  11.0154     0.990  745.41
  86   4.7466      0.980  11.0143     0.976  754.01
  87   4.7433      0.970  11.0148     0.986  762.58
  88   4.7430      0.980  11.0168     0.984  771.22
  89   4.7424      0.970  11.0152     0.972  779.84
  90   4.7425      0.980  11.0163     0.998  788.69
