Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7347      0.480  7.2549     0.598  8.74
   2   4.7324      0.510  7.2535     0.596  15.88
   3   4.7341      0.470  7.2544     0.576  23.06
   4   4.7343      0.480  7.2549     0.558  30.15
   5   4.7346      0.490  7.2525     0.582  37.31
   6   4.7358      0.490  7.2543     0.590  44.40
   7   4.7345      0.500  7.2539     0.628  51.50
   8   4.7368      0.520  7.2552     0.650  58.57
   9   4.7348      0.490  7.2539     0.574  65.70
  10   4.7337      0.500  7.2536     0.558  72.71
  11   4.7342      0.460  7.2541     0.532  79.86
  12   4.7355      0.500  7.2549     0.574  87.01
  13   4.7355      0.490  7.2551     0.626  94.16
  14   4.7328      0.490  7.2544     0.578  101.28
  15   4.7332      0.510  7.2536     0.600  108.25
  16   4.7339      0.490  7.2541     0.578  115.27
  17   4.7353      0.500  7.2543     0.562  122.32
  18   4.7351      0.480  7.2552     0.588  129.37
  19   4.7364      0.500  7.2534     0.632  136.49
  20   4.7351      0.490  7.2546     0.582  143.55
  21   4.7343      0.500  7.2536     0.608  150.62
  22   4.7359      0.500  7.2546     0.622  157.61
  23   4.7346      0.480  7.2547     0.610  164.70
  24   4.7343      0.480  7.2545     0.610  171.76
  25   4.7341      0.490  7.2539     0.590  178.67
  26   4.7346      0.470  7.2546     0.596  185.81
  27   4.7363      0.540  7.2566     0.616  192.73
  28   4.7336      0.450  7.2547     0.608  199.72
  29   4.7348      0.470  7.2542     0.582  206.85
  30   4.7344      0.490  7.2551     0.644  213.79
  31   4.7322      0.500  7.2559     0.620  220.80
  32   4.7370      0.500  7.2546     0.630  227.89
  33   4.7360      0.480  7.2543     0.594  235.00
  34   4.7339      0.470  7.2546     0.594  242.26
  35   4.7335      0.500  7.2533     0.584  249.24
  36   4.7337      0.490  7.2541     0.572  256.35
  37   4.7367      0.500  7.2539     0.608  263.34
  38   4.7341      0.510  7.2554     0.584  270.49
  39   4.7341      0.510  7.2523     0.592  277.58
  40   4.7328      0.490  7.2531     0.606  284.62
  41   4.7351      0.490  7.2542     0.630  291.69
  42   4.7366      0.520  7.2535     0.604  298.67
  43   4.7345      0.500  7.2530     0.588  305.68
  44   4.7356      0.480  7.2536     0.616  312.89
  45   4.7353      0.510  7.2544     0.598  319.96
  46   4.7349      0.510  7.2537     0.578  327.04
  47   4.7360      0.500  7.2537     0.590  333.98
  48   4.7349      0.490  7.2555     0.586  341.17
  49   4.7349      0.490  7.2540     0.576  348.22
  50   4.7331      0.490  7.2529     0.618  355.28
  51   4.7356      0.480  7.2545     0.594  362.23
  52   4.7361      0.480  7.2528     0.600  369.30
  53   4.7336      0.470  7.2541     0.582  376.40
  54   4.7366      0.500  7.2541     0.616  383.36
  55   4.7346      0.490  7.2552     0.584  390.43
  56   4.7341      0.490  7.2546     0.562  397.42
  57   4.7351      0.510  7.2533     0.638  404.54
  58   4.7368      0.480  7.2554     0.626  411.64
  59   4.7338      0.490  7.2546     0.582  418.62
  60   4.7357      0.500  7.2540     0.590  425.58
  61   4.7364      0.520  7.2540     0.638  432.59
  62   4.7348      0.530  7.2543     0.598  439.70
  63   4.7343      0.490  7.2536     0.594  446.69
  64   4.7351      0.490  7.2538     0.578  453.68
  65   4.7355      0.500  7.2545     0.612  460.71
  66   4.7359      0.490  7.2535     0.590  467.74
  67   4.7330      0.500  7.2537     0.608  474.78
  68   4.7356      0.500  7.2537     0.614  481.83
  69   4.7338      0.500  7.2535     0.592  488.93
  70   4.7350      0.490  7.2546     0.610  496.02
  71   4.7341      0.490  7.2554     0.554  503.01
  72   4.7368      0.540  7.2544     0.564  510.00
  73   4.7337      0.500  7.2542     0.562  516.99
  74   4.7354      0.540  7.2542     0.596  524.07
  75   4.7334      0.510  7.2545     0.590  531.18
  76   4.7350      0.480  7.2553     0.556  538.29
  77   4.7349      0.500  7.2538     0.604  545.20
  78   4.7340      0.510  7.2555     0.650  552.30
  79   4.7357      0.500  7.2536     0.600  559.52
  80   4.7343      0.500  7.2543     0.610  566.60
  81   4.7349      0.470  7.2547     0.584  573.68
  82   4.7349      0.510  7.2540     0.604  580.75
  83   4.7362      0.500  7.2546     0.566  587.78
  84   4.7332      0.490  7.2534     0.612  594.88
  85   4.7353      0.510  7.2552     0.566  601.95
