Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4483598848 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7476      0.980  7.2687     1.014  9.31
   2   4.7427      0.980  7.2686     0.994  16.95
   3   4.7474      0.980  7.2681     0.930  24.62
   4   4.7454      1.010  7.2704     0.964  32.22
   5   4.7474      0.960  7.2689     0.996  39.96
   6   4.7446      1.030  7.2684     0.924  47.63
   7   4.7478      0.960  7.2683     0.964  55.34
   8   4.7450      1.000  7.2690     0.948  63.19
   9   4.7475      1.000  7.2687     0.916  70.80
  10   4.7462      1.020  7.2700     0.934  78.40
  11   4.7481      0.980  7.2698     0.980  85.99
  12   4.7469      0.980  7.2686     0.946  93.80
  13   4.7481      1.020  7.2685     0.908  101.42
  14   4.7466      0.990  7.2688     1.010  109.11
  15   4.7494      0.960  7.2686     0.948  116.72
  16   4.7464      1.010  7.2683     1.052  124.33
  17   4.7481      1.010  7.2710     0.898  132.16
  18   4.7448      0.960  7.2689     0.920  139.78
  19   4.7438      0.970  7.2684     0.988  147.31
  20   4.7444      1.040  7.2688     0.956  154.91
  21   4.7477      1.000  7.2692     0.942  162.74
  22   4.7461      0.960  7.2689     0.922  170.45
  23   4.7460      1.000  7.2691     0.966  178.03
  24   4.7479      1.030  7.2696     0.912  185.66
  25   4.7452      0.980  7.2692     0.916  193.22
  26   4.7471      0.970  7.2690     0.960  200.76
  27   4.7447      1.040  7.2694     0.986  208.52
  28   4.7434      1.000  7.2691     0.942  216.07
  29   4.7457      0.990  7.2698     0.912  223.75
  30   4.7467      1.010  7.2706     0.964  231.47
  31   4.7471      0.990  7.2685     0.984  239.20
  32   4.7429      1.030  7.2687     0.944  246.71
  33   4.7481      0.990  7.2677     0.962  254.43
  34   4.7453      0.980  7.2692     0.908  262.13
  35   4.7482      0.970  7.2706     0.900  269.79
  36   4.7460      1.000  7.2698     0.950  277.57
  37   4.7468      0.960  7.2697     0.922  285.11
  38   4.7456      1.020  7.2697     0.904  292.69
  39   4.7465      0.990  7.2688     1.006  300.24
  40   4.7472      1.000  7.2688     0.898  308.07
  41   4.7490      0.980  7.2690     0.976  315.93
  42   4.7459      1.040  7.2696     0.888  323.50
  43   4.7484      0.970  7.2682     0.988  331.20
  44   4.7476      1.010  7.2697     0.970  338.98
  45   4.7472      0.980  7.2714     0.938  346.96
  46   4.7451      1.020  7.2695     0.956  354.64
  47   4.7478      1.000  7.2685     0.912  362.24
  48   4.7491      0.980  7.2692     0.984  369.88
  49   4.7442      1.010  7.2680     1.004  377.49
  50   4.7443      1.030  7.2684     1.018  385.15
  51   4.7482      0.990  7.2678     0.924  392.93
  52   4.7441      1.040  7.2694     0.996  400.62
  53   4.7476      0.970  7.2691     0.928  408.18
  54   4.7500      0.940  7.2687     0.916  415.95
  55   4.7458      0.940  7.2692     1.012  423.66
  56   4.7447      1.000  7.2701     0.938  431.26
  57   4.7478      0.950  7.2693     0.938  438.74
  58   4.7471      1.040  7.2685     0.912  446.59
  59   4.7449      1.000  7.2689     0.942  454.36
  60   4.7497      1.010  7.2681     0.948  461.92
  61   4.7455      0.980  7.2681     0.966  469.46
  62   4.7490      0.970  7.2688     0.960  477.20
  63   4.7461      1.000  7.2704     0.946  484.73
  64   4.7467      0.980  7.2693     0.962  492.23
  65   4.7507      0.960  7.2677     0.948  499.78
  66   4.7461      0.990  7.2703     0.998  507.32
  67   4.7438      1.010  7.2699     0.948  514.99
  68   4.7482      1.020  7.2683     0.940  522.63
  69   4.7472      1.000  7.2689     0.982  530.33
  70   4.7490      0.980  7.2705     0.940  538.02
  71   4.7477      1.010  7.2682     0.860  545.59
  72   4.7430      1.020  7.2704     0.932  553.29
  73   4.7475      1.030  7.2698     0.946  561.09
  74   4.7468      1.000  7.2687     0.988  568.91
  75   4.7468      0.980  7.2690     0.916  576.45
  76   4.7441      1.020  7.2696     0.976  584.07
  77   4.7447      1.000  7.2708     0.942  591.80
  78   4.7472      1.000  7.2690     0.930  599.46
  79   4.7475      0.950  7.2690     0.976  607.03
  80   4.7472      1.020  7.2691     0.896  614.64
  81   4.7460      1.000  7.2696     0.992  622.34
  82   4.7460      1.040  7.2682     0.932  630.20
  83   4.7471      0.960  7.2688     0.944  637.76
  84   4.7472      0.990  7.2683     0.930  645.38
  85   4.7453      1.010  7.2697     0.902  653.10
  86   4.7474      0.990  7.2688     0.892  660.92
  87   4.7487      0.960  7.2699     0.928  668.56
  88   4.7445      1.020  7.2691     0.890  676.52
  89   4.7454      0.950  7.2693     0.974  684.25
  90   4.7435      0.990  7.2677     0.992  691.75
