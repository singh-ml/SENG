Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 5380150784 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.5901      2.550  12.3347     1.546  10.13
   2   4.4783      3.410  10.7867     2.968  18.77
   3   4.3292      5.070  10.7002     3.834  27.38
   4   4.2413      5.400  10.5079     4.888  36.20
   5   4.1359      7.270  10.3877     6.116  44.81
   6   4.0715      8.510  10.2941     7.202  53.43
   7   4.0199      9.090  10.2193     8.444  62.11
   8   3.9270     10.870  10.1584     9.286  70.95
   9   3.9008     10.620  10.0622    10.280  79.63
  10   3.8641     10.950  10.0241    10.706  88.25
  11   3.8319     11.380  9.9783    11.236  96.95
  12   3.7977     11.530  9.9378    11.484  105.72
  13   3.7528     12.200  9.8805    12.078  114.34
  14   3.7232     12.750  9.8454    12.794  123.07
  15   3.7081     13.000  9.8125    13.016  131.64
  16   3.6857     13.590  9.7740    13.224  140.40
  17   3.6750     13.960  9.7530    13.672  149.14
  18   19.6564      2.290  9.9158    13.072  157.80
  19   5.1321      1.000  3033.7002     1.318  166.50
  20   160.6579      0.630  635.5306     1.040  175.13
  21   393.9066      1.000  488859.8598     1.024  183.97
  22   13.8532      1.000  369.8058     0.880  192.67
  23   30.0904      1.000  136.7151     1.058  201.37
  24   4.9302      1.120  48.8464     1.154  210.05
  25   5.9223      1.540  29.3425     1.338  218.79
  26   13492091.7250      1.070  38973178553.9338     1.090  227.30
  27   4553437.9712      1.150  57231336.0000     1.058  235.82
  28   5157076.5031      1.150  90648084.0612     1.038  244.52
  29   1797245.8441      1.160  42256829.6122     1.052  253.39
  30   2235837.1754      0.780  40922096.1633     0.952  262.04
  31   379454678.4000      1.000  5468842166.7347     0.970  270.66
  32   4152987443.2000      1.000  5764078074931.3262     1.080  279.32
  33   12849462.5500      1.090  1257696229976.8164     1.076  288.10
  34   212170543.2719      0.960  3807781924.5714     0.940  296.79
  35   172544188.8000      1.070  3784279429.2245     0.846  305.48
  36   407196471.9688      1.240  3570027999.3469     0.914  314.13
  37   346986376.2625      1.300  3555718167.5102     0.966  322.88
  38   428793665.1750      1.150  3557131897.4694     0.990  331.51
  39   233371586.4875      0.910  3545004438.2041     0.966  340.15
  40   1791723797.6000      1.050  12969770541.7143     1.076  348.88
  41   737773104.3000      0.930  3857473060.5714     0.950  357.92
  42   588555734.8500      0.790  3726063851.1020     0.814  366.50
  43   421693660.7000      0.770  3732993872.9796     0.762  375.00
  44   705242509.2250      0.820  3732233886.0408     0.830  383.63
  45   447330842.2000      0.750  3728531900.0816     0.818  392.25
  46   807260515.0750      0.870  3712181954.6122     0.836  400.82
  47   356585933.2750      0.750  3722981601.9592     0.744  409.23
  48   509794002.2250      0.800  3742338379.7551     0.798  417.58
  49   672020809.9500      0.840  3734978196.8980     0.834  426.00
  50   440109094.6500      0.740  3735498868.2449     0.830  434.49
  51   599655772.2250      0.790  3726747065.4694     0.756  442.77
  52   579721472.5500      0.840  3732673560.8163     0.820  451.05
  53   559242229.9750      0.810  3730952114.9388     0.800  459.43
  54   306196054.3000      0.680  3714208927.3469     0.828  468.01
  55   382011040.9500      0.740  3723401823.3469     0.750  476.26
  56   503395666.5500      0.790  3737143680.0000     0.820  484.66
  57   620663115.4750      0.790  3731937502.0408     0.808  493.05
  58   484199521.4000      0.790  3740668064.6531     0.768  501.52
  59   589742512.3500      0.840  3727690974.0408     0.796  509.82
  60   358541380.2500      0.680  3733118791.8367     0.836  518.21
  61   803885223.4500      0.860  3732190817.9592     0.774  526.61
  62   481257910.8500      0.760  3745256880.3265     0.766  535.00
  63   314539533.6750      0.720  3751226217.7959     0.750  543.43
  64   314049413.1000      0.740  3750142785.3061     0.840  551.86
  65   700959789.6500      0.860  3725053091.2653     0.810  560.32
  66   725338952.1500      0.830  3748208993.9592     0.840  568.76
  67   571336478.5000      0.780  3736431721.7959     0.824  577.14
  68   493610072.7500      0.810  3734349348.5714     0.792  585.52
  69   550793608.6500      0.760  3732330661.8776     0.814  593.88
  70   708046268.5000      0.850  3736729876.8980     0.816  602.32
  71   362305546.0000      0.720  3723844058.1224     0.744  610.84
  72   464839406.4750      0.810  3748939228.7347     0.730  619.19
  73   933905095.1500      0.870  3724562004.8980     0.740  627.59
  74   478953093.9750      0.780  3741799912.4898     0.738  635.96
  75   319277122.1750      0.710  3723339759.0204     0.800  644.51
  76   665654112.9750      0.800  3720031720.4898     0.830  652.88
  77   620115118.1500      0.830  3714537321.7959     0.776  661.22
  78   629162953.0750      0.810  3730910210.6122     0.788  669.58
  79   567610699.5000      0.810  3750064677.8776     0.778  677.89
  80   490679788.8500      0.770  3744579870.0408     0.828  686.31
  81   453140003.7500      0.760  3757853302.8571     0.722  694.76
  82   422314442.3750      0.730  3733128073.1429     0.794  703.35
  83   262715447.2750      0.640  3734730169.4694     0.728  711.72
  84   424463857.2500      0.740  3726110763.1020     0.816  720.27
  85   922612500.1500      0.900  3716782544.9796     0.818  728.78
