Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6968      0.850  5.9598     0.946  8.73
   2   4.6972      0.870  5.9609     0.868  15.98
   3   4.6977      0.850  5.9608     0.916  23.32
   4   4.6969      0.880  5.9601     0.928  30.68
   5   4.6972      0.820  5.9604     0.924  38.01
   6   4.6967      0.850  5.9603     0.924  45.38
   7   4.6972      0.860  5.9605     0.906  52.78
   8   4.6972      0.820  5.9603     0.952  60.26
   9   4.6976      0.870  5.9600     0.886  67.64
  10   4.6964      0.790  5.9589     0.978  75.00
  11   4.6961      0.830  5.9602     0.954  82.23
  12   4.6975      0.840  5.9592     0.918  89.51
  13   4.6961      0.820  5.9611     0.884  96.92
  14   4.6968      0.820  5.9602     0.910  104.30
  15   4.6976      0.910  5.9603     0.872  111.63
  16   4.6956      0.850  5.9597     0.924  119.01
  17   4.6963      0.850  5.9601     0.922  126.38
  18   4.6963      0.870  5.9598     0.994  133.64
  19   4.6981      0.880  5.9606     0.948  140.83
  20   4.6964      0.850  5.9603     0.954  148.07
  21   4.6966      0.810  5.9608     0.948  155.31
  22   4.6975      0.820  5.9593     0.920  162.72
  23   4.6975      0.860  5.9602     0.976  170.11
  24   4.6976      0.860  5.9608     0.940  177.38
  25   4.6963      0.830  5.9604     0.940  184.55
  26   4.6972      0.810  5.9607     0.946  191.85
  27   4.6963      0.820  5.9593     0.896  199.23
  28   4.6964      0.810  5.9602     0.888  206.71
  29   4.6960      0.830  5.9594     0.928  213.90
  30   4.6955      0.840  5.9604     0.920  221.22
  31   4.6970      0.870  5.9593     0.878  228.50
  32   4.6962      0.830  5.9607     0.920  235.75
  33   4.6974      0.870  5.9604     0.950  243.14
  34   4.6966      0.870  5.9598     0.950  250.50
  35   4.6967      0.820  5.9601     0.896  257.88
  36   4.6972      0.860  5.9592     0.910  265.29
  37   4.6962      0.790  5.9603     0.886  272.67
  38   4.6964      0.760  5.9590     0.886  280.23
  39   4.6968      0.820  5.9598     0.960  287.55
  40   4.6963      0.850  5.9596     0.988  294.86
  41   4.6963      0.840  5.9595     0.826  302.09
  42   4.6972      0.870  5.9596     0.962  309.44
  43   4.6978      0.830  5.9586     0.974  316.78
  44   4.6967      0.870  5.9600     0.888  324.13
  45   4.6961      0.880  5.9586     0.946  331.41
  46   4.6969      0.850  5.9600     0.898  338.73
  47   4.6965      0.850  5.9608     0.908  346.18
  48   4.6953      0.820  5.9595     0.948  353.50
  49   4.6975      0.810  5.9595     0.970  360.82
  50   4.6975      0.790  5.9597     0.930  368.12
  51   4.6971      0.870  5.9608     0.906  375.31
  52   4.6977      0.810  5.9593     0.938  382.80
  53   4.6964      0.860  5.9599     0.950  390.02
  54   4.6967      0.860  5.9595     0.944  397.30
  55   4.6964      0.830  5.9597     0.920  404.68
  56   4.6963      0.790  5.9596     0.954  412.02
  57   4.6965      0.890  5.9592     0.930  419.43
  58   4.6972      0.840  5.9597     0.882  426.64
  59   4.6965      0.850  5.9593     0.952  433.93
  60   4.6959      0.800  5.9595     0.988  441.52
  61   4.6972      0.860  5.9613     0.868  448.76
  62   4.6967      0.870  5.9599     0.982  456.15
  63   4.6958      0.840  5.9599     0.890  463.47
  64   4.6967      0.890  5.9595     0.960  470.74
  65   4.6964      0.860  5.9607     0.920  477.92
  66   4.6970      0.820  5.9597     0.910  485.24
  67   4.6966      0.860  5.9603     0.946  492.70
  68   4.6968      0.790  5.9606     0.944  500.06
  69   4.6962      0.820  5.9602     0.856  507.41
  70   4.6956      0.800  5.9593     0.936  514.76
  71   4.6969      0.840  5.9593     0.990  522.42
  72   4.6972      0.870  5.9600     0.918  529.93
  73   4.6966      0.840  5.9594     0.948  537.30
  74   4.6963      0.820  5.9597     0.970  544.69
  75   4.6954      0.800  5.9598     0.952  552.13
  76   4.6961      0.840  5.9601     0.856  559.48
  77   4.6971      0.860  5.9609     0.892  566.80
  78   4.6960      0.820  5.9607     0.914  574.17
  79   4.6956      0.820  5.9599     0.912  581.42
  80   4.6968      0.850  5.9600     0.958  588.74
  81   4.6965      0.850  5.9606     0.902  596.15
  82   4.6979      0.910  5.9591     0.864  603.46
  83   4.6957      0.820  5.9605     0.922  610.74
  84   4.6958      0.820  5.9596     0.898  618.10
  85   4.6972      0.910  5.9590     0.932  625.46
