Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7013      0.830  7.2274     0.940  8.75
   2   4.7036      0.820  7.2266     0.904  16.01
   3   4.7041      0.780  7.2265     0.910  23.20
   4   4.7038      0.780  7.2267     0.936  30.49
   5   4.7030      0.850  7.2272     0.880  37.75
   6   4.7045      0.820  7.2284     0.944  44.85
   7   4.7040      0.830  7.2262     0.910  52.09
   8   4.7043      0.800  7.2274     0.848  59.22
   9   4.7045      0.830  7.2272     0.888  66.20
  10   4.7035      0.830  7.2267     0.902  73.25
  11   4.7064      0.800  7.2285     0.876  80.32
  12   4.7030      0.840  7.2288     0.940  87.44
  13   4.7022      0.800  7.2268     0.912  94.54
  14   4.7021      0.800  7.2282     0.898  101.68
  15   4.7048      0.790  7.2277     0.848  108.69
  16   4.7019      0.830  7.2275     0.920  115.70
  17   4.7061      0.800  7.2280     0.858  122.57
  18   4.7036      0.780  7.2274     0.908  129.51
  19   4.7029      0.810  7.2277     0.890  136.54
  20   4.7037      0.800  7.2264     0.956  143.80
  21   4.7041      0.830  7.2276     0.872  150.91
  22   4.7057      0.800  7.2276     0.880  158.01
  23   4.7027      0.800  7.2264     0.946  164.94
  24   4.7015      0.770  7.2263     0.952  172.01
  25   4.7027      0.800  7.2274     0.916  178.99
  26   4.7034      0.780  7.2285     0.870  185.99
  27   4.7045      0.810  7.2267     0.896  193.13
  28   4.7042      0.810  7.2272     0.922  200.06
  29   4.7041      0.810  7.2269     0.936  207.13
  30   4.7041      0.780  7.2255     0.960  214.36
  31   4.7031      0.820  7.2281     0.922  221.34
  32   4.7027      0.780  7.2274     0.934  228.26
  33   4.7036      0.830  7.2285     0.916  235.26
  34   4.7035      0.810  7.2278     0.876  242.36
  35   4.7022      0.800  7.2268     0.878  249.35
  36   4.7034      0.820  7.2282     0.918  256.34
  37   4.7031      0.810  7.2281     0.896  263.34
  38   4.7036      0.770  7.2265     0.898  270.38
  39   4.7048      0.800  7.2270     0.876  277.33
  40   4.7042      0.810  7.2284     0.930  284.54
  41   4.7027      0.830  7.2285     0.904  291.67
  42   4.7015      0.830  7.2272     0.902  298.76
  43   4.7014      0.810  7.2267     0.916  305.87
  44   4.7038      0.800  7.2274     0.890  312.90
  45   4.7023      0.790  7.2268     0.898  320.01
  46   4.7044      0.830  7.2277     0.882  327.06
  47   4.7021      0.800  7.2277     0.916  334.13
  48   4.7041      0.780  7.2280     0.888  341.25
  49   4.7029      0.790  7.2275     0.958  348.38
  50   4.7046      0.790  7.2290     0.890  355.32
  51   4.7042      0.800  7.2273     0.900  362.62
  52   4.7043      0.820  7.2273     0.898  369.91
  53   4.7024      0.810  7.2276     0.916  376.93
  54   4.7024      0.790  7.2269     0.892  383.98
  55   4.7043      0.790  7.2273     0.928  391.06
  56   4.7031      0.790  7.2279     0.920  397.98
  57   4.7033      0.800  7.2283     0.910  405.11
  58   4.7059      0.790  7.2283     0.876  412.06
  59   4.7020      0.800  7.2282     0.924  419.25
  60   4.7000      0.790  7.2272     0.862  426.32
  61   4.7039      0.820  7.2280     0.914  433.33
  62   4.7034      0.800  7.2272     0.946  440.37
  63   4.6999      0.820  7.2269     0.920  447.41
  64   4.7024      0.820  7.2276     0.924  454.45
  65   4.7042      0.760  7.2278     0.954  461.43
  66   4.7043      0.790  7.2272     0.856  468.70
  67   4.7054      0.800  7.2274     0.878  475.81
  68   4.7021      0.790  7.2276     0.922  482.86
  69   4.7046      0.800  7.2276     0.842  489.96
  70   4.7017      0.810  7.2271     0.898  497.06
  71   4.7062      0.800  7.2280     0.918  504.08
  72   4.7027      0.800  7.2281     0.882  511.09
  73   4.7034      0.790  7.2272     0.950  518.05
  74   4.7047      0.800  7.2272     0.956  524.97
  75   4.7018      0.800  7.2273     0.950  532.07
  76   4.7027      0.770  7.2280     0.908  539.07
  77   4.7035      0.770  7.2282     0.938  546.09
  78   4.7054      0.800  7.2276     0.910  553.04
  79   4.7043      0.770  7.2288     0.908  560.09
  80   4.7052      0.800  7.2284     0.980  567.08
  81   4.7038      0.820  7.2287     0.908  574.12
  82   4.7049      0.770  7.2278     0.934  581.03
  83   4.7053      0.790  7.2269     0.888  587.96
  84   4.7032      0.810  7.2260     0.960  595.03
  85   4.7040      0.840  7.2277     0.924  602.03
  86   4.7040      0.810  7.2271     0.926  609.08
  87   4.7033      0.790  7.2276     0.918  616.07
  88   4.7024      0.840  7.2268     0.900  623.14
  89   4.7027      0.790  7.2274     0.872  630.32
  90   4.7049      0.820  7.2257     0.918  637.37
