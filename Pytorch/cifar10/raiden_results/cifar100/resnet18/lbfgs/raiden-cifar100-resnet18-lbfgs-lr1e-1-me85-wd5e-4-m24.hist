Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 5740051968 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   11.6429      1.000  14.2251     2.112  10.19
   2   4.4565      3.460  26.9872     2.098  18.94
   3   4.3631      4.360  10.6407     3.878  28.05
   4   4.7159      1.110  192.2875     2.734  37.03
   5   5.8187      2.280  16.2938     1.806  45.71
   6   4.7078      1.490  17.3046     2.002  54.52
   7   4.5453      3.250  9.3247     2.354  63.28
   8   4.3294      3.380  6.5103     3.244  72.05
   9   4.2721      3.760  5.8175     3.836  81.03
  10   74.0040      0.730  5.6143     4.300  89.87
  11   4.1187      5.380  5.2128     4.874  98.68
  12   4.3567      3.330  5.0816     5.780  107.64
  13   4.7040      1.000  1130.3053     5.082  116.59
  14   4.5240      2.170  6.6982     2.186  125.40
  15   4.1329      5.450  5.0356     4.678  134.10
  16   4.0925      5.620  4.8509     5.382  143.14
  17   4.0855      5.200  77.8463     4.610  152.06
  18   5.6120      1.000  3970.8893     3.948  160.90
  19   4.4264      3.220  5.7875     2.030  169.78
  20   11.1543      0.990  552007.1502     1.342  178.61
  21   13.4452      1.200  278.9396     1.088  187.42
  22   262.4802      1.200  243.1496     1.258  196.18
  23   19.4508      1.190  274.9002     1.266  205.06
  24   50.2877      1.200  12365.6499     1.116  214.05
  25   846.3562      1.000  318492.8357     1.050  222.85
  26   224978.8109      1.000  4328771466.5065     1.158  231.66
  27   505275.6125      1.170  2228988.9777     1.228  240.43
  28   459837.1371      1.150  2181615.0121     1.100  249.45
  29   657666.6994      1.110  14435582.9432     1.080  258.21
  30   97643.1939      1.000  1952532.1008     1.040  267.00
  31   63019.7667      1.390  1818636.5191     1.144  275.78
  32   44687.1686      1.110  1793636.2787     1.156  284.62
  33   55441.4792      1.030  1779669.0714     1.180  293.49
  34   53414.1580      1.140  1783474.6881     1.196  302.34
  35   36888.0008      1.350  1754741.3342     1.044  311.33
  36   45702.6276      1.140  1744793.4898     1.104  320.09
  37   33847.8761      1.100  1468950.5721     1.074  329.12
  38   82547.8293      1.070  1276601.3801     1.084  337.97
  39   54977.5960      1.030  1270681.9031     1.020  346.76
  40   48256.4498      1.400  1248697.0242     1.094  355.51
  41   59942.6015      1.040  1204781.0727     1.216  364.46
  42   69656.7110      1.280  1167797.4936     1.134  373.19
  43   48703.0031      1.340  1159864.6747     1.146  381.94
  44   61323.8797      1.270  1158551.8884     1.164  390.52
  45   42529.4457      1.260  1159720.0166     1.140  399.12
  46   67518.8613      1.240  1162135.8093     1.176  407.76
  47   56011.9056      1.220  1160877.0217     1.150  416.38
  48   46080.7776      1.270  1160640.5204     1.206  424.95
  49   45463.9601      1.250  1161439.2538     1.136  433.69
  50   84042.2585      1.250  1161552.2436     1.234  442.33
  51   89034.6917      1.230  1162314.1582     1.110  450.83
  52   71532.7581      1.250  1159035.5102     1.170  459.71
  53   58316.3516      1.240  1159728.1518     1.226  468.27
  54   77137.9815      1.320  1160047.7717     1.170  476.70
  55   77808.6489      1.250  1162272.2781     1.228  485.33
  56   77474.6121      1.190  1159201.6409     1.168  493.94
  57   58158.5606      1.230  1161811.4681     1.180  502.69
  58   61269.2785      1.310  1162589.6263     1.186  511.25
  59   79568.9682      1.280  1162212.8278     1.114  520.02
  60   96710.0361      1.220  1161265.4426     1.184  528.51
  61   59439.7384      1.290  1160784.0223     1.158  537.28
  62   64157.6072      1.310  1161517.4381     1.166  545.97
  63   52562.0409      1.400  1160063.7079     1.162  554.46
  64   89153.2560      1.250  1158745.8756     1.124  563.15
  65   59312.8089      1.240  1159911.4656     1.126  571.82
  66   66860.1183      1.290  1159912.5383     1.168  580.35
  67   57192.2528      1.360  1159293.9439     1.190  588.82
  68   56402.8992      1.220  1162874.7832     1.132  597.46
  69   75699.1347      1.190  1161427.8999     1.192  606.29
  70   48356.7114      1.330  1159497.2838     1.252  614.95
  71   60899.2554      1.260  1161580.8514     1.174  623.42
  72   66538.1812      1.220  1162498.8195     1.190  632.06
  73   58363.1045      1.220  1163122.0293     1.196  640.79
  74   70851.2821      1.230  1162574.6824     1.122  649.51
  75   73445.4786      1.290  1160949.7557     1.200  658.13
  76   61197.0010      1.290  1156998.1492     1.148  666.68
  77   78977.5104      1.180  1161180.9656     1.220  675.46
  78   66499.1962      1.240  1161123.2704     1.134  684.07
  79   53134.8446      1.240  1156774.9031     1.206  692.75
  80   66154.7515      1.300  1159021.8686     1.228  701.34
  81   61277.2092      1.260  1160376.5497     1.154  709.98
  82   75784.0276      1.220  1159951.1760     1.246  718.61
  83   61185.6325      1.300  1158711.8272     1.174  727.28
  84   58937.3278      1.260  1160068.8565     1.136  735.94
  85   45129.6381      1.270  1159766.6212     1.170  744.55
