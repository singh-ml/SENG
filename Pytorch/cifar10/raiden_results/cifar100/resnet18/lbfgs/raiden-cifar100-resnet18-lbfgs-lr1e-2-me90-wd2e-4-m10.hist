Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4303856128 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7324      0.960  7.2588     0.928  8.90
   2   4.7323      0.980  7.2594     1.042  16.00
   3   4.7321      0.950  7.2577     0.992  23.32
   4   4.7323      0.960  7.2585     0.908  30.70
   5   4.7332      0.960  7.2577     0.922  38.28
   6   4.7326      1.000  7.2597     0.994  45.57
   7   4.7312      0.960  7.2587     0.926  52.79
   8   4.7301      0.940  7.2592     1.014  60.06
   9   4.7338      0.950  7.2569     0.972  67.46
  10   4.7325      0.980  7.2590     1.002  74.76
  11   4.7313      0.930  7.2598     1.000  81.99
  12   4.7330      0.960  7.2599     0.934  89.18
  13   4.7315      0.920  7.2584     0.990  96.52
  14   4.7312      0.950  7.2599     1.010  103.90
  15   4.7329      0.980  7.2578     1.016  111.14
  16   4.7347      0.960  7.2594     0.930  118.47
  17   4.7323      0.950  7.2580     1.002  125.69
  18   4.7323      0.930  7.2587     0.972  132.87
  19   4.7337      0.930  7.2592     0.950  140.25
  20   4.7314      0.950  7.2579     0.982  147.50
  21   4.7317      0.920  7.2583     1.002  154.74
  22   4.7304      0.960  7.2591     0.964  162.01
  23   4.7328      0.930  7.2593     0.950  169.35
  24   4.7335      0.970  7.2599     0.972  176.72
  25   4.7318      0.930  7.2587     0.948  183.87
  26   4.7319      0.960  7.2585     0.986  191.24
  27   4.7316      0.970  7.2596     0.944  198.53
  28   4.7328      0.920  7.2583     0.950  206.00
  29   4.7325      0.940  7.2572     1.022  213.16
  30   4.7320      1.010  7.2594     0.936  220.75
  31   4.7299      1.010  7.2575     0.916  227.95
  32   4.7300      0.950  7.2593     1.008  235.39
  33   4.7324      0.930  7.2586     0.982  242.58
  34   4.7305      0.960  7.2586     0.994  249.85
  35   4.7303      0.900  7.2583     0.982  257.03
  36   4.7333      0.980  7.2590     0.988  264.37
  37   4.7303      1.000  7.2592     0.956  271.84
  38   4.7298      0.970  7.2590     0.946  279.13
  39   4.7303      0.930  7.2588     0.974  286.38
  40   4.7344      0.990  7.2570     0.960  293.64
  41   4.7293      1.010  7.2603     1.034  300.85
  42   4.7318      0.960  7.2583     0.986  308.22
  43   4.7323      0.980  7.2599     0.992  315.48
  44   4.7311      0.940  7.2592     1.030  322.77
  45   4.7321      0.920  7.2589     0.974  329.91
  46   4.7305      0.920  7.2586     1.024  337.21
  47   4.7322      0.960  7.2576     1.026  344.56
  48   4.7305      0.950  7.2583     0.964  351.89
  49   4.7314      0.960  7.2600     0.928  359.25
  50   4.7308      0.920  7.2576     1.066  366.34
  51   4.7322      0.930  7.2588     0.972  373.57
  52   4.7319      0.910  7.2586     1.038  380.86
  53   4.7308      0.970  7.2582     0.970  388.17
  54   4.7320      0.930  7.2610     0.990  395.50
  55   4.7324      0.940  7.2582     1.002  402.70
  56   4.7329      0.920  7.2578     0.980  409.80
  57   4.7312      0.920  7.2605     0.988  417.08
  58   4.7319      1.000  7.2587     0.972  424.40
  59   4.7295      0.920  7.2578     1.000  431.70
  60   4.7335      0.930  7.2577     0.918  438.88
  61   4.7305      0.920  7.2580     1.008  446.11
  62   4.7299      0.960  7.2583     0.990  453.29
  63   4.7326      0.950  7.2576     1.000  460.61
  64   4.7322      0.970  7.2579     0.970  467.84
  65   4.7310      0.920  7.2579     0.968  475.16
  66   4.7316      0.960  7.2584     0.992  482.46
  67   4.7317      0.990  7.2590     0.990  489.88
  68   4.7310      0.940  7.2586     0.984  497.09
  69   4.7306      0.950  7.2585     0.928  504.24
  70   4.7317      0.910  7.2578     0.988  511.46
  71   4.7311      0.950  7.2590     1.016  518.74
  72   4.7309      0.970  7.2580     1.006  526.08
  73   4.7329      0.980  7.2584     0.970  533.25
  74   4.7324      0.930  7.2592     0.976  540.80
  75   4.7318      0.920  7.2593     1.012  548.12
  76   4.7321      0.960  7.2590     1.016  555.53
  77   4.7298      0.970  7.2592     0.982  562.84
  78   4.7297      0.970  7.2583     1.006  570.07
  79   4.7313      0.950  7.2590     0.972  577.19
  80   4.7326      0.960  7.2590     0.932  584.38
  81   4.7316      0.950  7.2581     0.958  591.73
  82   4.7313      0.950  7.2581     1.016  599.12
  83   4.7309      1.010  7.2599     0.952  606.37
  84   4.7305      0.960  7.2584     1.002  613.89
  85   4.7300      0.940  7.2584     0.968  621.35
  86   4.7326      0.900  7.2584     0.996  628.58
  87   4.7280      1.000  7.2582     0.908  635.96
  88   4.7317      0.940  7.2587     1.038  643.14
  89   4.7321      0.930  7.2583     1.014  650.37
  90   4.7307      0.970  7.2602     0.992  657.68
