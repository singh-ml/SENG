Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6985      0.970  7.2225     0.968  10.00
   2   4.6969      0.960  7.2212     0.952  16.98
   3   4.6986      0.970  7.2210     0.990  24.08
   4   4.6991      0.970  7.2221     0.964  31.02
   5   4.6978      0.980  7.2212     0.984  38.13
   6   4.6993      1.010  7.2222     0.972  45.16
   7   4.6991      0.970  7.2216     0.990  52.20
   8   4.6977      0.970  7.2222     0.988  59.19
   9   4.6954      0.960  7.2217     0.958  66.15
  10   4.6974      1.000  7.2210     1.010  73.43
  11   4.6973      1.000  7.2207     0.988  80.45
  12   4.6981      0.970  7.2215     1.020  87.41
  13   4.6987      0.970  7.2210     0.974  94.35
  14   4.6970      0.980  7.2214     0.986  101.36
  15   4.7003      0.990  7.2213     0.978  108.39
  16   4.6985      0.970  7.2225     0.978  115.44
  17   4.6978      0.980  7.2220     0.984  122.35
  18   4.6967      0.960  7.2203     0.998  129.26
  19   4.6962      0.990  7.2222     0.998  136.31
  20   4.6978      0.960  7.2201     0.974  143.36
  21   4.6992      0.980  7.2217     0.992  150.31
  22   4.6982      0.950  7.2215     0.970  157.36
  23   4.6991      0.960  7.2227     0.988  164.36
  24   4.6975      0.960  7.2207     0.984  171.35
  25   4.6984      0.960  7.2219     0.946  178.47
  26   4.6983      0.990  7.2218     0.970  185.41
  27   4.7004      1.010  7.2206     0.958  192.41
  28   4.6986      0.960  7.2226     0.976  199.41
  29   4.6979      0.980  7.2221     0.982  206.48
  30   4.6970      1.010  7.2233     0.950  213.55
  31   4.6984      0.980  7.2215     1.010  220.68
  32   4.6975      0.980  7.2214     0.984  227.67
  33   4.6980      0.980  7.2211     1.000  234.75
  34   4.6979      1.000  7.2221     1.004  241.75
  35   4.6979      0.970  7.2227     0.924  249.02
  36   4.7000      0.960  7.2216     0.944  256.07
  37   4.7002      0.980  7.2213     0.982  263.00
  38   4.6986      0.980  7.2220     0.948  269.98
  39   4.7009      0.980  7.2223     0.996  276.88
  40   4.6984      1.000  7.2214     0.998  283.83
  41   4.6960      1.000  7.2214     0.962  290.84
  42   4.6981      0.990  7.2217     0.964  297.93
  43   4.6981      0.980  7.2221     0.976  304.93
  44   4.6983      1.000  7.2208     0.986  312.03
  45   4.6983      1.000  7.2220     0.962  319.25
  46   4.7013      0.990  7.2215     0.954  326.18
  47   4.6983      0.980  7.2226     1.042  333.21
  48   4.6978      0.980  7.2214     0.996  340.20
  49   4.6999      0.940  7.2219     0.996  347.30
  50   4.6973      0.970  7.2204     0.962  354.37
  51   4.6982      0.960  7.2216     0.940  361.50
  52   4.6992      1.010  7.2219     1.012  368.58
  53   4.7005      0.980  7.2221     0.958  375.62
  54   4.6991      1.000  7.2218     1.030  382.57
  55   4.6982      0.970  7.2210     0.918  389.57
  56   4.6991      1.000  7.2210     0.996  396.66
  57   4.6990      0.960  7.2219     0.974  403.65
  58   4.6987      0.950  7.2220     0.946  410.65
  59   4.6978      0.960  7.2210     0.966  417.52
  60   4.7004      0.980  7.2218     0.974  424.56
  61   4.6986      0.980  7.2217     0.998  431.70
  62   4.6985      1.020  7.2215     0.970  438.71
  63   4.6993      1.020  7.2218     0.964  445.80
  64   4.6972      1.000  7.2208     0.992  452.78
  65   4.6957      0.980  7.2216     0.974  459.81
  66   4.6986      0.960  7.2212     0.978  466.86
  67   4.6963      0.960  7.2221     0.974  473.84
  68   4.6978      0.940  7.2217     0.984  480.76
  69   4.6990      0.940  7.2218     0.970  487.82
  70   4.6984      0.960  7.2222     1.000  494.83
  71   4.6993      0.970  7.2207     1.004  501.74
  72   4.6969      1.000  7.2213     0.948  508.73
  73   4.6985      0.990  7.2215     0.966  515.72
  74   4.6970      0.990  7.2233     0.940  522.76
  75   4.6988      0.970  7.2216     0.968  529.96
  76   4.6983      0.960  7.2223     0.960  537.09
  77   4.6992      0.990  7.2204     0.974  544.15
  78   4.6990      0.950  7.2222     0.994  551.21
  79   4.6992      0.990  7.2209     0.988  558.30
  80   4.6989      0.970  7.2207     0.986  565.31
  81   4.6985      1.000  7.2216     1.012  572.30
  82   4.6981      0.990  7.2217     0.974  579.39
  83   4.6996      0.960  7.2206     0.996  586.34
  84   4.6991      0.970  7.2208     0.974  593.41
  85   4.6969      0.960  7.2210     0.992  600.45
