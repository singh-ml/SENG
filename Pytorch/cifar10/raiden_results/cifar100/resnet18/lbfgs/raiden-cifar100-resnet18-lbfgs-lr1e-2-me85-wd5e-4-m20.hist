Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6996      0.980  10.9759     0.990  8.71
   2   4.6999      0.970  10.9757     0.976  15.90
   3   4.7004      0.970  10.9755     0.972  23.14
   4   4.6989      0.980  10.9757     0.998  30.43
   5   4.6975      0.980  10.9760     0.968  37.74
   6   4.6983      0.980  10.9764     0.980  45.10
   7   4.6980      0.980  10.9760     0.998  52.41
   8   4.7006      0.990  10.9740     0.988  59.84
   9   4.7004      0.970  10.9755     0.966  67.13
  10   4.7006      0.980  10.9765     0.964  74.44
  11   4.6999      0.980  10.9764     0.988  81.59
  12   4.7000      0.990  10.9755     0.976  88.72
  13   4.6982      0.980  10.9759     0.968  95.98
  14   4.6989      0.980  10.9755     0.976  103.22
  15   4.6997      0.970  10.9748     0.984  110.51
  16   4.6993      0.980  10.9758     0.986  117.62
  17   4.6990      0.980  10.9766     0.974  124.84
  18   4.7001      0.970  10.9761     0.996  132.31
  19   4.6994      0.970  10.9761     0.982  139.52
  20   4.6997      0.980  10.9767     0.966  146.66
  21   4.6992      0.980  10.9754     0.972  153.91
  22   4.6983      0.980  10.9749     0.994  161.17
  23   4.6986      0.980  10.9756     0.972  168.51
  24   4.6986      0.980  10.9759     0.984  175.75
  25   4.6973      0.990  10.9762     0.986  182.98
  26   4.6988      0.980  10.9759     0.982  190.20
  27   4.6995      0.990  10.9755     0.978  197.34
  28   4.6998      0.980  10.9758     0.974  204.67
  29   4.6990      0.970  10.9754     0.966  211.87
  30   4.7000      0.970  10.9763     1.006  219.12
  31   4.6997      0.990  10.9758     0.982  226.40
  32   4.6983      0.980  10.9764     0.984  233.72
  33   4.6992      0.970  10.9767     0.974  241.09
  34   4.6998      0.970  10.9772     0.986  248.33
  35   4.7000      0.970  10.9755     0.988  255.64
  36   4.6997      0.970  10.9752     0.984  262.92
  37   4.6997      0.980  10.9756     0.982  270.17
  38   4.6975      0.970  10.9768     0.992  277.58
  39   4.6994      0.970  10.9751     0.974  284.82
  40   4.6995      0.980  10.9758     0.980  292.02
  41   4.6979      0.980  10.9755     0.986  299.32
  42   4.6978      0.980  10.9767     0.992  306.58
  43   4.7003      0.970  10.9765     0.976  314.01
  44   4.6982      0.990  10.9756     0.982  321.26
  45   4.6993      0.970  10.9762     0.986  328.50
  46   4.7000      0.980  10.9764     0.984  335.72
  47   4.6983      0.980  10.9750     0.978  343.21
  48   4.6993      0.980  10.9755     0.978  350.68
  49   4.6991      0.980  10.9767     0.972  357.98
  50   4.7005      0.970  10.9762     0.962  365.26
  51   4.6979      0.980  10.9760     0.986  372.57
  52   4.6993      0.980  10.9761     0.978  379.89
  53   4.6995      0.980  10.9770     0.976  387.18
  54   4.6982      0.990  10.9752     0.974  394.46
  55   4.6986      0.990  10.9764     0.992  401.68
  56   4.6992      0.980  10.9769     0.968  408.94
  57   4.6988      0.970  10.9766     0.998  416.17
  58   4.6994      0.990  10.9758     0.976  423.53
  59   4.6989      0.980  10.9769     0.980  430.89
  60   4.6980      0.980  10.9754     0.978  438.10
  61   4.6988      0.990  10.9764     0.988  445.29
  62   4.6959      0.980  10.9767     0.982  452.49
  63   4.6992      0.980  10.9763     0.994  459.74
  64   4.6982      0.980  10.9766     0.984  467.02
  65   4.7000      0.970  10.9768     0.974  474.34
  66   4.6990      0.990  10.9768     0.980  481.55
  67   4.6991      0.980  10.9747     0.980  488.74
  68   4.6981      0.980  10.9764     0.970  495.99
  69   4.6978      0.990  10.9754     1.002  503.19
  70   4.6991      0.990  10.9759     0.988  510.43
  71   4.7001      0.980  10.9761     0.982  517.74
  72   4.6994      0.990  10.9767     0.996  525.06
  73   4.7005      0.970  10.9761     0.970  532.40
  74   4.6988      0.980  10.9760     0.988  539.62
  75   4.6979      0.980  10.9748     0.990  546.87
  76   4.6986      0.980  10.9758     0.968  554.14
  77   4.6989      0.990  10.9760     0.976  561.45
  78   4.7000      0.970  10.9751     0.976  568.88
  79   4.6991      0.980  10.9763     0.972  576.11
  80   4.6979      0.970  10.9759     0.972  583.34
  81   4.6979      0.980  10.9764     0.998  590.51
  82   4.6996      0.980  10.9754     0.986  597.87
  83   4.6995      0.980  10.9753     0.974  605.08
  84   4.6997      0.990  10.9755     0.976  612.27
  85   4.6995      0.970  10.9769     0.958  619.60
