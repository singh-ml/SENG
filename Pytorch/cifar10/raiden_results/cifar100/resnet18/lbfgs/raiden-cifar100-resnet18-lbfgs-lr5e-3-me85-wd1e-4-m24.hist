Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7241      0.570  6.0001     0.782  8.48
   2   4.7247      0.560  5.9981     0.850  15.53
   3   4.7252      0.550  5.9975     0.782  22.59
   4   4.7237      0.560  5.9971     0.818  29.66
   5   4.7260      0.590  5.9987     0.804  36.66
   6   4.7231      0.550  5.9980     0.846  43.77
   7   4.7248      0.590  5.9987     0.728  50.81
   8   4.7252      0.560  5.9989     0.824  57.74
   9   4.7242      0.570  5.9985     0.858  64.73
  10   4.7223      0.590  5.9979     0.832  71.76
  11   4.7244      0.570  5.9986     0.852  78.71
  12   4.7229      0.570  5.9983     0.820  85.94
  13   4.7231      0.550  5.9976     0.788  93.01
  14   4.7238      0.570  5.9992     0.796  100.06
  15   4.7228      0.570  5.9976     0.792  107.08
  16   4.7222      0.580  5.9992     0.794  114.09
  17   4.7254      0.570  5.9997     0.846  121.11
  18   4.7235      0.570  5.9990     0.766  128.11
  19   4.7261      0.570  5.9983     0.772  135.18
  20   4.7255      0.600  5.9974     0.794  142.23
  21   4.7255      0.560  5.9997     0.754  149.39
  22   4.7255      0.550  5.9985     0.770  156.37
  23   4.7250      0.560  5.9981     0.852  163.47
  24   4.7243      0.570  5.9981     0.820  170.53
  25   4.7232      0.580  5.9988     0.770  177.45
  26   4.7232      0.570  5.9982     0.856  184.52
  27   4.7224      0.560  5.9987     0.774  191.48
  28   4.7241      0.580  5.9987     0.816  198.51
  29   4.7249      0.580  5.9990     0.734  205.62
  30   4.7235      0.600  5.9980     0.778  212.61
  31   4.7257      0.580  5.9979     0.828  219.76
  32   4.7255      0.580  5.9985     0.816  226.86
  33   4.7239      0.570  5.9990     0.846  233.93
  34   4.7253      0.590  5.9986     0.808  240.92
  35   4.7244      0.610  5.9979     0.834  248.00
  36   4.7235      0.560  5.9992     0.808  255.03
  37   4.7249      0.580  5.9987     0.804  262.04
  38   4.7240      0.560  5.9993     0.866  269.08
  39   4.7242      0.560  5.9974     0.810  276.05
  40   4.7238      0.600  5.9992     0.824  283.16
  41   4.7222      0.590  5.9989     0.834  290.27
  42   4.7232      0.590  5.9989     0.854  297.24
  43   4.7238      0.560  5.9983     0.828  304.30
  44   4.7240      0.580  5.9979     0.828  311.25
  45   4.7242      0.570  5.9990     0.776  318.33
  46   4.7223      0.610  5.9983     0.828  325.44
  47   4.7238      0.580  5.9986     0.832  332.45
  48   4.7237      0.570  5.9989     0.802  339.44
  49   4.7232      0.560  5.9985     0.770  346.54
  50   4.7245      0.560  5.9976     0.826  353.63
  51   4.7231      0.580  5.9984     0.808  360.77
  52   4.7249      0.590  5.9979     0.782  367.87
  53   4.7230      0.550  5.9969     0.836  374.83
  54   4.7217      0.560  5.9981     0.768  381.79
  55   4.7237      0.590  5.9979     0.750  388.91
  56   4.7245      0.580  5.9990     0.830  395.85
  57   4.7256      0.600  5.9977     0.810  402.93
  58   4.7232      0.580  5.9990     0.748  409.97
  59   4.7227      0.590  5.9980     0.820  416.90
  60   4.7253      0.590  5.9981     0.814  423.97
  61   4.7235      0.590  5.9982     0.826  430.89
  62   4.7244      0.550  5.9988     0.834  437.82
  63   4.7239      0.580  5.9978     0.814  444.89
  64   4.7255      0.580  5.9975     0.828  451.93
  65   4.7233      0.590  5.9992     0.842  459.17
  66   4.7225      0.590  5.9982     0.772  466.16
  67   4.7248      0.570  5.9983     0.830  473.10
  68   4.7233      0.570  5.9985     0.804  479.99
  69   4.7229      0.570  5.9988     0.818  487.01
  70   4.7229      0.540  5.9985     0.776  494.18
  71   4.7255      0.590  5.9984     0.784  501.30
  72   4.7238      0.570  5.9986     0.824  508.21
  73   4.7237      0.580  5.9976     0.848  515.27
  74   4.7235      0.580  5.9981     0.802  522.42
  75   4.7222      0.550  5.9983     0.792  529.54
  76   4.7229      0.580  5.9975     0.824  536.55
  77   4.7260      0.580  5.9992     0.792  543.52
  78   4.7254      0.570  5.9973     0.778  550.59
  79   4.7238      0.580  5.9979     0.820  557.66
  80   4.7229      0.560  6.0001     0.758  564.78
  81   4.7237      0.570  5.9981     0.818  571.70
  82   4.7240      0.570  5.9972     0.824  578.73
  83   4.7233      0.580  5.9989     0.814  585.83
  84   4.7245      0.590  5.9980     0.854  592.73
  85   4.7239      0.580  5.9977     0.848  599.84
