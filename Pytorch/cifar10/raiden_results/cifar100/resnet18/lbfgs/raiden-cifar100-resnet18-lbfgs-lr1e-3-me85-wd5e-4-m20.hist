Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7122      0.960  10.9851     0.996  8.51
   2   4.7191      1.000  10.9848     0.994  15.58
   3   4.7138      0.940  10.9855     1.020  22.65
   4   4.7134      0.980  10.9845     0.998  29.53
   5   4.7141      0.980  10.9843     0.982  36.71
   6   4.7140      0.960  10.9843     0.994  43.65
   7   4.7137      0.980  10.9845     1.020  50.60
   8   4.7172      0.950  10.9849     0.978  57.36
   9   4.7154      1.000  10.9868     0.956  64.24
  10   4.7166      0.950  10.9858     1.018  71.05
  11   4.7159      1.000  10.9850     0.988  77.93
  12   4.7152      1.000  10.9847     1.012  84.79
  13   4.7138      0.990  10.9850     0.980  91.60
  14   4.7144      1.000  10.9849     0.980  98.48
  15   4.7138      1.000  10.9857     0.970  105.42
  16   4.7119      1.020  10.9848     1.004  112.36
  17   4.7153      0.970  10.9844     0.980  119.13
  18   4.7136      1.000  10.9846     0.980  125.86
  19   4.7125      1.000  10.9855     0.982  132.79
  20   4.7135      1.000  10.9855     0.972  139.52
  21   4.7128      0.940  10.9843     0.984  146.42
  22   4.7146      1.000  10.9852     1.002  153.27
  23   4.7130      1.000  10.9850     1.010  159.99
  24   4.7161      0.970  10.9852     0.974  166.80
  25   4.7169      0.990  10.9853     0.948  173.57
  26   4.7158      1.020  10.9842     0.978  180.42
  27   4.7123      1.000  10.9855     1.014  187.26
  28   4.7145      0.990  10.9850     1.006  194.08
  29   4.7141      0.980  10.9851     0.978  201.01
  30   4.7145      0.990  10.9854     0.958  207.69
  31   4.7136      1.000  10.9856     1.030  214.69
  32   4.7159      0.990  10.9860     0.964  221.48
  33   4.7133      0.980  10.9855     0.966  228.28
  34   4.7167      0.990  10.9848     0.986  235.14
  35   4.7124      0.990  10.9844     1.006  241.96
  36   4.7132      0.980  10.9856     0.994  248.96
  37   4.7174      0.970  10.9854     0.960  255.80
  38   4.7162      0.970  10.9847     0.970  262.52
  39   4.7137      0.990  10.9855     0.994  269.27
  40   4.7134      0.960  10.9847     1.022  276.08
  41   4.7154      0.980  10.9853     0.976  282.91
  42   4.7161      0.980  10.9847     0.990  289.61
  43   4.7172      0.980  10.9858     0.972  296.50
  44   4.7133      0.970  10.9845     0.968  303.27
  45   4.7137      0.960  10.9853     0.946  310.05
  46   4.7121      0.980  10.9846     1.002  317.00
  47   4.7145      0.990  10.9851     1.008  324.11
  48   4.7120      1.000  10.9854     0.988  330.92
  49   4.7144      0.990  10.9840     0.982  337.75
  50   4.7138      0.970  10.9850     0.988  344.77
  51   4.7121      0.980  10.9852     0.968  351.61
  52   4.7177      1.000  10.9849     0.964  358.47
  53   4.7144      0.970  10.9852     0.954  365.30
  54   4.7135      0.970  10.9849     1.038  372.11
  55   4.7129      0.980  10.9853     1.002  378.97
  56   4.7133      1.000  10.9844     0.980  385.84
  57   4.7156      0.970  10.9855     0.990  392.59
  58   4.7147      0.990  10.9847     0.988  399.41
  59   4.7153      1.010  10.9849     0.956  406.24
  60   4.7159      0.980  10.9854     1.048  413.06
  61   4.7145      0.980  10.9853     1.000  419.98
  62   4.7153      1.010  10.9850     0.974  426.79
  63   4.7148      1.050  10.9852     0.982  433.68
  64   4.7151      1.000  10.9848     0.994  440.61
  65   4.7117      0.990  10.9838     1.010  447.52
  66   4.7123      0.970  10.9858     0.990  454.44
  67   4.7159      0.990  10.9846     0.970  461.34
  68   4.7142      0.970  10.9859     0.976  468.10
  69   4.7119      0.980  10.9852     1.022  474.96
  70   4.7169      1.000  10.9843     0.992  481.66
  71   4.7169      0.970  10.9851     0.950  488.62
  72   4.7115      0.990  10.9845     0.996  495.48
  73   4.7133      0.970  10.9860     0.962  502.29
  74   4.7140      0.960  10.9850     0.988  509.09
  75   4.7111      0.980  10.9851     0.962  516.01
  76   4.7127      1.000  10.9849     0.992  522.96
  77   4.7145      1.010  10.9836     1.022  529.74
  78   4.7146      1.000  10.9855     1.008  536.62
  79   4.7154      0.970  10.9847     1.006  543.47
  80   4.7137      1.010  10.9854     0.954  550.34
  81   4.7156      0.990  10.9842     1.002  557.30
  82   4.7129      0.990  10.9844     0.976  564.15
  83   4.7155      1.000  10.9847     0.994  570.96
  84   4.7149      1.000  10.9847     1.054  577.87
  85   4.7157      0.970  10.9854     0.980  584.69
