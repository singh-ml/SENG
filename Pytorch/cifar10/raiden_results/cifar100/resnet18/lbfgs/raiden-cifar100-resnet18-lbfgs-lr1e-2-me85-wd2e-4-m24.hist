Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6989      0.950  7.2195     0.802  8.90
   2   4.6975      0.920  7.2190     0.894  16.16
   3   4.6982      0.970  7.2186     0.858  23.36
   4   4.6979      0.950  7.2190     0.872  30.72
   5   4.6990      0.950  7.2180     0.818  37.95
   6   4.6987      0.910  7.2182     0.908  45.09
   7   4.6971      0.950  7.2184     0.898  52.50
   8   4.6976      0.970  7.2186     0.908  59.74
   9   4.6990      0.980  7.2185     0.836  67.01
  10   4.6978      0.980  7.2175     0.866  74.26
  11   4.6965      0.980  7.2182     0.936  81.74
  12   4.6993      0.950  7.2184     0.900  89.08
  13   4.6997      0.960  7.2180     0.858  96.36
  14   4.6990      0.980  7.2172     0.890  103.47
  15   4.6971      0.900  7.2192     0.882  110.71
  16   4.6957      0.970  7.2182     0.856  118.13
  17   4.6986      0.940  7.2194     0.882  125.32
  18   4.6977      0.920  7.2186     0.868  132.65
  19   4.6987      0.980  7.2180     0.886  139.74
  20   4.6995      0.940  7.2187     0.858  147.04
  21   4.6983      0.990  7.2199     0.822  154.37
  22   4.6987      0.950  7.2186     0.908  161.55
  23   4.6972      0.910  7.2184     0.876  168.73
  24   4.6977      0.970  7.2191     0.850  175.97
  25   4.6967      0.940  7.2180     0.874  183.25
  26   4.6987      0.970  7.2194     0.832  190.75
  27   4.7009      0.950  7.2194     0.834  197.88
  28   4.6958      0.940  7.2184     0.834  205.00
  29   4.6984      0.910  7.2178     0.904  212.22
  30   4.6969      0.870  7.2180     0.882  219.46
  31   4.6967      0.950  7.2202     0.886  226.67
  32   4.6986      0.950  7.2196     0.880  233.99
  33   4.6986      0.990  7.2193     0.862  241.26
  34   4.6973      0.880  7.2190     0.864  248.45
  35   4.6981      0.880  7.2186     0.912  255.73
  36   4.6971      0.870  7.2188     0.914  263.10
  37   4.6989      0.980  7.2183     0.932  270.21
  38   4.6968      0.920  7.2185     0.900  277.49
  39   4.6968      0.910  7.2193     0.880  284.73
  40   4.6980      0.920  7.2185     0.850  291.99
  41   4.6992      0.960  7.2178     0.934  299.38
  42   4.6999      0.980  7.2193     0.860  306.70
  43   4.7017      0.910  7.2182     0.918  313.89
  44   4.6978      0.940  7.2189     0.852  321.47
  45   4.6987      0.990  7.2187     0.926  328.71
  46   4.6971      0.930  7.2188     0.782  336.13
  47   4.6998      0.940  7.2203     0.876  343.30
  48   4.6972      0.940  7.2178     0.842  350.64
  49   4.6988      1.000  7.2180     0.852  357.85
  50   4.6978      0.960  7.2186     0.904  364.96
  51   4.6991      0.940  7.2184     0.904  372.31
  52   4.6989      0.910  7.2162     0.924  379.58
  53   4.7017      0.940  7.2182     0.872  386.73
  54   4.6999      0.970  7.2182     0.922  393.82
  55   4.6976      0.970  7.2188     0.892  400.96
  56   4.6985      0.970  7.2190     0.854  408.19
  57   4.6971      0.960  7.2185     0.878  415.39
  58   4.6988      0.970  7.2178     0.912  422.63
  59   4.6980      0.970  7.2182     0.870  429.82
  60   4.6949      0.880  7.2187     0.888  437.09
  61   4.7004      0.950  7.2182     0.870  444.31
  62   4.6987      0.940  7.2184     0.872  451.65
  63   4.7007      0.920  7.2188     0.860  458.92
  64   4.6965      0.930  7.2191     0.826  466.09
  65   4.7007      0.970  7.2185     0.882  473.24
  66   4.6976      0.940  7.2181     0.916  480.43
  67   4.6981      0.960  7.2191     0.826  487.64
  68   4.7014      0.960  7.2182     0.904  494.81
  69   4.6990      1.000  7.2177     0.850  501.99
  70   4.6995      0.930  7.2182     0.914  509.15
  71   4.6967      0.940  7.2185     0.854  516.47
  72   4.6975      0.910  7.2185     0.846  523.91
  73   4.6993      0.990  7.2196     0.860  531.06
  74   4.6955      0.940  7.2187     0.886  538.22
  75   4.6976      0.930  7.2187     0.916  545.40
  76   4.6998      0.930  7.2187     0.902  552.78
  77   4.6997      0.970  7.2194     0.932  560.08
  78   4.6970      0.950  7.2178     0.888  567.25
  79   4.6955      0.920  7.2174     0.860  574.39
  80   4.6974      0.900  7.2191     0.876  581.57
  81   4.6965      0.880  7.2172     0.894  588.99
  82   4.6988      0.900  7.2174     0.882  596.16
  83   4.6971      0.930  7.2183     0.876  603.40
  84   4.6947      0.920  7.2195     0.940  610.53
  85   4.6986      0.970  7.2183     0.860  617.57
