Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7201      0.940  10.9933     0.982  8.69
   2   4.7193      1.000  10.9933     0.976  15.99
   3   4.7208      0.950  10.9922     1.022  23.52
   4   4.7199      0.990  10.9938     1.012  30.80
   5   4.7186      0.980  10.9927     0.970  38.26
   6   4.7195      0.980  10.9932     0.996  45.40
   7   4.7206      0.990  10.9928     1.038  52.70
   8   4.7198      0.940  10.9938     0.994  60.02
   9   4.7192      0.970  10.9935     0.964  67.35
  10   4.7184      0.990  10.9941     0.994  74.64
  11   4.7195      0.980  10.9925     1.030  82.16
  12   4.7182      0.970  10.9934     1.046  89.42
  13   4.7199      0.970  10.9929     1.010  96.61
  14   4.7210      0.990  10.9927     0.986  104.15
  15   4.7224      0.980  10.9933     1.008  111.51
  16   4.7204      0.940  10.9936     1.024  118.63
  17   4.7215      0.990  10.9928     1.002  125.88
  18   4.7205      0.980  10.9931     0.982  133.21
  19   4.7199      1.000  10.9936     0.988  140.65
  20   4.7178      0.980  10.9933     0.998  147.86
  21   4.7195      1.000  10.9923     0.990  155.02
  22   4.7188      0.950  10.9929     1.036  162.23
  23   4.7201      0.960  10.9927     1.074  169.40
  24   4.7203      0.970  10.9938     0.986  176.67
  25   4.7202      1.000  10.9926     1.002  183.96
  26   4.7193      0.990  10.9931     0.974  191.09
  27   4.7211      0.980  10.9925     1.020  198.24
  28   4.7182      0.980  10.9932     0.990  205.53
  29   4.7196      0.980  10.9926     0.992  212.87
  30   4.7185      0.970  10.9925     0.980  220.38
  31   4.7193      0.960  10.9933     0.996  227.68
  32   4.7210      0.960  10.9935     0.974  234.86
  33   4.7202      1.000  10.9936     0.994  242.18
  34   4.7226      0.980  10.9933     1.034  249.62
  35   4.7182      0.990  10.9944     0.972  256.93
  36   4.7213      0.980  10.9930     0.934  264.25
  37   4.7195      0.990  10.9932     1.012  271.52
  38   4.7204      0.970  10.9938     0.984  278.81
  39   4.7201      1.010  10.9935     1.036  286.25
  40   4.7201      0.950  10.9932     1.030  293.58
  41   4.7198      0.980  10.9935     0.926  300.77
  42   4.7198      0.990  10.9934     0.986  307.97
  43   4.7203      0.950  10.9926     1.072  315.26
  44   4.7207      0.930  10.9934     0.966  322.48
  45   4.7214      1.010  10.9930     0.956  329.76
  46   4.7201      1.010  10.9928     1.020  337.11
  47   4.7211      0.940  10.9944     0.992  344.46
  48   4.7181      0.980  10.9915     0.998  351.72
  49   4.7178      0.970  10.9920     0.980  359.09
  50   4.7203      0.970  10.9921     0.994  366.33
  51   4.7177      1.010  10.9934     0.974  373.53
  52   4.7204      0.970  10.9926     1.016  380.69
  53   4.7188      0.980  10.9920     1.006  387.91
  54   4.7200      0.940  10.9939     0.984  395.13
  55   4.7189      0.980  10.9938     1.018  402.44
  56   4.7166      0.950  10.9938     1.006  409.62
  57   4.7160      0.970  10.9942     0.948  416.74
  58   4.7209      0.990  10.9923     1.016  424.09
  59   4.7185      0.970  10.9944     1.056  431.53
  60   4.7184      0.980  10.9919     1.004  438.96
  61   4.7209      0.990  10.9927     0.998  446.12
  62   4.7202      0.990  10.9933     0.956  453.44
  63   4.7210      0.970  10.9942     0.982  460.60
  64   4.7180      0.980  10.9931     0.970  468.07
  65   4.7221      1.000  10.9928     1.026  475.30
  66   4.7173      0.990  10.9929     1.008  482.58
  67   4.7181      0.960  10.9930     1.036  489.85
  68   4.7175      0.950  10.9932     1.036  497.22
  69   4.7185      0.980  10.9934     1.006  504.45
  70   4.7202      0.960  10.9928     1.030  511.83
  71   4.7216      1.000  10.9924     1.012  519.07
  72   4.7189      1.010  10.9928     0.980  526.42
  73   4.7201      1.010  10.9930     0.956  533.78
  74   4.7189      0.990  10.9928     1.028  541.05
  75   4.7190      1.010  10.9928     1.004  548.36
  76   4.7186      0.980  10.9930     0.970  555.51
  77   4.7183      0.960  10.9925     0.992  562.80
  78   4.7196      0.990  10.9926     0.976  570.22
  79   4.7222      1.000  10.9934     0.958  577.57
  80   4.7211      0.990  10.9927     0.996  584.83
  81   4.7200      0.930  10.9921     0.996  592.18
  82   4.7204      0.990  10.9920     0.994  599.52
  83   4.7180      0.990  10.9936     0.998  606.80
  84   4.7223      0.990  10.9928     0.970  614.02
  85   4.7206      0.940  10.9934     0.964  621.30
  86   4.7185      0.980  10.9925     0.990  628.57
  87   4.7228      0.990  10.9936     0.972  635.76
  88   4.7202      0.990  10.9924     1.008  643.16
  89   4.7192      1.000  10.9929     1.006  650.49
  90   4.7187      0.970  10.9929     1.012  657.83
