Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7002      0.780  5.9711     0.854  8.62
   2   4.7022      0.790  5.9732     0.828  15.71
   3   4.7032      0.790  5.9709     0.954  22.68
   4   4.7024      0.760  5.9715     0.942  29.68
   5   4.7017      0.760  5.9712     0.906  36.69
   6   4.7019      0.750  5.9723     0.872  43.60
   7   4.7036      0.740  5.9718     0.858  50.52
   8   4.7029      0.760  5.9728     0.922  57.59
   9   4.7038      0.730  5.9703     0.866  64.69
  10   4.7040      0.780  5.9713     0.898  71.57
  11   4.7042      0.760  5.9721     0.954  78.78
  12   4.7024      0.720  5.9704     0.900  85.81
  13   4.7011      0.800  5.9725     0.864  92.87
  14   4.7023      0.770  5.9722     0.850  100.00
  15   4.7022      0.810  5.9715     0.900  107.05
  16   4.7030      0.770  5.9711     0.938  114.24
  17   4.7032      0.740  5.9720     0.848  121.33
  18   4.7019      0.790  5.9719     0.904  128.59
  19   4.7010      0.810  5.9719     0.886  135.64
  20   4.7016      0.790  5.9718     0.920  142.62
  21   4.7033      0.750  5.9723     0.884  149.74
  22   4.7032      0.740  5.9718     0.916  156.71
  23   4.7031      0.770  5.9715     0.870  163.88
  24   4.7022      0.750  5.9714     0.872  170.79
  25   4.6999      0.750  5.9728     0.844  177.72
  26   4.7024      0.750  5.9724     0.874  184.78
  27   4.7016      0.770  5.9723     0.912  191.82
  28   4.7015      0.760  5.9718     0.862  198.81
  29   4.7022      0.810  5.9724     0.902  205.80
  30   4.7020      0.800  5.9713     0.840  212.76
  31   4.7017      0.800  5.9714     0.822  219.95
  32   4.7036      0.750  5.9713     0.888  226.92
  33   4.7025      0.770  5.9698     0.930  233.95
  34   4.7021      0.770  5.9714     0.876  240.80
  35   4.7026      0.740  5.9712     0.956  247.79
  36   4.7029      0.760  5.9713     0.884  254.98
  37   4.7017      0.760  5.9724     0.892  261.92
  38   4.7027      0.750  5.9720     0.848  268.95
  39   4.7004      0.800  5.9717     0.822  275.98
  40   4.7008      0.740  5.9710     0.852  282.96
  41   4.7023      0.800  5.9702     0.908  290.09
  42   4.7032      0.720  5.9708     0.922  297.13
  43   4.7025      0.800  5.9719     0.906  304.00
  44   4.7038      0.780  5.9715     0.904  310.99
  45   4.7017      0.800  5.9712     0.984  317.86
  46   4.7013      0.780  5.9701     0.860  324.89
  47   4.7016      0.820  5.9721     0.850  332.00
  48   4.7015      0.770  5.9721     0.944  338.98
  49   4.7025      0.760  5.9718     0.854  345.98
  50   4.7032      0.760  5.9716     0.944  352.93
  51   4.7037      0.740  5.9716     0.960  360.20
  52   4.7028      0.780  5.9729     0.900  367.30
  53   4.7038      0.800  5.9708     0.950  374.31
  54   4.7039      0.730  5.9718     0.884  381.36
  55   4.7025      0.770  5.9721     0.848  388.36
  56   4.7027      0.790  5.9720     0.868  395.31
  57   4.7014      0.800  5.9726     0.826  402.43
  58   4.7028      0.770  5.9711     0.958  409.37
  59   4.7020      0.750  5.9718     0.874  416.24
  60   4.7032      0.780  5.9700     0.870  423.23
  61   4.7022      0.770  5.9713     0.842  430.16
  62   4.7010      0.780  5.9714     0.852  437.56
  63   4.7017      0.810  5.9721     0.858  444.47
  64   4.7028      0.750  5.9721     0.938  451.42
  65   4.7019      0.810  5.9711     0.906  458.47
  66   4.7031      0.750  5.9725     0.974  465.58
  67   4.7020      0.780  5.9726     0.840  472.74
  68   4.7028      0.780  5.9721     0.868  479.78
  69   4.7037      0.750  5.9707     0.906  486.84
  70   4.7024      0.780  5.9730     0.860  493.85
  71   4.7021      0.810  5.9718     0.832  500.81
  72   4.7034      0.720  5.9702     0.882  507.94
  73   4.7041      0.760  5.9708     0.784  514.96
  74   4.7017      0.780  5.9724     0.938  521.97
  75   4.7024      0.750  5.9709     0.952  528.99
  76   4.7015      0.760  5.9713     0.862  535.95
  77   4.7018      0.770  5.9713     0.824  542.94
  78   4.7032      0.770  5.9720     0.860  550.07
  79   4.7007      0.810  5.9724     0.846  557.11
  80   4.7033      0.740  5.9708     0.840  564.09
  81   4.7023      0.810  5.9725     0.874  570.99
  82   4.7023      0.820  5.9736     0.820  578.13
  83   4.7027      0.750  5.9715     0.960  585.18
  84   4.7027      0.780  5.9718     1.006  592.24
  85   4.7029      0.770  5.9713     0.840  599.34
  86   4.7040      0.780  5.9724     0.884  606.27
  87   4.7035      0.730  5.9726     0.878  613.37
  88   4.7031      0.740  5.9712     0.902  620.38
  89   4.7021      0.780  5.9725     0.918  627.47
  90   4.7023      0.790  5.9708     0.902  634.46
