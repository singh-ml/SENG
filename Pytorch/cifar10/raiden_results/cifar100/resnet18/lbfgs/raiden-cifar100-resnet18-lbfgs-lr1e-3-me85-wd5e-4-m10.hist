Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239409664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7130      0.940  10.9855     0.966  8.59
   2   4.7149      0.960  10.9839     0.980  15.42
   3   4.7115      0.940  10.9867     0.976  22.57
   4   4.7117      0.970  10.9849     0.954  29.52
   5   4.7124      0.950  10.9855     0.932  36.36
   6   4.7135      0.970  10.9850     0.936  43.20
   7   4.7120      0.920  10.9856     0.958  50.08
   8   4.7115      0.940  10.9850     0.932  56.89
   9   4.7119      0.950  10.9851     0.922  63.88
  10   4.7127      0.950  10.9857     0.952  70.61
  11   4.7116      0.940  10.9856     0.934  77.46
  12   4.7115      0.940  10.9851     0.956  84.29
  13   4.7137      0.950  10.9853     0.956  91.19
  14   4.7113      0.930  10.9844     0.924  98.06
  15   4.7124      0.950  10.9855     0.970  105.05
  16   4.7117      0.950  10.9852     1.004  111.88
  17   4.7125      0.970  10.9846     0.956  118.81
  18   4.7129      0.950  10.9857     0.964  125.74
  19   4.7118      0.930  10.9854     0.942  132.70
  20   4.7101      0.930  10.9842     0.954  139.69
  21   4.7117      0.950  10.9845     0.982  146.73
  22   4.7130      0.930  10.9857     0.982  153.55
  23   4.7123      0.950  10.9846     0.988  160.44
  24   4.7134      0.960  10.9851     0.946  167.35
  25   4.7125      0.950  10.9852     0.970  174.21
  26   4.7127      0.940  10.9838     0.964  181.12
  27   4.7115      0.960  10.9856     0.906  188.03
  28   4.7133      0.960  10.9856     0.964  194.87
  29   4.7120      0.940  10.9849     0.968  201.85
  30   4.7116      0.950  10.9841     0.956  208.70
  31   4.7140      0.980  10.9856     0.936  215.55
  32   4.7120      0.940  10.9852     0.954  222.37
  33   4.7134      0.960  10.9847     0.928  229.32
  34   4.7120      0.930  10.9856     0.930  236.28
  35   4.7115      0.930  10.9858     0.962  243.19
  36   4.7125      0.960  10.9845     0.958  249.98
  37   4.7121      0.940  10.9856     0.996  256.89
  38   4.7134      0.930  10.9847     0.952  263.85
  39   4.7130      0.940  10.9860     0.956  270.69
  40   4.7112      0.940  10.9848     0.958  277.59
  41   4.7118      0.960  10.9859     0.954  284.52
  42   4.7121      0.940  10.9852     0.936  291.41
  43   4.7135      0.950  10.9857     0.898  298.28
  44   4.7107      0.950  10.9863     0.994  305.38
  45   4.7123      0.970  10.9870     0.970  312.18
  46   4.7109      0.940  10.9859     0.956  319.16
  47   4.7114      0.950  10.9850     0.918  325.96
  48   4.7112      0.960  10.9842     0.994  332.93
  49   4.7111      0.960  10.9849     0.970  339.85
  50   4.7136      0.960  10.9851     0.998  346.75
  51   4.7126      0.950  10.9853     0.984  353.65
  52   4.7123      0.950  10.9859     0.928  360.51
  53   4.7118      0.950  10.9844     0.922  367.31
  54   4.7117      0.940  10.9852     0.920  374.16
  55   4.7128      0.940  10.9845     0.968  381.04
  56   4.7133      0.940  10.9846     0.956  387.97
  57   4.7113      0.970  10.9851     0.964  394.95
  58   4.7117      0.950  10.9854     0.962  401.75
  59   4.7120      0.950  10.9849     0.940  408.63
  60   4.7113      0.950  10.9851     0.972  415.47
  61   4.7126      0.940  10.9853     0.962  422.42
  62   4.7125      0.950  10.9858     0.972  429.40
  63   4.7126      0.950  10.9863     0.990  436.28
  64   4.7115      0.950  10.9855     0.954  443.21
  65   4.7097      0.950  10.9863     0.922  450.15
  66   4.7116      0.960  10.9849     1.008  456.91
  67   4.7120      0.960  10.9841     0.976  463.98
  68   4.7115      0.960  10.9851     0.926  470.72
  69   4.7115      0.950  10.9850     0.934  477.56
  70   4.7132      0.970  10.9844     0.958  484.45
  71   4.7119      0.950  10.9851     1.002  491.35
  72   4.7118      0.940  10.9845     0.914  498.13
  73   4.7124      0.930  10.9850     0.928  505.00
  74   4.7131      0.960  10.9856     0.954  511.93
  75   4.7126      0.950  10.9845     0.960  518.66
  76   4.7125      0.950  10.9847     0.980  525.51
  77   4.7135      0.950  10.9858     0.966  532.35
  78   4.7125      0.960  10.9857     0.966  539.27
  79   4.7112      0.940  10.9856     0.928  546.24
  80   4.7146      0.950  10.9841     0.956  553.08
  81   4.7116      0.950  10.9848     0.964  559.85
  82   4.7124      0.940  10.9859     0.982  566.68
  83   4.7127      0.940  10.9841     0.992  573.65
  84   4.7107      0.930  10.9847     0.930  580.49
  85   4.7116      0.940  10.9842     0.942  587.33
