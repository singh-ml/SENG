Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7541093376 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.6694     13.340  4.0179     8.822  8.60
   2   3.1482     22.570  3.3815    18.512  15.52
   3   2.8006     28.810  2.9693    25.774  22.49
   4   2.5037     34.730  2.6345    32.418  29.43
   5   2.2945     39.300  2.3886    37.438  36.40
   6   2.1053     43.190  2.1857    41.718  43.38
   7   2.0239     45.300  2.0078    45.552  50.33
   8   1.8771     49.030  1.8660    48.770  57.36
   9   1.8240     50.220  1.7527    51.718  64.32
  10   1.7782     51.070  1.6338    54.390  71.24
  11   1.7174     53.660  1.5449    56.290  78.15
  12   1.6544     55.130  1.4507    58.766  85.06
  13   1.6056     56.140  1.3815    60.544  91.98
  14   1.6093     55.940  1.3118    62.340  98.97
  15   1.5460     57.870  1.2414    64.346  105.91
  16   1.5419     58.440  1.1795    65.806  112.83
  17   1.4997     59.210  1.1254    67.174  119.75
  18   1.4905     59.640  1.0725    68.774  126.70
  19   1.4567     60.310  1.0129    69.936  133.61
  20   1.4560     60.800  0.9638    71.464  140.51
  21   1.4653     61.040  0.9145    72.908  147.41
  22   1.4839     60.390  0.8654    74.220  154.35
  23   1.4693     61.100  0.8208    75.328  161.29
  24   1.4298     62.580  0.7833    76.444  168.27
  25   1.4484     62.080  0.7366    77.772  175.17
  26   1.4264     62.320  0.6997    78.978  182.14
  27   1.4652     63.000  0.6611    79.704  189.05
  28   1.4506     63.170  0.6140    81.270  195.98
  29   1.4683     62.740  0.5869    82.086  202.97
  30   1.4752     62.990  0.5505    83.198  209.92
  31   1.4672     63.450  0.5157    84.218  216.86
  32   1.4593     63.780  0.4831    85.124  223.75
  33   1.5055     63.900  0.4447    86.586  230.73
  34   1.5046     63.780  0.4190    87.240  237.67
  35   1.4727     64.270  0.3896    88.142  244.59
  36   1.4824     65.250  0.3613    89.252  251.52
  37   1.5508     63.960  0.3340    90.032  258.45
  38   1.5105     65.130  0.3114    90.650  265.47
  39   1.4956     65.810  0.2829    91.828  272.41
  40   1.5386     64.320  0.2675    92.156  279.37
  41   1.5295     65.300  0.2495    92.704  286.30
  42   1.5424     65.530  0.2272    93.524  293.24
  43   1.5434     65.160  0.2107    94.278  300.27
  44   1.5472     65.780  0.1922    94.870  307.22
  45   1.5504     65.610  0.1788    95.188  314.12
  46   1.5859     65.600  0.1686    95.600  321.06
  47   1.5938     65.260  0.1496    96.236  327.97
  48   1.5901     65.900  0.1350    96.668  334.96
  49   1.5787     66.420  0.1237    97.074  341.89
  50   1.5735     66.290  0.1147    97.346  348.71
  51   1.5760     66.500  0.1102    97.588  355.59
  52   1.6017     66.250  0.1030    97.746  362.53
  53   1.5870     66.610  0.0922    98.080  369.45
  54   1.6202     66.250  0.0872    98.266  376.49
  55   1.5976     66.640  0.0807    98.472  383.39
  56   1.5942     67.080  0.0757    98.554  390.33
  57   1.6106     66.800  0.0700    98.756  397.29
  58   1.6103     66.670  0.0679    98.784  404.21
  59   1.5900     66.850  0.0628    98.950  411.15
  60   1.6040     67.060  0.0586    99.040  418.06
  61   1.6051     67.030  0.0555    99.162  425.00
  62   1.6115     67.430  0.0524    99.230  431.93
  63   1.6132     67.340  0.0497    99.298  438.84
  64   1.6216     66.820  0.0476    99.346  445.82
  65   1.6201     67.100  0.0445    99.470  452.71
  66   1.6294     67.180  0.0452    99.426  459.64
  67   1.6181     67.170  0.0415    99.532  466.55
  68   1.6267     67.020  0.0412    99.514  473.49
  69   1.6219     67.160  0.0396    99.526  480.48
  70   1.6222     67.370  0.0372    99.588  487.38
  71   1.6251     67.380  0.0368    99.602  494.32
  72   1.6336     67.130  0.0348    99.682  501.26
  73   1.6275     67.420  0.0350    99.624  508.20
  74   1.6322     67.470  0.0328    99.712  515.16
  75   1.6298     67.260  0.0317    99.728  522.11
  76   1.6384     67.360  0.0308    99.756  529.02
  77   1.6346     67.460  0.0316    99.728  535.95
  78   1.6410     67.290  0.0311    99.722  542.91
  79   1.6390     67.380  0.0301    99.762  549.93
  80   1.6414     67.130  0.0301    99.730  556.87
  81   1.6454     67.250  0.0291    99.766  563.80
  82   1.6448     67.270  0.0294    99.746  570.71
  83   1.6452     67.430  0.0284    99.786  577.69
  84   1.6470     67.290  0.0280    99.804  584.63
  85   1.6419     67.360  0.0282    99.746  591.53
