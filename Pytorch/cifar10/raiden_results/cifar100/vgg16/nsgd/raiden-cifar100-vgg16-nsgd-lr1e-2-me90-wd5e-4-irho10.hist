Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 45875849728 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.5455      1.290  4.5944     1.120  30.67
   2   4.3163      3.030  4.4322     2.196  59.27
   3   4.1886      4.030  4.2411     3.238  86.26
   4   4.1092      5.160  4.0937     4.922  113.27
   5   4.0270      7.950  3.9598     6.638  140.18
   6   3.8115      9.870  3.8743     8.412  165.69
   7   3.6136     13.080  3.7827     9.668  192.73
   8   3.5851     13.620  3.6729    11.784  218.26
   9   3.4767     14.390  3.5660    14.042  245.18
  10   3.3275     18.820  3.4625    16.018  273.68
  11   3.2388     20.280  3.3557    18.068  300.80
  12   3.1666     21.950  3.2777    19.752  329.70
  13   3.1699     22.010  3.1765    21.706  358.17
  14   3.0703     24.320  3.1048    23.158  385.33
  15   2.8917     27.620  3.0418    24.730  413.82
  16   2.8748     27.840  2.9752    26.262  440.76
  17   2.8448     29.190  2.9076    27.882  469.43
  18   2.7081     31.480  2.8559    28.984  496.40
  19   2.7252     31.670  2.7895    30.604  523.31
  20   2.7166     32.000  2.7709    30.926  550.25
  21   2.5885     35.150  2.7269    32.360  578.80
  22   2.7034     33.250  2.6788    33.296  607.78
  23   2.6493     33.710  2.6607    34.052  634.70
  24   2.5824     35.320  2.6521    33.818  661.54
  25   2.6369     33.960  2.6361    34.608  688.48
  26   2.6151     34.370  2.6037    35.338  715.36
  27   2.5709     35.760  2.6001    35.308  742.49
  28   2.6272     35.780  2.5832    35.758  769.38
  29   2.5030     36.780  2.5903    35.628  796.34
  30   2.5243     37.770  2.5488    36.720  823.43
  31   2.4816     38.250  2.5535    36.702  851.92
  32   2.5272     37.560  2.5411    36.936  880.43
  33   2.4565     38.450  2.5472    36.862  907.51
  34   2.4790     39.250  2.5234    37.546  934.56
  35   2.4598     38.250  2.5046    38.108  961.51
  36   2.4627     38.300  2.4927    38.234  988.85
  37   2.4450     39.560  2.4438    39.086  1015.84
  38   2.5229     38.720  2.4813    38.778  1042.73
  39   2.4592     40.060  2.4698    38.642  1069.62
  40   2.4406     38.850  2.4374    39.582  1096.46
  41   2.4173     39.720  2.4243    40.002  1123.55
  42   2.4120     39.660  2.3979    40.526  1150.51
  43   2.3935     40.580  2.3905    40.524  1179.05
  44   2.4239     40.180  2.3732    41.040  1206.25
  45   2.2881     43.060  2.3263    41.900  1231.52
  46   2.2831     42.520  2.2985    42.714  1258.36
  47   2.3071     42.190  2.2763    42.974  1285.37
  48   2.2790     43.280  2.2328    44.136  1313.93
  49   2.2053     44.590  2.2049    44.802  1342.41
  50   2.1737     45.400  2.1454    45.858  1369.58
  51   2.1838     45.360  2.1198    46.254  1396.54
  52   2.1198     46.280  2.0843    47.448  1423.52
  53   2.0703     47.380  2.0180    48.798  1452.31
  54   2.0872     47.510  1.9881    49.440  1480.90
  55   2.0678     47.970  1.9460    50.302  1507.92
  56   2.0182     49.710  1.8944    51.632  1535.08
  57   1.9654     49.870  1.8690    52.006  1562.08
  58   1.9329     50.450  1.7905    53.450  1590.47
  59   1.9433     50.990  1.7442    54.354  1617.49
