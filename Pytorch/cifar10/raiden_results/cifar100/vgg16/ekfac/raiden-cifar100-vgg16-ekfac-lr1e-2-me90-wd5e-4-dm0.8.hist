Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14137708544 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.3642      3.170  4.5376     1.750  70.60
   2   4.0051      6.640  4.2011     4.404  133.50
   3   3.8995      7.720  3.9491     7.090  196.54
   4   3.6164     12.630  3.7781     9.682  259.58
   5   3.4829     15.080  3.5830    12.540  322.54
   6   3.1108     20.720  3.3605    16.130  385.60
   7   3.3444     20.140  3.1506    20.494  448.65
   8   3.0055     25.370  2.9853    23.856  511.90
   9   2.8646     29.060  2.8296    27.554  574.96
  10   2.6463     33.120  2.7404    29.784  637.95
  11   2.5040     34.800  2.6627    31.876  701.11
  12   2.5259     36.240  2.5780    33.892  764.21
  13   2.4809     35.930  2.5223    35.636  827.28
  14   2.4572     37.910  2.5107    36.114  890.42
  15   2.4784     37.270  2.4863    37.030  953.52
  16   2.3966     38.410  2.4648    37.578  1016.63
  17   2.3276     40.440  2.4366    38.218  1079.66
  18   2.3915     38.610  2.4307    38.644  1142.72
  19   2.4266     38.660  2.4078    39.020  1205.82
  20   2.4554     39.130  2.4166    39.282  1268.97
  21   2.4388     39.130  2.4386    39.218  1332.00
  22   2.4853     39.370  2.4704    38.254  1395.27
  23   2.4764     38.350  2.5119    37.682  1458.34
  24   2.5643     37.730  2.5253    37.382  1521.38
  25   2.4730     38.220  2.5947    36.022  1584.40
  26   2.5727     36.530  2.6181    35.588  1647.62
  27   2.5840     36.240  2.6562    34.516  1710.77
  28   2.6642     34.960  2.7122    33.658  1773.83
  29   2.6753     33.320  2.7525    32.994  1836.90
  30   2.8003     32.700  2.8023    32.302  1900.03
  31   2.9348     29.530  2.9315    29.490  1963.10
  32   2.9220     28.800  3.0382    26.530  2026.17
  33   3.0163     28.800  3.1035    25.168  2089.32
  34   3.0592     24.480  3.2336    22.272  2152.40
  35   3.2291     21.620  3.2901    20.920  2215.50
  36   3.2124     22.220  3.3312    19.828  2278.55
  37   3.1699     21.050  3.4042    17.978  2341.59
  38   3.3155     17.810  3.4386    16.990  2404.64
  39   3.1825     21.520  3.4067    16.696  2467.71
  40   3.3251     18.370  3.4280    17.016  2530.81
  41   3.2745     19.540  3.4564    16.310  2593.86
  42   3.2895     18.310  3.4317    16.954  2656.90
  43   3.4247     17.530  3.4706    16.304  2719.99
  44   3.3010     17.950  3.5334    14.752  2783.07
  45   3.3961     17.090  3.4665    15.730  2846.09
  46   3.3561     16.140  3.5107    14.392  2909.17
  47   3.1819     18.990  3.4050    16.138  2972.15
  48   3.2418     19.480  3.3522    17.416  3035.22
  49   3.1678     20.620  3.3093    18.058  3098.25
  50   3.1239     22.210  3.2309    18.986  3140.40
  51   3.0598     22.410  3.1589    20.836  3203.52
  52   2.9553     24.110  3.0912    22.384  3266.59
  53   2.8944     26.610  3.0239    23.946  3329.94
  54   2.9591     28.240  2.9537    25.130  3392.94
  55   2.8489     27.870  2.9066    25.818  3456.03
  56   2.7677     28.440  2.8165    27.740  3519.05
  57   2.6572     32.010  2.7821    28.764  3582.20
  58   2.6268     31.110  2.6948    30.530  3645.27
  59      nan      1.000     nan     6.132  3707.80
