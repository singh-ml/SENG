Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14137708544 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.4702      2.430  4.5420     1.584  65.00
   2   4.0772      4.770  4.2886     3.250  128.03
   3   3.9310      6.150  4.0134     5.804  191.14
   4   3.8664      8.930  3.8551     7.944  254.36
   5   3.6135     12.820  3.7075    10.800  317.45
   6   3.3378     16.710  3.4992    14.390  380.62
   7   3.1064     22.270  3.2860    17.716  443.68
   8   2.9878     24.840  3.1151    21.748  506.79
   9   2.8233     27.720  2.9375    25.354  569.92
  10   2.6436     31.820  2.8320    27.706  633.06
  11   2.5575     33.480  2.7163    30.382  696.13
  12   2.5349     35.670  2.6322    32.544  759.29
  13   2.4364     37.320  2.5693    34.104  822.51
  14   2.4887     36.270  2.5147    35.808  885.62
  15   2.5648     37.160  2.4700    36.824  948.80
  16   2.3737     40.130  2.4466    37.404  1011.97
  17   2.4622     38.500  2.4340    38.552  1075.15
  18   2.3924     40.160  2.4129    38.590  1138.30
  19   2.4125     40.060  2.4135    39.338  1201.50
  20   2.3364     40.490  2.4167    39.504  1264.69
  21   2.3999     39.350  2.3860    39.870  1327.82
  22   2.4217     40.220  2.4152    39.738  1390.95
  23   2.4454     39.800  2.4353    39.290  1454.12
  24   2.4125     40.700  2.4531    39.122  1517.27
  25   2.4497     39.580  2.4449    39.280  1580.38
  26   2.4147     39.460  2.4843    38.452  1643.51
  27   2.4930     38.030  2.5183    37.668  1706.74
  28   2.4946     38.740  2.5403    37.222  1769.94
  29   2.3882     40.900  2.6057    36.424  1833.01
  30   2.5214     38.230  2.5925    36.436  1896.19
  31   2.6368     35.660  2.6556    35.056  1959.36
  32   2.6694     35.690  2.6772    35.000  2022.59
  33   2.6766     34.840  2.7323    33.364  2085.72
  34   2.6932     34.310  2.7648    32.548  2148.89
  35   2.7097     35.230  2.8168    31.974  2212.06
  36   2.7153     34.530  2.8079    32.092  2275.22
  37   2.7696     33.380  2.9013    29.720  2338.36
  38   2.7473     33.060  2.9373    29.108  2401.59
  39   2.8903     30.600  2.9099    29.780  2464.73
  40   2.8480     31.320  2.9616    28.338  2527.94
  41   2.7789     32.280  2.9749    27.740  2591.02
  42   2.8000     30.680  2.9464    28.572  2654.32
  43   2.9315     28.390  2.9655    28.046  2717.49
  44   2.8474     30.580  2.9613    27.802  2780.76
  45   2.8695     30.110  2.9352    28.982  2843.96
  46   2.7476     33.690  2.9071    29.162  2907.12
  47   2.7079     33.440  2.8675    29.942  2970.22
  48   2.6973     33.880  2.8133    30.866  3033.46
  49   2.7296     33.310  2.7728    32.096  3096.57
  50   2.6527     35.480  2.7643    32.290  3138.75
  51   2.5949     36.080  2.7198    33.274  3201.97
  52   2.5368     36.090  2.6585    34.508  3265.38
  53   2.4704     38.950  2.5823    35.794  3328.72
  54   2.4603     39.970  2.5013    37.836  3391.86
  55   2.3813     41.250  2.4497    38.882  3455.01
  56      nan      1.000     nan     5.152  3517.52
