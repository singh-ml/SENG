Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7668664832 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6050      0.980  6.5483     0.898  10.32
   2   4.6050      0.980  6.5484     1.004  18.72
   3   4.6050      0.980  6.5482     1.042  27.06
   4   4.6050      0.980  6.5483     0.958  35.38
   5   4.6050      0.980  6.5480     0.968  43.71
   6   4.6050      0.980  6.5482     1.048  52.02
   7   4.6050      0.980  6.5481     1.010  60.42
   8   4.6050      0.980  6.5485     0.960  68.77
   9   4.6050      0.980  6.5483     0.988  77.11
  10   4.6050      0.980  6.5481     1.012  85.47
  11   4.6050      0.980  6.5483     0.912  93.88
  12   4.6050      0.980  6.5481     1.024  102.20
  13   4.6050      0.980  6.5482     1.016  110.53
  14   4.6050      0.980  6.5484     0.932  118.88
  15   4.6050      0.980  6.5483     0.978  127.31
  16   4.6050      0.980  6.5484     1.028  135.66
  17   4.6050      0.980  6.5484     0.900  143.99
  18   4.6050      0.980  6.5483     1.000  152.32
  19   4.6050      0.980  6.5482     1.002  160.67
  20   4.6050      0.980  6.5482     0.976  168.99
  21   4.6050      0.980  6.5483     0.962  177.32
  22   4.6050      0.980  6.5482     0.964  185.67
  23   4.6050      0.980  6.5481     0.994  193.99
  24   4.6050      0.980  6.5481     1.070  202.33
  25   4.6050      0.980  6.5484     0.980  210.66
  26   4.6050      0.980  6.5481     1.002  219.02
  27   4.6050      0.980  6.5483     0.966  227.45
  28   4.6050      0.980  6.5484     0.988  235.78
  29   4.6050      0.980  6.5484     0.924  244.10
  30   4.6050      0.980  6.5481     1.002  252.41
  31   4.6050      0.980  6.5482     0.968  260.85
  32   4.6050      0.980  6.5482     1.030  269.22
  33   4.6050      0.980  6.5482     0.988  277.54
  34   4.6050      0.980  6.5484     0.942  285.89
  35   4.6050      0.980  6.5482     1.046  294.22
  36   4.6050      0.980  6.5483     0.968  302.64
  37   4.6050      0.980  6.5484     0.938  310.96
  38   4.6050      0.980  6.5482     0.956  319.29
  39   4.6050      0.980  6.5482     1.050  327.67
  40   4.6050      0.980  6.5481     0.982  336.05
  41   4.6050      0.980  6.5482     0.986  344.38
  42   4.6050      0.980  6.5482     1.030  352.71
  43   4.6050      0.980  6.5482     0.964  361.02
  44   4.6050      0.980  6.5482     0.984  369.49
  45   4.6050      0.980  6.5483     0.994  377.82
  46   4.6050      0.980  6.5481     1.024  386.16
  47   4.6050      0.980  6.5485     1.002  394.52
  48   4.6050      0.980  6.5483     1.028  402.95
  49   4.6050      0.980  6.5484     1.010  411.31
  50   4.6050      0.980  6.5481     0.984  419.63
  51   4.6050      0.980  6.5482     0.998  427.98
  52   4.6050      0.980  6.5485     0.974  436.40
  53   4.6050      0.980  6.5482     0.982  444.72
  54   4.6050      0.980  6.5479     1.036  453.09
  55   4.6050      0.980  6.5483     1.006  461.41
  56   4.6050      0.980  6.5482     0.966  469.85
  57   4.6050      0.980  6.5481     1.048  478.18
  58   4.6050      0.980  6.5485     0.974  486.49
  59   4.6050      0.980  6.5482     1.024  494.87
  60   4.6050      0.980  6.5482     1.062  503.31
  61   4.6050      0.980  6.5483     1.078  511.68
  62   4.6050      0.980  6.5483     1.042  520.02
  63   4.6050      0.980  6.5481     1.010  528.38
  64   4.6050      0.980  6.5484     0.980  536.69
  65   4.6050      0.980  6.5481     1.032  545.07
  66   4.6050      0.980  6.5482     0.980  553.39
  67   4.6050      0.980  6.5483     1.048  561.73
  68   4.6050      0.980  6.5484     0.972  570.07
  69   4.6050      0.980  6.5483     0.980  578.45
  70   4.6050      0.980  6.5483     0.986  586.77
  71   4.6050      0.980  6.5483     1.072  595.10
  72   4.6050      0.980  6.5483     1.004  603.45
  73   4.6050      0.980  6.5484     0.990  611.86
  74   4.6050      0.980  6.5485     0.978  620.20
  75   4.6050      0.980  6.5481     0.980  628.51
  76   4.6050      0.980  6.5483     1.020  636.86
  77   4.6050      0.980  6.5482     0.936  645.25
  78   4.6050      0.980  6.5482     0.996  653.58
  79   4.6050      0.980  6.5483     0.978  661.95
  80   4.6050      0.980  6.5479     1.074  670.31
  81   4.6050      0.980  6.5482     1.032  678.73
  82   4.6050      0.980  6.5482     0.980  687.09
  83   4.6050      0.980  6.5483     1.014  695.41
  84   4.6050      0.980  6.5484     0.970  703.73
  85   4.6050      0.980  6.5483     0.924  712.11
