Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8745792512 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6052      0.930  8.4916     1.060  10.97
   2   4.6052      0.930  8.4917     0.950  20.08
   3   4.6052      0.930  8.4914     0.954  29.21
   4   4.6052      0.930  8.4915     1.046  38.43
   5   4.6052      0.930  8.4916     0.902  47.56
   6   4.6052      0.930  8.4916     0.998  56.71
   7   4.6052      0.930  8.4917     1.008  65.86
   8   4.6052      0.930  8.4920     0.952  75.07
   9   4.6052      0.930  8.4918     1.086  84.18
  10   4.6052      0.930  8.4917     1.006  93.31
  11   4.6052      0.930  8.4918     1.036  102.46
  12   4.6052      0.930  8.4919     0.900  111.65
  13   4.6052      0.930  8.4918     1.042  120.83
  14   4.6052      0.930  8.4917     1.028  129.98
  15   4.6052      0.930  8.4917     1.008  139.15
  16   4.6052      0.930  8.4917     0.982  148.28
  17   4.6052      0.930  8.4916     1.052  157.45
  18   4.6052      0.930  8.4921     0.928  166.56
  19   4.6052      0.930  8.4918     0.890  175.75
  20   4.6052      0.930  8.4915     1.016  184.89
  21   4.6052      0.930  8.4920     1.032  194.04
  22   4.6052      0.930  8.4916     1.026  203.16
  23   4.6052      0.930  8.4915     1.022  212.36
  24   4.6052      0.930  8.4914     0.980  221.46
  25   4.6052      0.930  8.4919     0.972  230.59
  26   4.6052      0.930  8.4914     1.044  239.76
  27   4.6052      0.930  8.4918     1.028  248.91
  28   4.6052      0.930  8.4916     0.934  258.02
  29   4.6052      0.930  8.4918     1.000  267.16
  30   4.6052      0.930  8.4919     0.976  276.36
  31   4.6052      0.930  8.4915     1.016  285.50
  32   4.6052      0.930  8.4916     1.034  294.61
  33   4.6052      0.930  8.4919     1.026  303.71
  34   4.6052      0.930  8.4915     1.004  312.88
  35   4.6052      0.930  8.4918     1.000  322.00
  36   4.6052      0.930  8.4919     1.038  331.14
  37   4.6052      0.930  8.4919     0.998  340.28
  38   4.6052      0.930  8.4914     0.992  349.47
  39   4.6052      0.930  8.4917     1.052  358.58
  40   4.6052      0.930  8.4918     1.088  367.73
  41   4.6052      0.930  8.4917     1.112  376.84
  42   4.6052      0.930  8.4914     1.002  386.04
  43   4.6052      0.930  8.4920     1.022  395.17
  44   4.6052      0.930  8.4916     0.978  404.30
  45   4.6052      0.930  8.4919     0.974  413.41
  46   4.6052      0.930  8.4919     0.992  422.61
  47   4.6052      0.930  8.4916     1.014  431.76
  48   4.6052      0.930  8.4917     1.026  440.91
  49   4.6052      0.930  8.4915     1.006  450.13
  50   4.6052      0.930  8.4920     0.918  459.23
  51   4.6052      0.930  8.4914     0.988  468.40
  52   4.6052      0.930  8.4920     0.938  477.51
  53   4.6052      0.930  8.4917     0.946  486.73
  54   4.6052      0.930  8.4919     1.060  495.84
  55   4.6052      0.930  8.4916     1.018  504.94
  56   4.6052      0.930  8.4920     0.964  514.09
  57   4.6052      0.930  8.4916     1.006  523.28
  58   4.6052      0.930  8.4919     0.962  532.44
  59   4.6052      0.930  8.4921     0.986  541.59
  60   4.6052      0.930  8.4915     1.054  550.69
  61   4.6052      0.930  8.4919     1.040  559.88
  62   4.6052      0.930  8.4915     1.056  569.00
  63   4.6052      0.930  8.4918     1.028  578.15
  64   4.6052      0.930  8.4913     1.004  587.26
  65   4.6052      0.930  8.4919     1.004  596.48
  66   4.6052      0.930  8.4919     0.980  605.63
  67   4.6052      0.930  8.4917     0.956  614.75
  68   4.6052      0.930  8.4915     0.924  624.00
  69   4.6052      0.930  8.4919     1.012  633.11
  70   4.6052      0.930  8.4915     1.020  642.22
  71   4.6052      0.930  8.4916     0.966  651.35
  72   4.6052      0.930  8.4916     0.896  660.54
  73   4.6052      0.930  8.4915     1.098  669.65
  74   4.6052      0.930  8.4923     0.940  678.79
  75   4.6052      0.930  8.4914     1.028  687.96
  76   4.6052      0.930  8.4917     0.986  697.11
  77   4.6052      0.930  8.4919     0.944  706.26
  78   4.6052      0.930  8.4917     1.046  715.36
  79   4.6052      0.930  8.4918     0.952  724.60
  80   4.6052      0.930  8.4917     1.022  733.73
  81   4.6052      0.930  8.4916     0.994  742.86
  82   4.6052      0.930  8.4917     1.052  751.98
  83   4.6052      0.930  8.4918     0.974  761.18
  84   4.6052      0.930  8.4918     0.968  770.29
  85   4.6052      0.930  8.4919     0.950  779.45
