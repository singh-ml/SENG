Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8746603520 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6056      0.910  6.5485     0.986  11.20
   2   4.6056      0.910  6.5485     0.964  20.36
   3   4.6056      0.910  6.5484     0.996  29.49
   4   4.6056      0.910  6.5486     1.002  38.64
   5   4.6056      0.910  6.5484     0.956  47.85
   6   4.6056      0.910  6.5483     1.002  57.00
   7   4.6056      0.910  6.5485     1.008  66.16
   8   4.6056      0.910  6.5485     0.942  75.30
   9   4.6056      0.910  6.5483     1.080  84.51
  10   4.6056      0.910  6.5484     1.002  93.63
  11   4.6056      0.910  6.5483     1.012  102.75
  12   4.6056      0.910  6.5483     0.994  111.87
  13   4.6056      0.910  6.5483     0.998  121.09
  14   4.6056      0.910  6.5484     0.930  130.21
  15   4.6056      0.910  6.5484     0.984  139.34
  16   4.6056      0.910  6.5486     1.022  148.56
  17   4.6056      0.910  6.5480     0.972  157.73
  18   4.6056      0.910  6.5485     0.996  166.89
  19   4.6056      0.910  6.5483     0.970  176.00
  20   4.6056      0.910  6.5486     0.948  185.20
  21   4.6056      0.910  6.5484     1.022  194.33
  22   4.6056      0.910  6.5486     0.934  203.46
  23   4.6056      0.910  6.5484     0.912  212.57
  24   4.6056      0.910  6.5484     0.970  221.80
  25   4.6056      0.910  6.5485     1.000  230.94
  26   4.6056      0.910  6.5485     1.086  240.08
  27   4.6056      0.910  6.5485     0.900  249.27
  28   4.6056      0.910  6.5484     0.982  258.39
  29   4.6056      0.910  6.5485     1.012  267.55
  30   4.6056      0.910  6.5485     1.012  276.69
  31   4.6056      0.910  6.5484     0.988  285.92
  32   4.6056      0.910  6.5485     0.994  295.04
  33   4.6056      0.910  6.5484     0.982  304.17
  34   4.6056      0.910  6.5484     0.964  313.33
  35   4.6056      0.910  6.5483     1.002  322.53
  36   4.6056      0.910  6.5484     0.962  331.65
  37   4.6056      0.910  6.5484     1.020  340.81
  38   4.6056      0.910  6.5483     1.004  349.95
  39   4.6056      0.910  6.5483     0.988  359.20
  40   4.6056      0.910  6.5485     0.942  368.33
  41   4.6056      0.910  6.5484     0.940  377.48
  42   4.6056      0.910  6.5482     1.012  386.65
  43   4.6056      0.910  6.5485     0.968  395.84
  44   4.6056      0.910  6.5484     0.930  404.97
  45   4.6056      0.910  6.5483     1.002  414.12
  46   4.6056      0.910  6.5482     0.954  423.23
  47   4.6056      0.910  6.5486     0.972  432.47
  48   4.6056      0.910  6.5484     0.970  441.62
  49   4.6056      0.910  6.5483     1.044  450.74
  50   4.6056      0.910  6.5483     0.964  459.98
  51   4.6056      0.910  6.5484     0.994  469.08
  52   4.6056      0.910  6.5483     0.986  478.22
  53   4.6056      0.910  6.5481     0.952  487.35
  54   4.6056      0.910  6.5482     0.962  496.56
  55   4.6056      0.910  6.5481     1.082  505.69
  56   4.6056      0.910  6.5484     1.018  514.85
  57   4.6056      0.910  6.5483     0.952  524.12
  58   4.6056      0.910  6.5484     1.024  533.24
  59   4.6056      0.910  6.5485     0.900  542.39
  60   4.6056      0.910  6.5484     0.996  551.52
  61   4.6056      0.910  6.5482     1.066  560.71
  62   4.6056      0.910  6.5485     1.040  569.84
  63   4.6056      0.910  6.5480     0.940  578.98
  64   4.6056      0.910  6.5484     1.056  588.08
  65   4.6056      0.910  6.5485     0.976  597.30
  66   4.6056      0.910  6.5483     0.984  606.41
  67   4.6056      0.910  6.5483     1.026  615.56
  68   4.6056      0.910  6.5486     1.022  624.71
  69   4.6056      0.910  6.5487     0.924  633.90
  70   4.6056      0.910  6.5482     1.062  643.04
  71   4.6056      0.910  6.5484     0.974  652.18
  72   4.6056      0.910  6.5482     1.130  661.33
  73   4.6056      0.910  6.5486     0.974  670.53
  74   4.6056      0.910  6.5484     0.926  679.68
  75   4.6056      0.910  6.5486     0.976  688.81
  76   4.6056      0.910  6.5484     0.926  697.96
  77   4.6056      0.910  6.5485     0.974  707.17
  78   4.6056      0.910  6.5483     0.978  716.28
  79   4.6056      0.910  6.5484     0.994  725.39
  80   4.6056      0.910  6.5485     1.030  734.58
  81   4.6056      0.910  6.5483     1.026  743.74
  82   4.6056      0.910  6.5484     1.044  752.86
  83   4.6056      0.910  6.5485     1.052  761.99
  84   4.6056      0.910  6.5486     0.984  771.17
  85   4.6056      0.910  6.5483     1.038  780.30
