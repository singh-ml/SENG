Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9821237760 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6052      0.930  14.3170     0.996  11.07
   2   4.6052      0.930  14.3169     0.976  20.24
   3   4.6052      0.930  14.3167     1.040  29.45
   4   4.6052      0.930  14.3171     0.974  38.65
   5   4.6052      0.930  14.3170     0.924  47.89
   6   4.6052      0.930  14.3171     0.880  57.17
   7   4.6052      0.930  14.3172     0.992  66.33
   8   4.6052      0.930  14.3169     0.878  75.49
   9   4.6052      0.930  14.3169     0.934  84.68
  10   4.6052      0.930  14.3168     1.068  94.02
  11   4.6052      0.930  14.3170     0.998  103.24
  12   4.6052      0.930  14.3172     0.986  112.46
  13   4.6052      0.930  14.3172     0.962  121.68
  14   4.6052      0.930  14.3169     1.018  130.85
  15   4.6052      0.930  14.3170     1.000  140.03
  16   4.6052      0.930  14.3171     0.952  149.25
  17   4.6052      0.930  14.3168     0.988  158.55
  18   4.6052      0.930  14.3170     1.018  167.70
  19   4.6052      0.930  14.3170     0.974  176.91
  20   4.6052      0.930  14.3170     0.950  186.04
  21   4.6052      0.930  14.3170     0.952  195.32
  22   4.6052      0.930  14.3167     1.072  204.54
  23   4.6052      0.930  14.3169     0.962  213.71
  24   4.6052      0.930  14.3170     0.990  223.02
  25   4.6052      0.930  14.3167     0.970  232.15
  26   4.6052      0.930  14.3169     0.996  241.32
  27   4.6052      0.930  14.3168     0.902  250.51
  28   4.6052      0.930  14.3168     1.100  259.82
  29   4.6052      0.930  14.3169     0.976  269.02
  30   4.6052      0.930  14.3172     0.984  278.21
  31   4.6052      0.930  14.3170     0.972  287.43
  32   4.6052      0.930  14.3169     1.018  296.70
  33   4.6052      0.930  14.3169     1.004  305.89
  34   4.6052      0.930  14.3168     1.026  315.13
  35   4.6052      0.930  14.3171     1.060  324.35
  36   4.6052      0.930  14.3170     0.978  333.63
  37   4.6052      0.930  14.3172     1.042  342.86
  38   4.6052      0.930  14.3170     0.964  352.08
  39   4.6052      0.930  14.3171     0.992  361.32
  40   4.6052      0.930  14.3170     0.980  370.54
  41   4.6052      0.930  14.3171     1.026  379.76
  42   4.6052      0.930  14.3169     1.042  388.98
  43   4.6052      0.930  14.3170     0.918  398.24
  44   4.6052      0.930  14.3167     1.076  407.46
  45   4.6052      0.930  14.3171     0.972  416.64
  46   4.6052      0.930  14.3170     1.074  425.85
  47   4.6052      0.930  14.3172     1.006  435.15
  48   4.6052      0.930  14.3170     0.992  444.33
  49   4.6052      0.930  14.3168     0.938  453.56
  50   4.6052      0.930  14.3172     0.876  462.83
  51   4.6052      0.930  14.3170     0.972  472.01
  52   4.6052      0.930  14.3170     0.952  481.25
  53   4.6052      0.930  14.3169     1.016  490.43
  54   4.6052      0.930  14.3170     0.954  499.73
  55   4.6052      0.930  14.3168     0.992  508.94
  56   4.6052      0.930  14.3169     1.016  518.15
  57   4.6052      0.930  14.3169     0.986  527.39
  58   4.6052      0.930  14.3169     1.054  536.61
  59   4.6052      0.930  14.3171     0.972  545.85
  60   4.6052      0.930  14.3169     0.972  555.03
  61   4.6052      0.930  14.3170     0.970  564.19
  62   4.6052      0.930  14.3171     1.040  573.49
  63   4.6052      0.930  14.3170     1.032  582.70
  64   4.6052      0.930  14.3171     1.008  591.89
  65   4.6052      0.930  14.3168     1.008  601.03
  66   4.6052      0.930  14.3170     1.006  610.28
  67   4.6052      0.930  14.3170     1.052  619.46
  68   4.6052      0.930  14.3169     0.940  628.65
  69   4.6052      0.930  14.3168     0.974  637.89
  70   4.6052      0.930  14.3169     1.002  647.06
  71   4.6052      0.930  14.3167     0.988  656.28
  72   4.6052      0.930  14.3171     0.940  665.46
  73   4.6052      0.930  14.3171     0.972  674.73
  74   4.6052      0.930  14.3169     0.994  683.89
  75   4.6052      0.930  14.3168     0.998  693.10
  76   4.6052      0.930  14.3170     1.014  702.26
  77   4.6052      0.930  14.3171     0.962  711.50
  78   4.6052      0.930  14.3169     0.954  720.67
  79   4.6052      0.930  14.3170     0.944  729.85
  80   4.6052      0.930  14.3169     1.022  739.10
  81   4.6052      0.930  14.3170     1.028  748.28
  82   4.6052      0.930  14.3170     1.040  757.49
  83   4.6052      0.930  14.3170     0.922  766.70
  84   4.6052      0.930  14.3169     0.936  775.96
  85   4.6052      0.930  14.3170     0.966  785.12
  86   4.6052      0.930  14.3172     0.936  794.37
  87   4.6052      0.930  14.3170     0.958  803.54
  88   4.6052      0.930  14.3169     1.028  812.78
  89   4.6052      0.930  14.3170     0.948  822.00
  90   4.6052      0.930  14.3169     1.028  831.18
