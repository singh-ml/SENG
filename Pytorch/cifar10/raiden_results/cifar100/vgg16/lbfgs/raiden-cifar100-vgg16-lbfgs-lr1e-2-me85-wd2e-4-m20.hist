Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8746603520 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6055      0.980  8.4914     1.054  10.86
   2   4.6055      0.980  8.4916     0.962  19.98
   3   4.6055      0.980  8.4913     0.996  29.20
   4   4.6055      0.980  8.4917     1.024  38.32
   5   4.6055      0.980  8.4916     0.986  47.48
   6   4.6055      0.980  8.4916     0.936  56.63
   7   4.6055      0.980  8.4916     0.934  65.89
   8   4.6055      0.980  8.4916     0.918  75.02
   9   4.6055      0.980  8.4915     1.036  84.15
  10   4.6055      0.980  8.4915     0.996  93.29
  11   4.6055      0.980  8.4915     0.992  102.44
  12   4.6055      0.980  8.4915     0.994  111.57
  13   4.6055      0.980  8.4915     1.080  120.72
  14   4.6055      0.980  8.4914     1.046  129.83
  15   4.6055      0.980  8.4915     1.006  139.03
  16   4.6055      0.980  8.4918     0.972  148.17
  17   4.6055      0.980  8.4914     1.074  157.29
  18   4.6055      0.980  8.4916     0.970  166.49
  19   4.6055      0.980  8.4915     1.030  175.61
  20   4.6055      0.980  8.4917     0.888  184.70
  21   4.6055      0.980  8.4916     1.018  193.80
  22   4.6055      0.980  8.4917     0.970  203.01
  23   4.6055      0.980  8.4916     1.000  212.14
  24   4.6055      0.980  8.4917     1.008  221.27
  25   4.6055      0.980  8.4915     1.034  230.42
  26   4.6055      0.980  8.4914     1.032  239.68
  27   4.6055      0.980  8.4915     1.036  248.81
  28   4.6055      0.980  8.4916     0.962  257.94
  29   4.6055      0.980  8.4913     0.936  267.15
  30   4.6055      0.980  8.4915     1.038  276.29
  31   4.6055      0.980  8.4914     1.026  285.43
  32   4.6055      0.980  8.4916     1.040  294.55
  33   4.6055      0.980  8.4916     0.958  303.73
  34   4.6055      0.980  8.4916     0.956  312.85
  35   4.6055      0.980  8.4917     0.980  321.98
  36   4.6055      0.980  8.4916     1.034  331.09
  37   4.6055      0.980  8.4916     1.068  340.28
  38   4.6055      0.980  8.4916     0.988  349.39
  39   4.6055      0.980  8.4916     0.994  358.52
  40   4.6055      0.980  8.4916     0.958  367.63
  41   4.6055      0.980  8.4916     0.974  376.83
  42   4.6055      0.980  8.4916     0.948  385.95
  43   4.6055      0.980  8.4913     0.996  395.07
  44   4.6055      0.980  8.4915     0.954  404.17
  45   4.6055      0.980  8.4915     1.026  413.35
  46   4.6055      0.980  8.4915     0.958  422.46
  47   4.6055      0.980  8.4917     0.964  431.59
  48   4.6055      0.980  8.4915     1.030  440.81
  49   4.6055      0.980  8.4916     0.998  449.91
  50   4.6055      0.980  8.4915     1.020  459.03
  51   4.6055      0.980  8.4917     1.004  468.16
  52   4.6055      0.980  8.4917     0.942  477.37
  53   4.6055      0.980  8.4918     0.992  486.55
  54   4.6055      0.980  8.4915     1.034  495.67
  55   4.6055      0.980  8.4916     0.960  504.80
  56   4.6055      0.980  8.4915     1.082  514.02
  57   4.6055      0.980  8.4917     0.954  523.13
  58   4.6055      0.980  8.4914     1.088  532.24
  59   4.6055      0.980  8.4918     0.960  541.46
  60   4.6055      0.980  8.4913     1.018  550.61
  61   4.6055      0.980  8.4915     0.966  559.72
  62   4.6055      0.980  8.4916     0.994  568.85
  63   4.6055      0.980  8.4916     1.076  577.99
  64   4.6055      0.980  8.4916     0.996  587.17
  65   4.6055      0.980  8.4917     1.052  596.30
  66   4.6055      0.980  8.4914     0.950  605.43
  67   4.6055      0.980  8.4917     1.038  614.59
  68   4.6055      0.980  8.4914     1.032  623.75
  69   4.6055      0.980  8.4914     1.070  632.88
  70   4.6055      0.980  8.4917     1.066  642.00
  71   4.6055      0.980  8.4917     1.034  651.23
  72   4.6055      0.980  8.4917     1.014  660.34
  73   4.6055      0.980  8.4915     0.948  669.50
  74   4.6055      0.980  8.4917     0.994  678.60
  75   4.6055      0.980  8.4916     1.004  687.78
  76   4.6055      0.980  8.4915     1.046  696.90
  77   4.6055      0.980  8.4916     1.012  706.03
  78   4.6055      0.980  8.4914     1.074  715.18
  79   4.6055      0.980  8.4915     0.974  724.36
  80   4.6055      0.980  8.4916     0.946  733.48
  81   4.6055      0.980  8.4916     1.040  742.63
  82   4.6055      0.980  8.4916     0.998  751.81
  83   4.6055      0.980  8.4916     1.008  760.92
  84   4.6055      0.980  8.4915     1.078  770.03
  85   4.6055      0.980  8.4915     1.026  779.13
