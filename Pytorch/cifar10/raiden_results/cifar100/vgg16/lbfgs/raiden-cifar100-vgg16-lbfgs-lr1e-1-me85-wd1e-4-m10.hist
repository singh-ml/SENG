Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14135478272 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6058      0.910  6.5488     1.062  14.81
   2   4.6058      0.910  6.5493     0.980  28.02
   3   4.6058      0.910  6.5491     0.998  41.29
   4   4.6058      0.910  6.5491     1.004  54.50
   5   4.6058      0.910  6.5490     1.022  67.70
   6   4.6058      0.910  6.5487     1.030  80.98
   7   4.6058      0.910  6.5492     0.888  94.16
   8   4.6058      0.910  6.5495     0.986  107.44
   9   4.6058      0.910  6.5492     0.982  120.64
  10   4.6058      0.910  6.5490     1.026  133.83
  11   4.6058      0.910  6.5493     1.024  147.10
  12   4.6058      0.910  6.5491     1.012  160.27
  13   4.6058      0.910  6.5489     1.030  173.48
  14   4.6058      0.910  6.5489     1.018  186.72
  15   4.6058      0.910  6.5492     0.952  199.89
  16   4.6058      0.910  6.5491     0.932  213.09
  17   4.6058      0.910  6.5488     1.028  226.41
  18   4.6058      0.910  6.5491     0.992  239.61
  19   4.6058      0.910  6.5490     0.942  252.95
  20   4.6058      0.910  6.5487     0.998  266.17
  21   4.6058      0.910  6.5488     1.072  279.37
  22   4.6058      0.910  6.5487     0.966  292.71
  23   4.6058      0.910  6.5488     1.038  305.90
  24   4.6058      0.910  6.5492     0.962  319.12
  25   4.6058      0.910  6.5489     0.914  332.36
  26   4.6058      0.910  6.5494     0.912  345.57
  27   4.6058      0.910  6.5489     0.984  358.74
  28   4.6058      0.910  6.5489     0.994  371.99
  29   4.6058      0.910  6.5489     1.040  385.18
  30   4.6058      0.910  6.5490     0.986  398.38
  31   4.6058      0.910  6.5488     0.992  411.61
  32   4.6058      0.910  6.5491     0.922  424.80
  33   4.6058      0.910  6.5490     1.000  438.08
  34   4.6058      0.910  6.5490     0.950  451.28
  35   4.6058      0.910  6.5490     0.986  464.47
  36   4.6058      0.910  6.5493     1.006  477.73
  37   4.6058      0.910  6.5491     0.958  490.93
  38   4.6058      0.910  6.5491     0.930  504.11
  39   4.6058      0.910  6.5489     0.972  517.39
  40   4.6058      0.910  6.5490     0.974  530.61
  41   4.6058      0.910  6.5489     1.038  543.79
  42   4.6058      0.910  6.5491     0.976  557.07
  43   4.6058      0.910  6.5491     0.980  570.31
  44   4.6058      0.910  6.5488     0.978  583.60
  45   4.6058      0.910  6.5490     0.998  596.77
  46   4.6058      0.910  6.5493     0.946  609.97
  47   4.6058      0.910  6.5489     1.004  623.24
  48   4.6058      0.910  6.5489     1.004  636.40
  49   4.6058      0.910  6.5490     0.948  649.56
  50   4.6058      0.910  6.5488     0.972  662.80
  51   4.6058      0.910  6.5490     1.006  675.98
  52   4.6058      0.910  6.5493     0.966  689.26
  53   4.6058      0.910  6.5491     0.984  702.44
  54   4.6058      0.910  6.5491     0.950  715.64
  55   4.6058      0.910  6.5493     0.942  728.92
  56   4.6058      0.910  6.5489     1.042  742.13
  57   4.6058      0.910  6.5491     0.990  755.35
  58   4.6058      0.910  6.5488     1.008  768.62
  59   4.6058      0.910  6.5490     0.932  781.85
  60   4.6058      0.910  6.5491     0.998  795.05
  61   4.6058      0.910  6.5492     1.028  808.34
  62   4.6058      0.910  6.5491     0.932  821.54
  63   4.6058      0.910  6.5490     1.032  834.83
  64   4.6058      0.910  6.5488     1.032  848.00
  65   4.6058      0.910  6.5491     1.060  861.20
  66   4.6058      0.910  6.5489     1.006  874.45
  67   4.6058      0.910  6.5494     0.940  887.67
  68   4.6058      0.910  6.5492     0.966  900.89
  69   4.6058      0.910  6.5489     1.018  914.19
  70   4.6058      0.910  6.5488     1.028  927.38
  71   4.6058      0.910  6.5492     0.936  940.59
  72   4.6058      0.910  6.5491     0.996  953.76
  73   4.6058      0.910  6.5492     1.040  966.96
  74   4.6058      0.910  6.5492     1.038  980.22
  75   4.6058      0.910  6.5494     0.992  993.41
  76   4.6058      0.910  6.5493     0.942  1006.60
  77   4.6058      0.910  6.5493     1.064  1019.87
  78   4.6058      0.910  6.5491     1.028  1033.07
  79   4.6058      0.910  6.5490     0.998  1046.32
  80   4.6058      0.910  6.5490     1.028  1059.64
  81   4.6058      0.910  6.5489     1.062  1072.86
  82   4.6058      0.910  6.5494     0.944  1086.08
  83   4.6058      0.910  6.5489     1.020  1099.27
  84   4.6058      0.910  6.5489     1.026  1112.49
  85   4.6058      0.910  6.5489     1.000  1125.81
