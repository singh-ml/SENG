Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11980414976 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6052      0.980  6.5481     1.010  13.09
   2   4.6052      0.980  6.5482     0.974  24.64
   3   4.6052      0.980  6.5480     1.028  36.25
   4   4.6052      0.980  6.5480     0.952  47.77
   5   4.6052      0.980  6.5480     0.992  59.31
   6   4.6052      0.980  6.5480     1.034  70.89
   7   4.6052      0.980  6.5481     1.016  82.43
   8   4.6052      0.980  6.5481     0.998  93.96
   9   4.6052      0.980  6.5479     1.056  105.55
  10   4.6052      0.980  6.5480     0.952  117.09
  11   4.6052      0.980  6.5481     1.000  128.61
  12   4.6052      0.980  6.5480     0.978  140.24
  13   4.6052      0.980  6.5481     0.948  151.78
  14   4.6052      0.980  6.5480     0.990  163.31
  15   4.6052      0.980  6.5481     1.046  174.91
  16   4.6052      0.980  6.5481     1.052  186.44
  17   4.6052      0.980  6.5481     1.036  197.97
  18   4.6052      0.980  6.5480     1.034  209.58
  19   4.6052      0.980  6.5481     1.028  221.11
  20   4.6052      0.980  6.5480     0.966  232.62
  21   4.6052      0.980  6.5479     0.974  244.19
  22   4.6052      0.980  6.5482     1.066  255.70
  23   4.6052      0.980  6.5482     1.004  267.30
  24   4.6052      0.980  6.5480     0.974  278.84
  25   4.6052      0.980  6.5482     0.970  290.38
  26   4.6052      0.980  6.5480     1.072  301.98
  27   4.6052      0.980  6.5479     1.046  313.53
  28   4.6052      0.980  6.5482     0.988  325.08
  29   4.6052      0.980  6.5483     0.910  336.69
  30   4.6052      0.980  6.5480     0.984  348.22
  31   4.6052      0.980  6.5480     0.980  359.72
  32   4.6052      0.980  6.5483     0.920  371.22
  33   4.6052      0.980  6.5479     1.010  382.76
  34   4.6052      0.980  6.5479     1.064  394.28
  35   4.6052      0.980  6.5479     1.096  405.90
  36   4.6052      0.980  6.5480     1.018  417.41
  37   4.6052      0.980  6.5479     0.942  428.96
  38   4.6052      0.980  6.5482     0.950  440.62
  39   4.6052      0.980  6.5481     0.976  452.18
  40   4.6052      0.980  6.5479     1.070  463.70
  41   4.6052      0.980  6.5481     0.966  475.35
  42   4.6052      0.980  6.5481     0.966  486.89
  43   4.6052      0.980  6.5480     0.948  498.45
  44   4.6052      0.980  6.5480     0.922  510.09
  45   4.6052      0.980  6.5481     0.930  521.61
  46   4.6052      0.980  6.5481     1.004  533.12
  47   4.6052      0.980  6.5480     1.074  544.71
  48   4.6052      0.980  6.5479     1.078  556.23
  49   4.6052      0.980  6.5481     0.968  567.75
  50   4.6052      0.980  6.5481     1.050  579.31
  51   4.6052      0.980  6.5483     0.986  590.82
  52   4.6052      0.980  6.5479     0.964  602.33
  53   4.6052      0.980  6.5479     1.036  613.90
  54   4.6052      0.980  6.5480     0.974  625.45
  55   4.6052      0.980  6.5479     0.986  636.98
  56   4.6052      0.980  6.5481     0.964  648.60
  57   4.6052      0.980  6.5478     1.016  660.16
  58   4.6052      0.980  6.5481     1.022  671.71
  59   4.6052      0.980  6.5483     1.004  683.32
  60   4.6052      0.980  6.5481     0.992  694.84
  61   4.6052      0.980  6.5480     0.962  706.36
  62   4.6052      0.980  6.5485     0.888  717.96
  63   4.6052      0.980  6.5484     1.000  729.49
  64   4.6052      0.980  6.5479     1.000  741.02
  65   4.6052      0.980  6.5481     0.994  752.61
  66   4.6052      0.980  6.5480     1.004  764.12
  67   4.6052      0.980  6.5481     1.040  775.63
  68   4.6052      0.980  6.5483     1.036  787.31
  69   4.6052      0.980  6.5482     0.954  798.83
  70   4.6052      0.980  6.5482     1.052  810.37
  71   4.6052      0.980  6.5481     0.960  821.91
  72   4.6052      0.980  6.5480     1.008  833.50
  73   4.6052      0.980  6.5482     0.982  845.02
  74   4.6052      0.980  6.5482     1.010  856.59
  75   4.6052      0.980  6.5481     0.980  868.13
  76   4.6052      0.980  6.5483     1.040  879.67
  77   4.6052      0.980  6.5482     0.960  891.25
  78   4.6052      0.980  6.5483     0.980  902.76
  79   4.6052      0.980  6.5480     1.046  914.30
  80   4.6052      0.980  6.5480     1.042  925.91
  81   4.6052      0.980  6.5480     0.988  937.49
  82   4.6052      0.980  6.5480     1.060  949.00
  83   4.6052      0.980  6.5479     0.944  960.60
  84   4.6052      0.980  6.5480     0.986  972.15
  85   4.6052      0.980  6.5480     0.970  983.66
