Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14136287744 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6057      0.980  14.3202     1.044  13.94
   2   4.6057      0.980  14.3200     0.970  26.39
   3   4.6057      0.980  14.3201     0.976  38.89
   4   4.6057      0.980  14.3204     0.962  51.34
   5   4.6057      0.980  14.3201     0.994  63.80
   6   4.6057      0.980  14.3202     1.034  76.23
   7   4.6057      0.980  14.3202     1.006  88.65
   8   4.6057      0.980  14.3204     0.894  101.14
   9   4.6057      0.980  14.3200     1.024  113.55
  10   4.6057      0.980  14.3201     1.024  125.93
  11   4.6057      0.980  14.3200     1.006  138.47
  12   4.6057      0.980  14.3203     0.970  150.88
  13   4.6057      0.980  14.3202     0.960  163.33
  14   4.6057      0.980  14.3199     1.016  175.78
  15   4.6057      0.980  14.3203     0.972  188.19
  16   4.6057      0.980  14.3199     1.064  200.67
  17   4.6057      0.980  14.3202     0.972  213.13
  18   4.6057      0.980  14.3203     0.930  225.53
  19   4.6057      0.980  14.3203     1.048  238.03
  20   4.6057      0.980  14.3202     1.044  250.47
  21   4.6057      0.980  14.3203     0.980  262.90
  22   4.6057      0.980  14.3200     1.102  275.42
  23   4.6057      0.980  14.3200     1.026  287.82
  24   4.6057      0.980  14.3202     0.994  300.27
  25   4.6057      0.980  14.3202     0.930  312.77
  26   4.6057      0.980  14.3204     0.974  325.21
  27   4.6057      0.980  14.3201     0.966  337.67
  28   4.6057      0.980  14.3202     1.018  350.08
  29   4.6057      0.980  14.3203     0.960  362.51
  30   4.6057      0.980  14.3202     1.004  374.98
  31   4.6057      0.980  14.3201     1.050  387.42
  32   4.6057      0.980  14.3200     1.022  399.86
  33   4.6057      0.980  14.3202     0.956  412.35
  34   4.6057      0.980  14.3201     1.040  424.78
  35   4.6057      0.980  14.3198     0.992  437.21
  36   4.6057      0.980  14.3202     1.012  449.65
  37   4.6057      0.980  14.3201     1.010  462.09
  38   4.6057      0.980  14.3204     0.970  474.53
  39   4.6057      0.980  14.3204     1.038  487.06
  40   4.6057      0.980  14.3200     0.950  499.45
  41   4.6057      0.980  14.3201     0.954  511.91
  42   4.6057      0.980  14.3202     0.964  524.44
  43   4.6057      0.980  14.3201     0.966  536.87
  44   4.6057      0.980  14.3200     0.994  549.37
  45   4.6057      0.980  14.3203     0.966  561.75
  46   4.6057      0.980  14.3202     1.014  574.19
  47   4.6057      0.980  14.3203     1.000  586.66
  48   4.6057      0.980  14.3203     1.034  599.06
  49   4.6057      0.980  14.3203     1.064  611.51
  50   4.6057      0.980  14.3200     0.986  623.98
  51   4.6057      0.980  14.3200     0.992  636.37
  52   4.6057      0.980  14.3200     0.990  648.79
  53   4.6057      0.980  14.3204     0.966  661.19
  54   4.6057      0.980  14.3204     0.950  673.58
  55   4.6057      0.980  14.3204     1.032  685.99
  56   4.6057      0.980  14.3201     0.948  698.47
  57   4.6057      0.980  14.3202     1.014  710.89
  58   4.6057      0.980  14.3199     0.978  723.37
  59   4.6057      0.980  14.3202     1.032  735.80
  60   4.6057      0.980  14.3201     0.982  748.20
  61   4.6057      0.980  14.3203     0.992  760.71
  62   4.6057      0.980  14.3203     1.028  773.13
  63   4.6057      0.980  14.3201     0.976  785.59
  64   4.6057      0.980  14.3203     0.952  798.09
  65   4.6057      0.980  14.3202     0.960  810.48
  66   4.6057      0.980  14.3203     0.956  822.88
  67   4.6057      0.980  14.3203     1.004  835.41
  68   4.6057      0.980  14.3201     1.022  847.87
  69   4.6057      0.980  14.3201     1.044  860.31
  70   4.6057      0.980  14.3203     0.988  872.73
  71   4.6057      0.980  14.3203     0.952  885.13
  72   4.6057      0.980  14.3202     0.952  897.59
  73   4.6057      0.980  14.3199     1.028  909.99
  74   4.6057      0.980  14.3201     1.026  922.41
  75   4.6057      0.980  14.3200     0.980  934.97
  76   4.6057      0.980  14.3203     0.988  947.38
  77   4.6057      0.980  14.3202     0.980  959.84
  78   4.6057      0.980  14.3202     0.984  972.32
  79   4.6057      0.980  14.3204     0.948  984.74
  80   4.6057      0.980  14.3201     1.002  997.17
  81   4.6057      0.980  14.3201     0.988  1009.64
  82   4.6057      0.980  14.3202     0.944  1022.05
  83   4.6057      0.980  14.3200     1.006  1034.44
  84   4.6057      0.980  14.3201     1.028  1046.85
  85   4.6057      0.980  14.3200     1.078  1059.27
