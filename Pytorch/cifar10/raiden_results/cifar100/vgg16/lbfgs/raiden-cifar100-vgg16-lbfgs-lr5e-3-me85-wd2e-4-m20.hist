Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8746601984 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6054      0.980  8.4903     0.936  10.52
   2   4.6054      0.980  8.4903     0.944  19.33
   3   4.6054      0.980  8.4903     1.030  28.14
   4   4.6054      0.980  8.4901     0.956  36.95
   5   4.6054      0.980  8.4903     1.026  45.67
   6   4.6054      0.980  8.4902     1.008  54.52
   7   4.6054      0.980  8.4902     1.006  63.34
   8   4.6054      0.980  8.4904     1.050  72.10
   9   4.6054      0.980  8.4904     0.994  80.90
  10   4.6054      0.980  8.4904     0.988  89.82
  11   4.6054      0.980  8.4902     0.972  98.71
  12   4.6054      0.980  8.4902     1.012  107.49
  13   4.6054      0.980  8.4899     1.094  116.40
  14   4.6054      0.980  8.4901     1.058  125.22
  15   4.6054      0.980  8.4901     0.934  134.07
  16   4.6054      0.980  8.4901     1.024  142.90
  17   4.6054      0.980  8.4904     0.960  151.71
  18   4.6054      0.980  8.4902     1.034  160.53
  19   4.6054      0.980  8.4900     0.986  169.36
  20   4.6054      0.980  8.4902     0.922  178.18
  21   4.6054      0.980  8.4898     0.998  187.00
  22   4.6054      0.980  8.4901     1.010  195.85
  23   4.6054      0.980  8.4902     0.960  204.66
  24   4.6054      0.980  8.4900     0.930  213.60
  25   4.6054      0.980  8.4904     1.018  222.40
  26   4.6054      0.980  8.4900     1.036  231.20
  27   4.6054      0.980  8.4901     0.998  240.02
  28   4.6054      0.980  8.4903     1.022  248.91
  29   4.6054      0.980  8.4904     0.916  257.72
  30   4.6054      0.980  8.4902     1.040  266.53
  31   4.6054      0.980  8.4901     0.936  275.30
  32   4.6054      0.980  8.4904     0.882  284.07
  33   4.6054      0.980  8.4899     1.022  292.93
  34   4.6054      0.980  8.4900     0.978  301.74
  35   4.6054      0.980  8.4902     1.002  310.58
  36   4.6054      0.980  8.4901     1.112  319.35
  37   4.6054      0.980  8.4901     1.006  328.24
  38   4.6054      0.980  8.4900     0.968  337.04
  39   4.6054      0.980  8.4901     1.020  345.86
  40   4.6054      0.980  8.4902     1.008  354.73
  41   4.6054      0.980  8.4902     0.950  363.51
  42   4.6054      0.980  8.4901     0.954  372.36
  43   4.6054      0.980  8.4900     0.958  381.15
  44   4.6054      0.980  8.4901     0.878  390.06
  45   4.6054      0.980  8.4902     0.940  398.92
  46   4.6054      0.980  8.4902     1.056  407.73
  47   4.6054      0.980  8.4903     0.984  416.54
  48   4.6054      0.980  8.4903     0.988  425.38
  49   4.6054      0.980  8.4902     0.970  434.16
  50   4.6054      0.980  8.4901     1.030  442.97
  51   4.6054      0.980  8.4902     1.072  451.82
  52   4.6054      0.980  8.4902     1.040  460.75
  53   4.6054      0.980  8.4898     1.046  469.53
  54   4.6054      0.980  8.4899     1.048  478.36
  55   4.6054      0.980  8.4902     0.942  487.14
  56   4.6054      0.980  8.4902     0.960  496.03
  57   4.6054      0.980  8.4903     0.978  504.81
  58   4.6054      0.980  8.4904     0.984  513.63
  59   4.6054      0.980  8.4901     0.940  522.51
  60   4.6054      0.980  8.4901     1.042  531.39
  61   4.6054      0.980  8.4904     0.928  540.21
  62   4.6054      0.980  8.4902     0.948  548.99
  63   4.6054      0.980  8.4900     0.998  557.79
  64   4.6054      0.980  8.4901     1.090  566.70
  65   4.6054      0.980  8.4902     0.988  575.50
  66   4.6054      0.980  8.4900     0.992  584.26
  67   4.6054      0.980  8.4902     1.028  592.98
  68   4.6054      0.980  8.4904     1.040  601.82
  69   4.6054      0.980  8.4901     0.900  610.62
  70   4.6054      0.980  8.4902     1.000  619.49
  71   4.6054      0.980  8.4902     0.970  628.34
  72   4.6054      0.980  8.4901     0.990  637.21
  73   4.6054      0.980  8.4902     1.054  646.04
  74   4.6054      0.980  8.4900     1.028  654.87
  75   4.6054      0.980  8.4901     0.954  663.79
  76   4.6054      0.980  8.4901     1.002  672.57
  77   4.6054      0.980  8.4904     0.948  681.42
  78   4.6054      0.980  8.4904     0.978  690.29
  79   4.6054      0.980  8.4901     1.014  699.19
  80   4.6054      0.980  8.4903     0.966  707.98
  81   4.6054      0.980  8.4901     0.988  716.75
  82   4.6054      0.980  8.4900     0.972  725.53
  83   4.6054      0.980  8.4901     1.012  734.39
  84   4.6054      0.980  8.4902     1.028  743.23
  85   4.6054      0.980  8.4904     0.964  752.02
