Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 10901796352 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6053      0.870  8.4906     0.978  11.68
   2   4.6053      0.870  8.4905     0.978  22.43
   3   4.6053      0.870  8.4906     0.982  33.25
   4   4.6053      0.870  8.4906     0.974  44.00
   5   4.6053      0.870  8.4907     0.924  54.73
   6   4.6053      0.870  8.4904     1.036  65.49
   7   4.6053      0.870  8.4907     0.906  76.32
   8   4.6053      0.870  8.4902     0.986  87.04
   9   4.6053      0.870  8.4905     1.006  97.82
  10   4.6053      0.870  8.4907     0.986  108.63
  11   4.6053      0.870  8.4906     1.026  119.38
  12   4.6053      0.870  8.4907     0.980  130.13
  13   4.6053      0.870  8.4906     1.032  140.92
  14   4.6053      0.870  8.4905     0.968  151.66
  15   4.6053      0.870  8.4905     0.960  162.41
  16   4.6053      0.870  8.4905     1.064  173.22
  17   4.6053      0.870  8.4904     1.002  183.96
  18   4.6053      0.870  8.4904     1.018  194.71
  19   4.6053      0.870  8.4907     0.968  205.50
  20   4.6053      0.870  8.4906     1.026  216.30
  21   4.6053      0.870  8.4904     1.088  227.03
  22   4.6053      0.870  8.4905     1.084  237.78
  23   4.6053      0.870  8.4907     0.978  248.57
  24   4.6053      0.870  8.4905     0.958  259.33
  25   4.6053      0.870  8.4904     1.026  270.09
  26   4.6053      0.870  8.4906     0.982  280.94
  27   4.6053      0.870  8.4906     1.000  291.69
  28   4.6053      0.870  8.4907     0.988  302.44
  29   4.6053      0.870  8.4904     0.996  313.24
  30   4.6053      0.870  8.4905     1.064  323.99
  31   4.6053      0.870  8.4905     1.024  334.74
  32   4.6053      0.870  8.4905     0.938  345.57
  33   4.6053      0.870  8.4907     1.032  356.30
  34   4.6053      0.870  8.4905     1.040  367.05
  35   4.6053      0.870  8.4905     1.032  377.79
  36   4.6053      0.870  8.4905     0.976  388.59
  37   4.6053      0.870  8.4907     1.046  399.34
  38   4.6053      0.870  8.4905     1.072  410.07
  39   4.6053      0.870  8.4904     0.998  420.91
  40   4.6053      0.870  8.4905     1.008  431.67
  41   4.6053      0.870  8.4906     1.018  442.38
  42   4.6053      0.870  8.4906     1.018  453.19
  43   4.6053      0.870  8.4904     0.940  463.93
  44   4.6053      0.870  8.4905     0.932  474.66
  45   4.6053      0.870  8.4905     0.990  485.50
  46   4.6053      0.870  8.4905     0.994  496.23
  47   4.6053      0.870  8.4904     0.962  506.98
  48   4.6053      0.870  8.4905     1.080  517.79
  49   4.6053      0.870  8.4906     1.006  528.54
  50   4.6053      0.870  8.4906     0.926  539.27
  51   4.6053      0.870  8.4904     1.046  550.02
  52   4.6053      0.870  8.4907     0.980  560.87
  53   4.6053      0.870  8.4905     0.972  571.62
  54   4.6053      0.870  8.4904     1.072  582.35
  55   4.6053      0.870  8.4903     1.026  593.15
  56   4.6053      0.870  8.4906     1.022  603.90
  57   4.6053      0.870  8.4905     1.000  614.63
  58   4.6053      0.870  8.4905     0.970  625.35
  59   4.6053      0.870  8.4906     0.966  636.07
  60   4.6053      0.870  8.4907     0.954  646.82
  61   4.6053      0.870  8.4905     0.982  657.56
  62   4.6053      0.870  8.4904     0.972  668.41
  63   4.6053      0.870  8.4907     1.042  679.13
  64   4.6053      0.870  8.4904     0.996  689.86
  65   4.6053      0.870  8.4902     1.016  700.69
  66   4.6053      0.870  8.4907     0.976  711.45
  67   4.6053      0.870  8.4905     1.026  722.19
  68   4.6053      0.870  8.4903     1.014  733.00
  69   4.6053      0.870  8.4904     0.980  743.74
  70   4.6053      0.870  8.4905     1.032  754.47
  71   4.6053      0.870  8.4905     1.016  765.28
  72   4.6053      0.870  8.4906     0.890  776.02
  73   4.6053      0.870  8.4904     1.006  786.76
  74   4.6053      0.870  8.4905     1.050  797.54
  75   4.6053      0.870  8.4906     0.990  808.33
  76   4.6053      0.870  8.4908     1.044  819.08
  77   4.6053      0.870  8.4904     1.038  829.82
  78   4.6053      0.870  8.4906     1.008  840.64
  79   4.6053      0.870  8.4906     1.016  851.40
  80   4.6053      0.870  8.4906     0.994  862.11
  81   4.6053      0.870  8.4905     0.946  872.91
  82   4.6053      0.870  8.4905     1.042  883.65
  83   4.6053      0.870  8.4906     1.024  894.42
  84   4.6053      0.870  8.4907     0.992  905.22
  85   4.6053      0.870  8.4906     0.976  915.97
