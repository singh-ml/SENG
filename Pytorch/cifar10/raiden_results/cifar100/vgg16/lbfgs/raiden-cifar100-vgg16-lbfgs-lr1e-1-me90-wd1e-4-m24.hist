Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 13057541120 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6055      0.980  6.5481     1.014  13.49
   2   4.6055      0.980  6.5484     1.034  25.87
   3   4.6055      0.980  6.5485     0.946  38.25
   4   4.6055      0.980  6.5484     0.926  50.67
   5   4.6055      0.980  6.5483     1.004  63.08
   6   4.6055      0.980  6.5484     0.994  75.48
   7   4.6055      0.980  6.5481     0.984  87.92
   8   4.6055      0.980  6.5483     0.954  100.33
   9   4.6055      0.980  6.5483     0.968  112.73
  10   4.6055      0.980  6.5483     0.964  125.16
  11   4.6055      0.980  6.5482     0.970  137.54
  12   4.6055      0.980  6.5482     0.988  149.91
  13   4.6055      0.980  6.5482     0.994  162.35
  14   4.6055      0.980  6.5484     1.014  174.74
  15   4.6055      0.980  6.5483     0.984  187.19
  16   4.6055      0.980  6.5483     1.050  199.57
  17   4.6055      0.980  6.5482     1.036  211.99
  18   4.6055      0.980  6.5483     0.972  224.42
  19   4.6055      0.980  6.5484     0.924  236.81
  20   4.6055      0.980  6.5483     0.998  249.20
  21   4.6055      0.980  6.5482     0.986  261.67
  22   4.6055      0.980  6.5481     0.994  274.06
  23   4.6055      0.980  6.5484     0.972  286.50
  24   4.6055      0.980  6.5482     0.948  298.89
  25   4.6055      0.980  6.5483     1.038  311.29
  26   4.6055      0.980  6.5481     0.968  323.73
  27   4.6055      0.980  6.5484     0.972  336.08
  28   4.6055      0.980  6.5480     1.050  348.47
  29   4.6055      0.980  6.5483     0.974  360.95
  30   4.6055      0.980  6.5483     0.970  373.35
  31   4.6055      0.980  6.5482     0.940  385.77
  32   4.6055      0.980  6.5482     1.034  398.19
  33   4.6055      0.980  6.5482     0.998  410.60
  34   4.6055      0.980  6.5483     1.052  422.98
  35   4.6055      0.980  6.5482     0.974  435.45
  36   4.6055      0.980  6.5483     0.954  447.85
  37   4.6055      0.980  6.5482     0.976  460.29
  38   4.6055      0.980  6.5484     1.048  472.66
  39   4.6055      0.980  6.5482     1.010  485.04
  40   4.6055      0.980  6.5482     1.022  497.50
  41   4.6055      0.980  6.5483     1.004  509.88
  42   4.6055      0.980  6.5481     1.026  522.30
  43   4.6055      0.980  6.5482     1.022  534.72
  44   4.6055      0.980  6.5481     0.980  547.10
  45   4.6055      0.980  6.5483     0.966  559.46
  46   4.6055      0.980  6.5485     0.980  571.92
  47   4.6055      0.980  6.5482     0.976  584.32
  48   4.6055      0.980  6.5482     0.988  596.73
  49   4.6055      0.980  6.5483     1.026  609.15
  50   4.6055      0.980  6.5481     0.984  621.55
  51   4.6055      0.980  6.5481     1.006  634.00
  52   4.6055      0.980  6.5481     0.928  646.40
  53   4.6055      0.980  6.5482     0.994  658.81
  54   4.6055      0.980  6.5482     0.942  671.27
  55   4.6055      0.980  6.5483     1.062  683.68
  56   4.6055      0.980  6.5485     1.026  696.07
  57   4.6055      0.980  6.5483     1.040  708.51
  58   4.6055      0.980  6.5484     1.016  720.91
  59   4.6055      0.980  6.5482     1.000  733.40
  60   4.6055      0.980  6.5483     1.000  745.79
  61   4.6055      0.980  6.5482     1.016  758.18
  62   4.6055      0.980  6.5485     1.028  770.57
  63   4.6055      0.980  6.5483     0.942  783.01
  64   4.6055      0.980  6.5482     1.034  795.37
  65   4.6055      0.980  6.5484     0.948  807.82
  66   4.6055      0.980  6.5482     1.032  820.21
  67   4.6055      0.980  6.5483     1.042  832.62
  68   4.6055      0.980  6.5481     1.016  845.11
  69   4.6055      0.980  6.5483     1.018  857.47
  70   4.6055      0.980  6.5478     1.014  869.87
  71   4.6055      0.980  6.5482     0.970  882.34
  72   4.6055      0.980  6.5480     0.972  894.73
  73   4.6055      0.980  6.5483     0.966  907.12
  74   4.6055      0.980  6.5482     0.986  919.65
  75   4.6055      0.980  6.5484     0.962  932.05
  76   4.6055      0.980  6.5485     0.886  944.41
  77   4.6055      0.980  6.5482     1.010  956.91
  78   4.6055      0.980  6.5482     0.938  969.32
  79   4.6055      0.980  6.5482     0.964  981.76
  80   4.6055      0.980  6.5482     1.020  994.13
  81   4.6055      0.980  6.5483     0.954  1006.50
  82   4.6055      0.980  6.5482     0.942  1018.94
  83   4.6055      0.980  6.5481     1.018  1031.31
  84   4.6055      0.980  6.5484     0.976  1043.69
  85   4.6055      0.980  6.5483     0.962  1056.16
  86   4.6055      0.980  6.5483     0.926  1068.56
  87   4.6055      0.980  6.5482     0.934  1080.97
  88   4.6055      0.980  6.5485     0.986  1093.43
  89   4.6055      0.980  6.5483     1.074  1105.84
  90   4.6055      0.980  6.5486     0.962  1118.22
