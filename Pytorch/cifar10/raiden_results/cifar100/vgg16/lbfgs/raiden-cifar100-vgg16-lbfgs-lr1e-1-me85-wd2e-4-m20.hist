Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 15214226432 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6052      0.980  8.4907     0.958  15.51
   2   4.6052      0.980  8.4907     0.966  29.65
   3   4.6052      0.980  8.4906     0.968  43.77
   4   4.6052      0.980  8.4908     1.036  57.94
   5   4.6052      0.980  8.4910     0.990  72.04
   6   4.6052      0.980  8.4908     1.004  86.19
   7   4.6052      0.980  8.4906     1.006  100.32
   8   4.6052      0.980  8.4909     0.984  114.40
   9   4.6052      0.980  8.4908     1.016  128.60
  10   4.6052      0.980  8.4908     1.038  142.68
  11   4.6052      0.980  8.4904     0.992  156.75
  12   4.6052      0.980  8.4909     1.094  170.94
  13   4.6052      0.980  8.4906     1.002  185.07
  14   4.6052      0.980  8.4909     1.000  199.25
  15   4.6052      0.980  8.4907     1.038  213.33
  16   4.6052      0.980  8.4907     0.998  227.44
  17   4.6052      0.980  8.4907     0.994  241.61
  18   4.6052      0.980  8.4908     0.992  255.70
  19   4.6052      0.980  8.4907     1.020  269.89
  20   4.6052      0.980  8.4907     1.018  284.00
  21   4.6052      0.980  8.4908     1.022  298.10
  22   4.6052      0.980  8.4908     0.988  312.27
  23   4.6052      0.980  8.4908     1.040  326.36
  24   4.6052      0.980  8.4909     1.056  340.45
  25   4.6052      0.980  8.4908     1.008  354.63
  26   4.6052      0.980  8.4907     1.086  368.74
  27   4.6052      0.980  8.4909     0.950  382.90
  28   4.6052      0.980  8.4907     0.996  397.00
  29   4.6052      0.980  8.4908     0.984  411.12
  30   4.6052      0.980  8.4908     0.992  425.28
  31   4.6052      0.980  8.4907     0.982  439.44
  32   4.6052      0.980  8.4909     0.904  453.64
  33   4.6052      0.980  8.4908     1.060  467.75
  34   4.6052      0.980  8.4907     0.942  481.87
  35   4.6052      0.980  8.4909     0.986  496.00
  36   4.6052      0.980  8.4908     0.952  510.11
  37   4.6052      0.980  8.4909     0.984  524.24
  38   4.6052      0.980  8.4909     1.006  538.34
  39   4.6052      0.980  8.4907     1.052  552.43
  40   4.6052      0.980  8.4909     0.898  566.60
  41   4.6052      0.980  8.4908     1.060  581.50
  42   4.6052      0.980  8.4907     1.090  596.48
  43   4.6052      0.980  8.4907     1.060  611.40
  44   4.6052      0.980  8.4910     0.936  626.31
  45   4.6052      0.980  8.4910     0.998  641.30
  46   4.6052      0.980  8.4907     1.024  656.21
  47   4.6052      0.980  8.4906     0.998  671.17
  48   4.6052      0.980  8.4907     1.018  686.09
  49   4.6052      0.980  8.4908     0.998  701.01
  50   4.6052      0.980  8.4908     1.034  715.95
  51   4.6052      0.980  8.4911     1.038  730.87
  52   4.6052      0.980  8.4908     1.050  745.81
  53   4.6052      0.980  8.4907     1.044  760.75
  54   4.6052      0.980  8.4907     0.980  775.74
  55   4.6052      0.980  8.4909     0.948  790.66
  56   4.6052      0.980  8.4908     1.040  805.57
  57   4.6052      0.980  8.4910     0.938  820.51
  58   4.6052      0.980  8.4909     0.954  835.42
  59   4.6052      0.980  8.4908     1.054  850.35
  60   4.6052      0.980  8.4907     1.036  865.25
  61   4.6052      0.980  8.4910     0.950  880.14
  62   4.6052      0.980  8.4907     0.990  895.07
  63   4.6052      0.980  8.4908     0.928  909.96
  64   4.6052      0.980  8.4909     0.998  924.92
  65   4.6052      0.980  8.4908     1.056  939.78
  66   4.6052      0.980  8.4907     1.000  954.69
  67   4.6052      0.980  8.4906     0.992  969.66
  68   4.6052      0.980  8.4907     1.024  984.55
  69   4.6052      0.980  8.4909     0.980  999.52
  70   4.6052      0.980  8.4908     0.960  1014.40
  71   4.6052      0.980  8.4910     0.954  1029.33
  72   4.6052      0.980  8.4909     1.048  1044.20
  73   4.6052      0.980  8.4908     0.940  1059.10
  74   4.6052      0.980  8.4908     1.062  1074.05
  75   4.6052      0.980  8.4909     0.990  1088.96
  76   4.6052      0.980  8.4906     1.034  1103.94
  77   4.6052      0.980  8.4909     1.038  1118.84
  78   4.6052      0.980  8.4910     1.004  1133.77
  79   4.6052      0.980  8.4908     0.950  1148.71
  80   4.6052      0.980  8.4907     1.036  1163.63
  81   4.6052      0.980  8.4906     1.042  1178.60
  82   4.6052      0.980  8.4907     0.904  1193.51
  83   4.6052      0.980  8.4908     0.994  1208.46
  84   4.6052      0.980  8.4908     1.004  1223.35
  85   4.6052      0.980  8.4905     1.024  1238.26
