Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7668664832 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6053      0.970  14.3180     0.962  10.40
   2   4.6053      0.970  14.3178     0.974  19.02
   3   4.6053      0.970  14.3178     1.010  27.57
   4   4.6053      0.970  14.3177     1.050  36.09
   5   4.6053      0.970  14.3178     0.982  44.58
   6   4.6053      0.970  14.3176     0.984  53.17
   7   4.6053      0.970  14.3179     1.030  61.73
   8   4.6053      0.970  14.3181     0.956  70.26
   9   4.6053      0.970  14.3178     0.988  78.75
  10   4.6053      0.970  14.3177     1.028  87.32
  11   4.6053      0.970  14.3179     0.972  95.89
  12   4.6053      0.970  14.3178     0.988  104.42
  13   4.6053      0.970  14.3179     1.020  112.98
  14   4.6053      0.970  14.3180     0.908  121.49
  15   4.6053      0.970  14.3178     1.024  130.01
  16   4.6053      0.970  14.3178     0.978  138.58
  17   4.6053      0.970  14.3178     1.036  147.11
  18   4.6053      0.970  14.3177     0.996  155.77
  19   4.6053      0.970  14.3179     0.998  164.32
  20   4.6053      0.970  14.3177     1.004  172.86
  21   4.6053      0.970  14.3179     1.052  181.45
  22   4.6053      0.970  14.3178     0.980  189.97
  23   4.6053      0.970  14.3178     0.954  198.50
  24   4.6053      0.970  14.3178     1.008  207.04
  25   4.6053      0.970  14.3180     0.916  215.67
  26   4.6053      0.970  14.3179     0.948  224.16
  27   4.6053      0.970  14.3178     1.000  232.68
  28   4.6053      0.970  14.3179     1.030  241.29
  29   4.6053      0.970  14.3177     0.964  249.83
  30   4.6053      0.970  14.3176     0.974  258.40
  31   4.6053      0.970  14.3177     1.042  266.92
  32   4.6053      0.970  14.3177     1.020  275.46
  33   4.6053      0.970  14.3178     0.916  283.95
  34   4.6053      0.970  14.3179     0.956  292.52
  35   4.6053      0.970  14.3179     0.978  300.98
  36   4.6053      0.970  14.3178     0.948  309.53
  37   4.6053      0.970  14.3178     1.010  318.07
  38   4.6053      0.970  14.3180     0.976  326.72
  39   4.6053      0.970  14.3179     0.972  335.28
  40   4.6053      0.970  14.3177     0.972  343.86
  41   4.6053      0.970  14.3178     1.060  352.39
  42   4.6053      0.970  14.3179     0.962  360.99
  43   4.6053      0.970  14.3178     0.966  369.60
  44   4.6053      0.970  14.3177     0.976  378.12
  45   4.6053      0.970  14.3177     0.912  386.63
  46   4.6053      0.970  14.3178     0.984  395.16
  47   4.6053      0.970  14.3177     0.994  403.68
  48   4.6053      0.970  14.3178     0.974  412.22
  49   4.6053      0.970  14.3179     0.960  420.83
  50   4.6053      0.970  14.3178     0.948  429.35
  51   4.6053      0.970  14.3179     0.970  437.88
  52   4.6053      0.970  14.3177     0.982  446.48
  53   4.6053      0.970  14.3178     1.016  455.04
  54   4.6053      0.970  14.3177     0.988  463.58
  55   4.6053      0.970  14.3177     0.888  472.09
  56   4.6053      0.970  14.3177     0.954  480.61
  57   4.6053      0.970  14.3178     0.984  489.17
  58   4.6053      0.970  14.3175     0.966  497.77
  59   4.6053      0.970  14.3177     1.028  506.35
  60   4.6053      0.970  14.3177     1.026  514.88
  61   4.6053      0.970  14.3178     0.950  523.40
  62   4.6053      0.970  14.3178     1.040  532.02
  63   4.6053      0.970  14.3178     1.040  540.53
  64   4.6053      0.970  14.3178     0.930  549.06
  65   4.6053      0.970  14.3179     0.938  557.59
  66   4.6053      0.970  14.3178     1.006  566.20
  67   4.6053      0.970  14.3180     1.030  574.74
  68   4.6053      0.970  14.3176     1.044  583.32
  69   4.6053      0.970  14.3179     0.914  591.90
  70   4.6053      0.970  14.3177     0.982  600.54
  71   4.6053      0.970  14.3177     0.958  609.11
  72   4.6053      0.970  14.3176     0.958  617.62
  73   4.6053      0.970  14.3180     0.970  626.13
  74   4.6053      0.970  14.3179     0.994  634.75
  75   4.6053      0.970  14.3178     1.002  643.27
  76   4.6053      0.970  14.3176     1.010  651.82
  77   4.6053      0.970  14.3177     0.978  660.34
  78   4.6053      0.970  14.3179     1.048  668.93
  79   4.6053      0.970  14.3179     1.010  677.49
  80   4.6053      0.970  14.3180     0.894  686.04
  81   4.6053      0.970  14.3178     1.010  694.57
  82   4.6053      0.970  14.3179     1.008  703.17
  83   4.6053      0.970  14.3176     1.050  711.66
  84   4.6053      0.970  14.3177     1.010  720.21
  85   4.6053      0.970  14.3177     0.996  728.84
