Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9822835200 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6052      0.980  14.3191     1.034  10.88
   2   4.6052      0.980  14.3192     1.006  20.24
   3   4.6052      0.980  14.3192     0.986  29.50
   4   4.6052      0.980  14.3192     1.082  38.80
   5   4.6052      0.980  14.3193     0.974  48.10
   6   4.6052      0.980  14.3191     0.948  57.44
   7   4.6052      0.980  14.3191     0.968  66.73
   8   4.6052      0.980  14.3191     0.942  76.00
   9   4.6052      0.980  14.3192     0.976  85.26
  10   4.6052      0.980  14.3192     0.948  94.57
  11   4.6052      0.980  14.3190     1.028  103.80
  12   4.6052      0.980  14.3192     0.962  113.06
  13   4.6052      0.980  14.3194     0.964  122.35
  14   4.6052      0.980  14.3192     1.014  131.76
  15   4.6052      0.980  14.3191     0.998  141.07
  16   4.6052      0.980  14.3193     1.026  150.37
  17   4.6052      0.980  14.3191     1.002  159.71
  18   4.6052      0.980  14.3191     1.000  169.13
  19   4.6052      0.980  14.3190     0.954  178.42
  20   4.6052      0.980  14.3192     0.998  187.73
  21   4.6052      0.980  14.3193     1.056  197.01
  22   4.6052      0.980  14.3190     0.976  206.39
  23   4.6052      0.980  14.3193     0.982  215.69
  24   4.6052      0.980  14.3192     0.980  224.96
  25   4.6052      0.980  14.3189     1.092  234.30
  26   4.6052      0.980  14.3191     1.018  243.57
  27   4.6052      0.980  14.3192     1.002  252.84
  28   4.6052      0.980  14.3192     0.926  262.15
  29   4.6052      0.980  14.3193     1.000  271.46
  30   4.6052      0.980  14.3192     1.018  280.80
  31   4.6052      0.980  14.3189     1.006  290.08
  32   4.6052      0.980  14.3191     1.014  299.45
  33   4.6052      0.980  14.3191     0.996  308.75
  34   4.6052      0.980  14.3190     1.004  317.99
  35   4.6052      0.980  14.3190     0.938  327.22
  36   4.6052      0.980  14.3192     0.906  336.57
  37   4.6052      0.980  14.3192     1.024  345.83
  38   4.6052      0.980  14.3191     0.986  355.13
  39   4.6052      0.980  14.3191     1.048  364.39
  40   4.6052      0.980  14.3190     0.940  373.73
  41   4.6052      0.980  14.3191     1.042  383.04
  42   4.6052      0.980  14.3190     0.934  392.27
  43   4.6052      0.980  14.3190     1.070  401.56
  44   4.6052      0.980  14.3190     0.976  410.93
  45   4.6052      0.980  14.3190     1.042  420.21
  46   4.6052      0.980  14.3192     0.894  429.52
  47   4.6052      0.980  14.3191     1.064  438.82
  48   4.6052      0.980  14.3190     0.984  448.14
  49   4.6052      0.980  14.3189     0.996  457.41
  50   4.6052      0.980  14.3193     0.958  466.73
  51   4.6052      0.980  14.3191     0.926  476.07
  52   4.6052      0.980  14.3190     0.966  485.37
  53   4.6052      0.980  14.3191     0.948  494.65
  54   4.6052      0.980  14.3189     0.994  503.95
  55   4.6052      0.980  14.3191     0.966  513.34
  56   4.6052      0.980  14.3190     1.002  522.68
  57   4.6052      0.980  14.3191     0.978  531.93
  58   4.6052      0.980  14.3192     1.108  541.22
  59   4.6052      0.980  14.3190     1.038  550.50
  60   4.6052      0.980  14.3192     1.068  559.75
  61   4.6052      0.980  14.3193     1.046  569.01
  62   4.6052      0.980  14.3192     0.924  578.33
  63   4.6052      0.980  14.3191     0.974  587.64
  64   4.6052      0.980  14.3190     1.100  596.88
  65   4.6052      0.980  14.3189     0.982  606.19
  66   4.6052      0.980  14.3193     1.004  615.56
  67   4.6052      0.980  14.3191     0.980  624.79
  68   4.6052      0.980  14.3192     1.034  634.12
  69   4.6052      0.980  14.3190     1.030  643.44
  70   4.6052      0.980  14.3193     0.940  652.82
  71   4.6052      0.980  14.3189     0.976  662.13
  72   4.6052      0.980  14.3192     1.026  671.45
  73   4.6052      0.980  14.3189     0.978  680.77
  74   4.6052      0.980  14.3191     1.080  690.12
  75   4.6052      0.980  14.3192     0.990  699.44
  76   4.6052      0.980  14.3194     1.022  708.78
  77   4.6052      0.980  14.3192     1.040  718.08
  78   4.6052      0.980  14.3190     1.004  727.45
  79   4.6052      0.980  14.3193     1.020  736.69
  80   4.6052      0.980  14.3192     0.898  745.98
  81   4.6052      0.980  14.3192     0.928  755.26
  82   4.6052      0.980  14.3193     0.958  764.59
  83   4.6052      0.980  14.3192     0.984  773.88
  84   4.6052      0.980  14.3190     1.060  783.20
  85   4.6052      0.980  14.3191     0.996  792.61
