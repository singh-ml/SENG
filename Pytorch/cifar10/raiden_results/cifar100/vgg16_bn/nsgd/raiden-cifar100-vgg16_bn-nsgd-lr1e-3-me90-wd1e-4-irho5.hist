Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--batch-size=256', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '5', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 58825122304 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.8364     10.100  4.3261     4.220  36.35
   2   3.4622     14.980  3.7511    10.972  68.85
   3   3.2902     18.780  3.4849    15.102  100.18
   4   3.0534     23.400  3.2643    19.018  131.02
   5   2.9838     25.040  3.0548    22.918  161.99
   6   2.8185     27.960  2.8833    26.166  192.85
   7   2.5640     33.100  2.7301    29.244  223.73
   8   2.5146     34.720  2.5823    32.322  254.54
   9   2.4212     36.290  2.4554    34.934  285.60
  10   2.3263     38.560  2.3398    37.684  316.58
  11   2.1837     41.110  2.2353    39.828  347.60
  12   2.1315     43.070  2.1359    42.358  378.42
  13   2.0116     45.810  2.0536    44.100  409.42
  14   2.0183     45.310  1.9702    45.960  440.46
  15   1.9684     47.270  1.8944    48.046  469.47
  16   1.9262     47.310  1.8292    49.316  500.23
  17   1.9204     48.230  1.7655    51.086  531.54
  18   1.8815     49.330  1.7048    52.462  562.37
  19   1.8288     50.420  1.6386    54.138  593.23
  20   1.7611     51.610  1.5878    55.148  625.63
  21   1.7492     52.510  1.5452    56.200  656.68
  22   1.6694     53.920  1.4835    57.748  687.65
  23   1.7176     53.060  1.4372    58.808  718.48
  24   1.7133     53.730  1.3894    60.078  749.33
  25   1.6962     53.780  1.3474    61.318  780.34
  26   1.6621     55.390  1.3132    62.156  812.81
  27   1.6711     55.130  1.2636    63.142  843.65
  28   1.6331     56.260  1.2273    64.268  874.44
  29   1.6414     55.950  1.1824    65.240  905.52
  30   1.6994     55.420  1.1439    66.362  936.47
  31   1.6266     56.300  1.1046    67.238  968.89
  32   1.6283     57.270  1.0663    68.612  999.73
  33   1.6604     56.170  1.0394    69.076  1030.77
  34   1.6400     56.920  1.0149    69.668  1061.71
  35   1.5789     58.190  0.9870    70.490  1092.59
  36   1.6045     58.030  0.9403    71.588  1123.44
  37   1.5958     58.460  0.9129    72.636  1154.26
  38   1.6438     57.850  0.8955    72.860  1185.16
  39   1.6618     57.830  0.8532    74.270  1214.21
  40   1.6033     59.150  0.8366    74.538  1245.11
  41   1.6221     58.540  0.7945    75.712  1275.91
  42   1.6343     59.450  0.7663    76.482  1306.87
  43   1.6533     58.700  0.7496    77.040  1337.71
  44   1.6377     59.330  0.7170    78.044  1368.51
  45   1.6366     59.430  0.7050    78.294  1399.28
  46   1.6701     59.630  0.6682    79.296  1430.25
  47   1.6567     59.860  0.6473    79.972  1461.04
  48   1.7283     59.120  0.6206    80.668  1491.94
  49   1.6851     60.110  0.6075    80.974  1524.36
  50   1.7436     59.860  0.5706    81.970  1555.38
  51   1.7441     58.940  0.5576    82.780  1586.36
  52   1.7835     59.420  0.5460    82.870  1618.82
  53   1.8128     59.220  0.5155    83.776  1649.64
  54   1.8099     59.780  0.4973    84.256  1680.68
  55   1.7489     60.750  0.4695    85.094  1711.67
