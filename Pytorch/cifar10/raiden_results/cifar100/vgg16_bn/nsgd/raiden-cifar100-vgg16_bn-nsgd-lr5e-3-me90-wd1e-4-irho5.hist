Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--batch-size=256', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '5', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 53976100864 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.1509      4.590  4.4873     2.344  34.84
   2   3.8673      7.730  4.0976     5.386  65.51
   3   3.7116     10.870  3.8721     7.922  94.31
   4   3.4515     14.260  3.6447    11.272  123.33
   5   3.2897     17.840  3.4016    15.296  152.56
   6   2.9771     23.460  3.1518    19.690  183.58
   7   2.8178     26.700  2.9228    24.362  212.32
   8   2.7898     30.330  2.7375    28.268  241.52
   9   2.4826     35.160  2.5324    32.930  270.50
  10   2.2328     40.060  2.3619    36.662  300.01
  11   2.1909     40.860  2.2147    40.286  331.24
  12   2.0657     44.180  2.1351    42.438  362.44
  13   2.0441     45.000  1.9973    45.584  391.63
  14   1.8603     49.180  1.8787    48.502  422.65
  15   1.7844     50.980  1.7688    51.214  453.88
  16   1.8564     49.660  1.6863    53.140  485.13
  17   2.1235     46.340  1.6056    55.116  516.22
  18   1.7611     52.840  1.5307    56.950  547.24
  19   1.6786     54.970  1.4529    59.210  578.39
  20   1.6498     54.810  1.4051    60.432  609.50
  21   1.7771     53.980  1.3353    62.078  640.61
  22   1.7012     54.440  1.2833    63.302  671.62
  23   1.6903     56.290  1.2124    65.210  702.78
  24   1.6264     56.510  1.1675    66.320  733.87
  25   1.6305     58.400  1.1289    67.316  764.91
  26   1.6347     57.660  1.0670    68.952  796.12
  27   1.5096     59.880  1.0192    70.140  827.31
  28   1.5678     58.970  0.9773    71.414  858.42
  29   1.5859     59.580  0.9172    72.962  889.52
  30   1.5726     59.940  0.8924    73.588  920.60
  31   1.5572     60.720  0.8520    74.658  951.83
  32   1.5783     60.530  0.8189    75.430  982.91
  33   1.5446     61.900  0.7813    76.622  1013.87
  34   1.5591     62.070  0.7318    77.812  1045.09
  35   1.5744     61.370  0.6936    78.740  1074.59
  36   1.5903     61.230  0.6662    79.672  1105.63
  37   1.5820     62.430  0.6311    80.738  1138.26
  38   1.5769     62.110  0.5955    81.678  1169.53
  39   1.5920     63.020  0.5743    82.280  1200.75
  40   1.6074     62.620  0.5453    83.130  1231.87
  41   1.5857     62.870  0.5218    83.912  1262.94
  42   1.6518     63.260  0.4942    84.656  1294.20
  43   1.6402     63.160  0.4632    85.658  1325.41
  44   1.6656     63.690  0.4304    86.550  1358.12
  45   1.7173     63.530  0.4088    87.068  1389.14
  46   1.6819     64.040  0.3825    87.798  1420.35
  47   1.7420     63.430  0.3519    89.034  1451.59
