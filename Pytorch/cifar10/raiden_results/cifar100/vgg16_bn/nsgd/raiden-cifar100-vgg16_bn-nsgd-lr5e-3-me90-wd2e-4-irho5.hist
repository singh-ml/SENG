Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--batch-size=256', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '5', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 60441773056 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.1575      4.470  4.4545     2.376  36.31
   2   3.9201      7.680  4.1003     5.180  65.05
   3   3.6641     10.940  3.8966     7.702  95.42
   4   3.3933     16.110  3.6818    10.742  124.85
   5   3.2850     18.920  3.4328    14.774  153.52
   6   2.9979     23.330  3.1774    19.576  182.67
   7   2.7945     27.770  2.9398    24.576  211.69
   8   2.5701     32.380  2.7160    28.420  240.72
   9   2.3932     36.020  2.5269    32.892  271.71
  10   2.2754     38.650  2.3621    36.614  302.84
  11   2.1703     41.300  2.2187    40.008  331.97
  12   2.1834     42.170  2.0825    43.250  360.98
  13   2.0843     44.950  1.9741    46.038  390.55
  14   1.8549     49.390  1.8745    48.482  420.16
  15   1.8625     49.950  1.7696    51.018  452.73
  16   1.8868     49.520  1.7052    52.766  483.68
  17   1.7571     52.790  1.6020    55.222  513.28
  18   1.8337     51.770  1.5479    56.714  544.25
  19   1.8646     51.330  1.4772    58.650  573.27
  20   1.6902     54.750  1.4474    59.362  604.41
  21   1.6942     54.320  1.3517    61.532  634.03
  22   1.6528     56.140  1.2878    63.252  665.07
  23   1.6292     56.800  1.2289    64.372  694.50
  24   1.6486     55.930  1.1698    66.376  725.66
  25   1.7252     56.140  1.1270    67.638  755.25
  26   1.6456     58.300  1.0728    68.604  786.28
  27   1.5413     59.210  1.0352    69.740  817.20
  28   1.5333     60.470  0.9772    71.346  848.39
  29   1.5161     60.660  0.9350    72.560  879.51
  30   1.5481     60.260  0.9012    73.348  908.96
  31   1.5400     60.290  0.8621    74.608  940.00
  32   1.4900     61.990  0.8192    75.596  971.20
  33   1.4999     61.440  0.7909    76.292  1003.94
  34   1.5144     62.070  0.7376    77.766  1035.47
  35   1.5268     62.600  0.7094    78.790  1066.45
  36   1.6819     60.860  0.6646    79.734  1097.42
  37   1.5414     63.440  0.6426    80.598  1128.60
  38   1.5263     63.120  0.6143    81.208  1159.61
  39   1.5117     64.100  0.5780    82.124  1190.62
  40   1.5165     63.870  0.5460    83.122  1221.85
  41   1.5580     63.910  0.5183    83.924  1253.00
  42   1.6491     63.000  0.4771    85.262  1282.43
  43   1.6382     63.470  0.4623    85.518  1313.37
  44   1.6075     63.540  0.4382    86.430  1344.33
  45   1.6263     64.000  0.4155    86.840  1375.46
  46   1.6621     64.030  0.3896    87.836  1406.50
  47   1.6423     64.200  0.3670    88.468  1437.54
  48   1.6315     65.280  0.3446    89.354  1468.50
  49   1.7239     64.480  0.3166    89.812  1499.70
  50   1.7805     64.700  0.3014    90.316  1530.69
  51   1.7001     64.720  0.2797    91.120  1563.28
  52   1.7376     65.170  0.2650    91.606  1594.27
  53   1.8416     64.790  0.2478    92.208  1625.36
  54   1.8795     64.980  0.2278    92.872  1656.37
  55   1.9118     65.500  0.2221    92.780  1687.38
  56   1.8441     65.310  0.2014    93.588  1718.33
  57   1.9022     64.900  0.1823    94.184  1749.54
  58   1.9572     64.830  0.1685    94.590  1780.54
