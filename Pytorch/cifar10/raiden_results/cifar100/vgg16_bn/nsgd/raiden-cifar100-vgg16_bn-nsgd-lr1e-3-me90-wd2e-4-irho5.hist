Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--batch-size=256', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '5', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 62059225600 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.8025     10.790  4.3041     4.612  36.28
   2   3.5199     13.880  3.7228    11.312  68.80
   3   3.2507     19.900  3.4610    15.362  99.98
   4   3.0879     23.060  3.2460    19.084  131.06
   5   2.8773     26.290  3.0405    23.238  162.01
   6   2.6952     30.690  2.8641    26.526  192.90
   7   2.5825     32.720  2.7045    29.880  224.03
   8   2.5696     33.600  2.5656    32.586  254.92
   9   2.3469     37.590  2.4470    35.084  285.84
  10   2.2469     40.130  2.3176    37.932  316.78
  11   2.2746     39.770  2.2280    40.016  347.85
  12   2.1594     41.910  2.1225    42.344  378.84
  13   2.0979     43.620  2.0386    44.394  409.83
  14   2.0179     45.690  1.9578    46.062  442.41
  15   2.0383     45.780  1.8991    47.584  471.58
  16   1.9439     47.990  1.8152    49.918  502.70
  17   1.9021     48.510  1.7543    51.130  533.64
  18   1.8589     50.640  1.6852    52.814  564.60
  19   1.9648     47.360  1.6255    54.128  595.70
  20   1.7578     52.000  1.5679    55.508  626.81
  21   1.7833     51.210  1.5250    56.672  657.81
  22   1.7618     52.510  1.4742    58.130  688.81
  23   1.6977     54.370  1.4258    59.332  719.96
  24   1.6600     54.650  1.3762    60.388  751.06
  25   1.6428     55.680  1.3380    61.578  782.04
  26   1.6863     54.410  1.2998    62.374  814.50
  27   1.7108     54.150  1.2603    63.392  845.68
  28   1.6377     56.230  1.2126    64.588  876.77
  29   1.6033     57.350  1.1730    65.486  907.79
  30   1.6117     56.690  1.1412    66.528  938.70
  31   1.5961     56.970  1.1051    67.458  969.85
  32   1.6540     56.170  1.0675    68.372  1002.58
  33   1.5733     57.940  1.0371    69.120  1035.08
  34   1.6548     56.180  0.9915    70.428  1065.94
  35   1.6076     58.410  0.9633    71.268  1097.10
  36   1.7294     55.960  0.9296    71.904  1128.21
  37   1.5911     58.960  0.9064    72.742  1159.23
  38   1.5788     58.370  0.8797    73.434  1191.77
  39   1.6314     58.160  0.8441    74.374  1222.89
  40   1.6329     58.170  0.8113    75.346  1253.95
  41   1.6889     57.830  0.7943    75.894  1286.85
  42   1.6767     58.460  0.7608    76.482  1317.77
  43   1.6455     59.290  0.7334    77.372  1348.92
  44   1.7003     58.370  0.7080    78.158  1380.04
  45   1.7387     58.800  0.6779    78.900  1411.00
  46   1.6472     59.530  0.6517    79.698  1441.92
  47   1.8002     58.010  0.6368    80.240  1473.06
  48   1.7600     58.560  0.6176    80.622  1504.18
  49   1.7207     59.320  0.5918    81.422  1535.19
  50   1.7314     60.190  0.5603    82.256  1566.12
  51   1.7329     59.510  0.5627    82.124  1597.28
  52   1.7613     59.930  0.5273    83.346  1628.37
  53   1.7587     59.550  0.5099    83.882  1659.35
  54   1.7686     59.430  0.4869    84.524  1690.36
  55   1.8379     59.410  0.4777    84.806  1722.93
  56   1.8576     59.380  0.4518    85.574  1754.09
  57   1.8113     60.400  0.4531    85.566  1785.19
  58   1.8326     60.640  0.4177    86.646  1816.22
  59   1.8487     60.190  0.4041    87.094  1847.12
  60   1.9169     59.480  0.3905    87.420  1878.22
  61   1.9115     59.940  0.3784    87.748  1909.35
  62   1.8892     59.730  0.3650    88.364  1940.29
  63   1.8692     61.070  0.3469    88.810  1971.24
  64   1.9555     60.810  0.3340    89.172  2002.21
  65   1.9855     60.590  0.3265    89.600  2033.32
  66   2.0175     59.770  0.3176    89.782  2064.26
  67   1.9657     60.300  0.3023    90.200  2095.20
