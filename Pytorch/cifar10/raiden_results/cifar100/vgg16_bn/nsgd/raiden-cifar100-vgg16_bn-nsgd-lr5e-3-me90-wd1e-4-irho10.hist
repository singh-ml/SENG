Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--batch-size=256', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 55592244736 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.3102      3.440  4.5416     1.800  35.00
   2   4.1531      4.160  4.3105     3.040  65.54
   3   3.9754      7.300  4.1526     4.330  94.72
   4   3.7913      8.640  3.9787     6.176  123.44
   5   3.6146     11.080  3.7734     8.936  152.74
   6   3.2974     17.290  3.5290    12.780  182.00
   7   3.2096     19.360  3.2638    17.466  211.03
   8   2.9240     24.890  3.0135    22.374  242.20
   9   2.7047     29.860  2.8131    26.208  273.41
  10   2.5911     32.440  2.6032    31.080  304.48
  11   2.5000     34.950  2.4223    35.002  335.45
  12   2.2606     39.840  2.2806    38.428  364.89
  13   2.1437     43.250  2.1282    42.416  395.97
  14   2.2472     41.730  2.0156    45.192  426.93
  15   2.0457     46.900  1.9018    47.764  457.96
  16   1.8759     48.950  1.7964    50.488  489.16
  17   1.9224     48.650  1.7131    52.336  520.37
  18   1.8411     50.060  1.6389    54.412  551.40
  19   1.7648     52.950  1.5620    56.354  582.41
  20   1.6855     54.050  1.4881    58.228  612.14
  21   1.7760     53.210  1.4140    60.240  643.34
  22   1.8107     53.190  1.3593    61.504  674.42
  23   1.6317     55.960  1.3048    62.930  705.54
  24   1.6809     55.600  1.2438    64.448  736.72
  25   1.6866     55.810  1.1971    65.748  767.92
  26   1.8649     51.520  1.1366    67.322  798.98
  27   1.6261     57.470  1.1125    67.804  829.99
  28   1.5429     59.540  1.0442    69.672  860.96
  29   1.5144     60.820  0.9902    71.096  892.11
  30   1.5186     60.690  0.9524    72.240  923.20
  31   1.7976     56.180  0.9199    73.324  954.24
  32   1.4908     61.220  0.9287    72.890  985.23
  33   1.5322     60.560  0.8521    74.846  1016.43
  34   1.5204     61.580  0.8082    76.138  1049.08
  35   1.4528     62.810  0.7719    77.266  1080.12
  36   1.4846     62.590  0.7316    78.082  1111.10
  37   1.5038     62.730  0.7048    78.880  1142.27
  38   1.4760     62.580  0.6623    80.066  1173.47
  39   1.6276     61.270  0.6394    80.686  1204.56
  40   1.5722     62.700  0.5995    81.852  1235.58
  41   1.5362     63.980  0.5762    82.470  1266.76
  42   1.5646     63.190  0.5486    83.308  1297.98
  43   1.5694     63.240  0.5285    84.016  1329.10
  44   1.5639     63.820  0.4863    85.204  1360.14
  45   1.6230     63.590  0.4625    85.968  1391.13
  46   1.5451     64.770  0.4467    86.364  1422.30
  47   1.6270     64.380  0.4241    86.944  1453.38
  48   1.6273     63.860  0.4037    87.518  1484.48
  49   1.6924     64.100  0.3723    88.546  1515.72
  50   1.6138     65.380  0.3584    88.950  1546.95
  51   1.6276     64.880  0.3461    89.150  1577.94
  52   1.6894     64.850  0.3197    89.968  1608.92
  53   1.7656     63.930  0.2949    90.668  1640.10
  54   1.7009     64.870  0.2827    91.124  1671.26
  55   1.8214     65.190  0.2559    91.732  1702.39
  56   1.7217     65.310  0.2552    92.054  1733.35
  57   1.7705     65.410  0.2333    92.688  1764.57
  58   1.8385     65.430  0.2167    93.108  1795.78
  59   1.7941     65.330  0.2063    93.476  1826.82
  60   1.8654     65.270  0.1806    94.262  1857.84
  61   1.7834     65.910  0.1798    94.350  1889.06
  62   1.8314     65.690  0.1646    94.818  1920.27
  63   1.9112     65.960  0.1493    95.270  1951.38
  64   1.9485     65.830  0.1408    95.600  1982.39
  65   1.8997     66.070  0.1292    95.902  2013.58
  66   1.9277     65.940  0.1236    96.032  2044.73
  67   2.0327     66.260  0.1142    96.306  2075.78
