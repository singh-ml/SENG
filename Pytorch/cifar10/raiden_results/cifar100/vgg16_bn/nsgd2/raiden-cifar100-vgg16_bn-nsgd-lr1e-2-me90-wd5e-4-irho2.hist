Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '2', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 57208471040 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.9319      7.270  4.3331     3.570  36.42
   2   3.6548     10.900  3.8976     7.922  65.23
   3   3.3662     15.910  3.6474    11.680  94.40
   4   3.3572     17.560  3.4026    15.752  123.68
   5   3.0530     24.110  3.1743    19.934  152.49
   6   2.8756     26.220  2.9381    24.296  181.59
   7   2.5142     32.470  2.7281    28.700  210.42
   8   2.5808     31.900  2.5255    33.046  239.27
   9   2.3606     36.840  2.3836    35.992  270.39
  10   2.1506     41.860  2.2206    39.830  299.68
  11   2.1311     41.940  2.0874    43.412  328.77
  12   2.0660     44.190  1.9864    45.778  357.94
  13   1.9889     46.240  1.8717    48.374  389.04
  14   1.9613     47.900  1.7774    50.764  418.07
  15   1.9193     48.510  1.6984    52.794  448.97
  16   1.7598     52.310  1.6189    54.698  478.23
  17   1.7921     51.480  1.5418    56.840  509.21
  18   1.7746     51.570  1.4696    58.442  540.06
  19   1.7483     53.170  1.4157    60.198  570.88
  20   1.6817     55.170  1.3624    61.286  600.04
  21   1.6391     56.440  1.2971    63.040  631.16
  22   1.6118     56.130  1.2408    64.440  662.17
  23   1.5735     57.480  1.1883    65.776  693.17
  24   1.6348     57.320  1.1436    66.948  722.52
  25   1.5398     59.260  1.1017    68.134  753.54
  26   1.5562     58.680  1.0439    69.418  784.51
  27   1.7642     56.530  1.0061    70.404  815.40
  28   1.5712     59.490  0.9642    71.726  846.55
  29   1.6623     57.870  0.9372    72.414  877.58
  30   1.5374     60.910  0.8922    73.816  908.56
  31   1.5807     59.900  0.8555    74.708  939.44
  32   1.5541     60.360  0.8144    75.828  970.56
  33   1.5433     61.260  0.7791    76.592  1001.74
  34   1.5603     61.170  0.7501    77.432  1031.12
  35   1.5426     62.240  0.7108    78.654  1062.03
  36   1.5685     61.820  0.6803    79.446  1093.25
  37   1.4972     62.720  0.6448    80.508  1124.22
  38   1.7034     61.120  0.6072    81.468  1155.24
  39   1.6238     61.770  0.5881    82.000  1186.26
  40   1.5717     62.800  0.5694    82.294  1217.41
  41   1.5422     63.500  0.5203    83.826  1248.40
  42   1.6351     63.220  0.4984    84.486  1279.31
  43   1.6504     63.700  0.4647    85.524  1310.19
  44   1.6720     63.710  0.4414    86.196  1341.23
  45   1.7188     63.670  0.4179    86.890  1372.06
  46   1.7338     63.830  0.4009    87.434  1402.89
  47   1.6913     64.350  0.3689    88.432  1433.70
  48   1.6819     64.710  0.3469    88.968  1464.78
  49   1.6404     65.680  0.3201    89.828  1495.70
  50   1.7293     64.810  0.3043    90.312  1526.65
  51   1.7448     64.890  0.2819    91.042  1557.66
  52   1.8365     64.530  0.2639    91.636  1588.72
  53   1.8059     64.840  0.2440    92.244  1619.57
  54   1.8177     64.520  0.2362    92.514  1650.59
  55   1.8636     65.100  0.2164    93.128  1681.72
  56   1.8552     65.290  0.1998    93.586  1713.10
