Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901920768 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6734      0.950  9.6463     1.062  11.78
   2   4.6738      0.910  9.6495     0.984  21.98
   3   4.6719      0.990  9.6488     1.022  32.21
   4   4.6743      0.900  9.6493     1.004  42.46
   5   4.6729      0.950  9.6492     1.014  52.78
   6   4.6724      0.940  9.6546     0.996  63.06
   7   4.6735      0.940  9.6510     1.066  73.27
   8   4.6722      0.930  9.6512     1.006  83.53
   9   4.6724      0.970  9.6510     0.970  93.73
  10   4.6733      0.980  9.6515     1.020  104.48
  11   4.6727      0.980  9.6505     1.086  114.84
  12   4.6735      0.960  9.6472     0.994  125.11
  13   4.6741      0.900  9.6531     0.994  135.33
  14   4.6714      0.990  9.6479     1.088  145.57
  15   4.6729      0.910  9.6560     0.982  155.86
  16   4.6739      0.960  9.6474     1.006  166.06
  17   4.6738      0.940  9.6494     1.036  176.24
  18   4.6725      0.890  9.6538     1.000  186.45
  19   4.6725      0.970  9.6528     0.994  196.71
  20   4.6731      0.960  9.6573     0.918  206.93
  21   4.6743      1.020  9.6501     1.042  217.16
  22   4.6741      0.940  9.6510     0.954  227.49
  23   4.6725      0.930  9.6502     1.016  237.72
  24   4.6744      1.000  9.6542     1.014  247.97
  25   4.6734      0.980  9.6482     0.978  258.25
  26   4.6733      1.000  9.6515     0.992  268.48
  27   4.6717      0.960  9.6511     0.976  278.69
  28   4.6728      0.880  9.6545     0.992  288.92
  29   4.6727      0.930  9.6506     0.984  299.18
  30   4.6718      0.930  9.6520     0.948  309.38
  31   4.6722      0.980  9.6494     1.012  319.62
  32   4.6723      0.910  9.6541     0.970  329.87
  33   4.6732      0.950  9.6506     1.010  340.04
  34   4.6729      0.940  9.6574     0.940  350.31
  35   4.6728      0.990  9.6499     0.954  360.50
  36   4.6723      0.940  9.6509     0.964  370.73
  37   4.6722      0.960  9.6486     0.948  380.99
  38   4.6730      0.950  9.6460     1.008  391.26
  39   4.6731      0.980  9.6515     0.936  401.44
  40   4.6727      0.920  9.6452     0.968  411.80
  41   4.6735      0.970  9.6493     1.096  422.01
  42   4.6737      0.960  9.6488     1.018  432.21
  43   4.6740      0.930  9.6574     1.030  442.49
  44   4.6715      0.960  9.6526     1.072  452.69
  45   4.6731      0.970  9.6435     0.976  462.76
  46   4.6723      0.980  9.6518     1.048  472.80
  47   4.6736      0.980  9.6503     1.054  482.76
  48   4.6725      0.950  9.6534     1.040  492.71
  49   4.6734      0.920  9.6511     0.976  502.65
  50   4.6740      0.870  9.6462     0.982  512.66
  51   4.6722      0.960  9.6481     1.074  522.60
  52   4.6726      0.940  9.6501     0.990  532.54
  53   4.6724      0.950  9.6517     0.986  542.58
  54   4.6719      0.920  9.6496     0.988  552.52
  55   4.6730      0.940  9.6578     1.020  562.46
  56   4.6723      0.960  9.6473     1.048  572.41
  57   4.6737      0.980  9.6507     1.030  582.44
  58   4.6743      0.940  9.6496     1.000  592.41
  59   4.6743      0.990  9.6522     1.068  602.33
  60   4.6745      0.910  9.6501     1.006  612.30
  61   4.6729      0.960  9.6511     1.064  622.24
  62   4.6728      0.970  9.6510     1.046  632.19
  63   4.6718      0.960  9.6529     0.966  642.13
  64   4.6731      0.900  9.6516     1.046  652.16
  65   4.6719      1.020  9.6543     1.014  662.10
  66   4.6732      0.900  9.6516     1.020  672.05
  67   4.6737      0.940  9.6507     0.986  682.07
  68   4.6732      0.900  9.6509     1.028  692.04
  69   4.6730      0.930  9.6499     1.012  701.99
  70   4.6745      0.930  9.6524     0.998  711.93
  71   4.6730      0.900  9.6482     0.976  721.96
  72   4.6732      0.930  9.6503     0.994  731.90
  73   4.6722      0.920  9.6472     1.016  741.84
  74   4.6738      0.940  9.6544     1.008  751.84
  75   4.6736      0.920  9.6474     1.032  761.78
  76   4.6731      0.930  9.6471     1.054  771.71
  77   4.6740      0.960  9.6456     1.054  781.65
  78   4.6729      0.940  9.6484     1.008  791.67
  79   4.6745      1.010  9.6534     0.978  801.61
  80   4.6726      0.950  9.6504     0.940  811.53
  81   4.6730      0.960  9.6460     1.008  821.45
  82   4.6728      0.940  9.6465     1.040  831.49
  83   4.6733      0.960  9.6544     0.992  841.44
  84   4.6737      0.950  9.6460     1.050  851.40
  85   4.6732      0.930  9.6512     1.118  861.43
