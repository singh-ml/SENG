Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 20755306496 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6788      1.110  16.7561     1.038  19.24
   2   4.6792      1.080  16.7497     1.012  37.23
   3   4.6790      1.090  16.7586     1.064  55.31
   4   4.6774      1.130  16.7536     0.978  73.26
   5   4.6782      1.090  16.7497     1.004  91.30
   6   4.6795      1.120  16.7540     1.094  109.26
   7   4.6790      1.100  16.7528     1.006  127.28
   8   4.6794      1.120  16.7584     1.018  145.23
   9   4.6780      1.090  16.7541     1.016  163.29
  10   4.6782      1.080  16.7552     1.034  181.25
  11   4.6783      1.080  16.7525     0.996  199.29
  12   4.6790      1.040  16.7525     1.022  217.27
  13   4.6788      1.100  16.7527     0.988  235.31
  14   4.6788      1.070  16.7518     1.022  253.26
  15   4.6784      1.130  16.7539     0.998  271.28
  16   4.6795      1.060  16.7604     0.998  289.25
  17   4.6789      1.170  16.7566     1.006  307.28
  18   4.6787      1.000  16.7591     0.910  325.22
  19   4.6787      1.130  16.7543     1.072  343.28
  20   4.6782      1.120  16.7591     0.966  361.27
  21   4.6785      1.090  16.7533     1.010  379.27
  22   4.6797      1.110  16.7529     0.984  397.23
  23   4.6791      1.080  16.7512     1.034  415.24
  24   4.6797      1.030  16.7567     0.982  433.22
  25   4.6785      1.090  16.7573     1.046  451.24
  26   4.6798      1.110  16.7579     1.032  469.21
  27   4.6786      1.110  16.7561     0.948  487.23
  28   4.6783      1.140  16.7523     1.022  505.20
  29   4.6786      1.050  16.7533     0.994  523.23
  30   4.6794      1.120  16.7487     1.004  541.19
  31   4.6778      1.070  16.7510     1.020  559.18
  32   4.6788      1.090  16.7496     1.004  577.31
  33   4.6792      1.090  16.7530     1.124  595.37
  34   4.6777      1.080  16.7612     0.984  613.34
  35   4.6791      1.100  16.7566     0.940  631.36
  36   4.6781      1.080  16.7555     1.028  649.35
  37   4.6794      1.090  16.7575     1.052  667.36
  38   4.6781      1.060  16.7567     0.986  685.33
  39   4.6781      1.060  16.7528     0.972  703.37
  40   4.6790      1.110  16.7519     0.918  721.35
  41   4.6781      1.100  16.7526     1.016  739.39
  42   4.6798      1.120  16.7601     1.022  757.37
  43   4.6792      1.110  16.7550     0.994  775.41
  44   4.6784      1.110  16.7528     1.060  793.38
  45   4.6779      1.130  16.7507     0.986  811.36
  46   4.6786      1.110  16.7663     1.018  829.31
  47   4.6786      1.060  16.7597     0.990  847.32
  48   4.6780      1.130  16.7555     1.050  865.31
  49   4.6781      1.100  16.7508     1.034  883.28
  50   4.6792      1.060  16.7552     1.044  901.31
  51   4.6785      1.150  16.7563     1.092  919.25
  52   4.6788      1.080  16.7566     0.982  937.29
  53   4.6790      1.110  16.7566     0.942  955.26
  54   4.6787      1.060  16.7598     0.982  973.22
  55   4.6792      1.080  16.7495     1.046  991.25
  56   4.6787      1.090  16.7572     0.978  1009.21
  57   4.6795      1.090  16.7514     1.152  1027.19
  58   4.6786      1.050  16.7503     1.026  1045.27
  59   4.6783      1.080  16.7543     1.034  1063.23
  60   4.6789      1.080  16.7564     0.984  1081.25
  61   4.6793      1.140  16.7572     1.052  1099.21
  62   4.6787      1.080  16.7614     0.948  1117.27
  63   4.6786      1.060  16.7578     1.074  1135.24
  64   4.6787      1.090  16.7504     1.024  1153.29
  65   4.6784      1.130  16.7515     1.028  1171.26
  66   4.6787      1.050  16.7539     1.026  1189.32
  67   4.6785      1.060  16.7586     0.954  1207.30
  68   4.6788      1.030  16.7568     1.044  1225.35
  69   4.6798      1.070  16.7544     1.020  1243.31
  70   4.6792      1.090  16.7510     1.084  1261.37
  71   4.6783      1.080  16.7527     1.056  1279.32
  72   4.6797      1.090  16.7549     1.034  1297.36
  73   4.6789      1.090  16.7514     1.028  1315.33
  74   4.6791      1.120  16.7568     1.050  1333.33
  75   4.6795      1.140  16.7575     0.986  1351.29
  76   4.6786      1.100  16.7601     1.018  1369.33
  77   4.6782      1.090  16.7555     0.974  1387.31
  78   4.6790      1.090  16.7533     1.050  1405.34
  79   4.6783      1.120  16.7490     1.050  1423.29
  80   4.6795      1.090  16.7552     1.048  1441.32
  81   4.6787      1.120  16.7603     0.980  1459.27
  82   4.6781      1.050  16.7612     1.002  1477.34
  83   4.6787      1.100  16.7576     0.980  1495.30
  84   4.6783      1.110  16.7614     0.940  1513.34
  85   4.6778      1.130  16.7512     1.008  1531.29
