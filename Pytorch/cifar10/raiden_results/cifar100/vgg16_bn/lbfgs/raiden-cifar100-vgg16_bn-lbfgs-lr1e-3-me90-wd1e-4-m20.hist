Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8904017920 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6691      1.120  7.3004     1.054  11.91
   2   4.6687      1.060  7.2997     0.980  22.20
   3   4.6684      1.120  7.2981     1.016  32.50
   4   4.6677      1.150  7.3023     0.992  42.78
   5   4.6684      1.150  7.2984     0.964  53.14
   6   4.6687      1.130  7.3008     0.962  63.44
   7   4.6696      1.120  7.3020     0.996  73.77
   8   4.6689      1.140  7.3046     0.978  84.08
   9   4.6695      1.080  7.3013     1.008  94.42
  10   4.6685      1.080  7.2988     1.034  104.70
  11   4.6691      1.080  7.2992     1.018  114.99
  12   4.6692      1.130  7.2973     1.062  125.36
  13   4.6685      1.130  7.2987     1.020  135.67
  14   4.6691      1.130  7.2993     1.018  145.96
  15   4.6696      1.140  7.2992     0.948  156.36
  16   4.6685      1.090  7.3064     0.968  166.61
  17   4.6684      1.090  7.3016     1.000  176.88
  18   4.6696      1.120  7.3041     1.078  187.15
  19   4.6684      1.140  7.2975     1.026  197.49
  20   4.6681      1.200  7.2968     0.986  207.77
  21   4.6692      1.120  7.2980     0.996  218.06
  22   4.6695      1.100  7.3012     0.954  228.37
  23   4.6697      1.140  7.2970     1.022  238.68
  24   4.6684      1.120  7.3027     0.988  248.98
  25   4.6683      1.090  7.2978     0.984  259.25
  26   4.6686      1.170  7.2979     0.944  269.62
  27   4.6702      1.150  7.3007     0.940  279.90
  28   4.6702      1.080  7.3030     1.028  290.20
  29   4.6694      1.120  7.2934     1.062  300.49
  30   4.6684      1.170  7.3009     1.012  310.80
  31   4.6678      1.110  7.3026     1.062  321.07
  32   4.6688      1.160  7.2961     0.998  331.39
  33   4.6698      1.130  7.2972     1.024  341.75
  34   4.6690      1.120  7.2943     0.994  352.06
  35   4.6686      1.140  7.3008     0.982  362.33
  36   4.6685      1.110  7.3030     1.050  372.66
  37   4.6692      1.060  7.3024     1.018  382.97
  38   4.6686      1.160  7.2984     1.016  393.27
  39   4.6682      1.080  7.3067     0.968  403.55
  40   4.6688      1.120  7.3036     0.994  413.95
  41   4.6692      1.130  7.2983     0.992  424.21
  42   4.6691      1.140  7.3051     0.988  434.51
  43   4.6690      1.110  7.2979     1.058  444.82
  44   4.6685      1.130  7.2936     1.022  455.20
  45   4.6691      1.130  7.2975     1.022  465.46
  46   4.6690      1.140  7.2991     1.034  475.76
  47   4.6688      1.100  7.3044     0.972  486.13
  48   4.6693      1.140  7.2942     0.994  496.41
  49   4.6695      1.140  7.2982     1.050  506.69
  50   4.6688      1.140  7.2991     1.032  517.01
  51   4.6685      1.180  7.3011     1.006  527.38
  52   4.6685      1.160  7.2980     1.036  537.72
  53   4.6693      1.140  7.3014     1.008  548.00
  54   4.6692      1.160  7.3022     0.988  558.32
  55   4.6686      1.200  7.2998     1.002  568.58
  56   4.6692      1.110  7.3013     1.040  578.87
  57   4.6691      1.100  7.3024     1.058  589.25
  58   4.6694      1.110  7.2964     1.004  599.54
  59   4.6681      1.120  7.2967     1.010  609.82
  60   4.6697      1.100  7.3002     0.966  620.08
  61   4.6691      1.120  7.3007     0.982  630.48
  62   4.6689      1.140  7.3011     1.016  640.71
  63   4.6687      1.110  7.2972     1.006  650.99
  64   4.6691      1.150  7.2965     0.982  661.25
  65   4.6680      1.100  7.2999     0.996  671.60
  66   4.6693      1.110  7.3000     0.936  681.89
  67   4.6683      1.110  7.2952     1.024  692.16
  68   4.6685      1.110  7.2978     1.122  702.53
  69   4.6684      1.130  7.2973     1.044  712.84
  70   4.6691      1.180  7.3021     0.988  723.12
  71   4.6695      1.120  7.2989     1.022  733.42
  72   4.6683      1.120  7.2942     1.028  743.71
  73   4.6684      1.130  7.3015     1.052  753.98
  74   4.6683      1.110  7.3005     0.976  764.28
  75   4.6681      1.110  7.3008     1.004  774.64
  76   4.6702      1.130  7.2973     0.960  784.91
  77   4.6681      1.110  7.3042     0.900  795.17
  78   4.6685      1.130  7.2984     0.972  805.56
  79   4.6684      1.130  7.2985     1.060  815.86
  80   4.6686      1.150  7.2961     0.966  826.13
  81   4.6681      1.170  7.2980     0.948  836.37
  82   4.6688      1.160  7.3012     1.034  846.73
  83   4.6687      1.150  7.2994     1.032  857.02
  84   4.6681      1.110  7.3032     1.006  867.28
  85   4.6695      1.140  7.3013     0.988  877.59
  86   4.6695      1.130  7.2970     1.086  887.91
  87   4.6682      1.150  7.2986     1.002  898.25
  88   4.6688      1.130  7.2996     1.020  908.54
  89   4.6694      1.120  7.2951     1.064  918.91
  90   4.6684      1.100  7.2948     1.004  929.18
