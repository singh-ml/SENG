Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9981299712 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6962      1.040  16.7942     1.104  12.81
   2   4.6988      1.020  16.7901     0.996  23.75
   3   4.6976      1.050  16.7922     0.928  34.55
   4   4.6967      1.040  16.7958     1.052  45.33
   5   4.6981      0.980  16.7987     1.040  56.20
   6   4.6969      1.010  16.7900     0.960  66.98
   7   4.6958      1.050  16.7907     1.012  77.74
   8   4.6978      1.010  16.7918     0.978  88.61
   9   4.6968      0.940  16.7890     0.962  99.37
  10   4.6981      0.980  16.7895     1.138  110.15
  11   4.6972      1.000  16.7978     0.976  121.00
  12   4.6986      1.020  16.7894     1.038  131.77
  13   4.6974      1.000  16.7996     1.010  142.56
  14   4.6977      1.050  16.7922     0.982  153.39
  15   4.6973      1.030  16.7901     1.020  164.18
  16   4.6967      1.000  16.7896     1.020  174.96
  17   4.6984      0.990  16.7924     1.046  185.75
  18   4.6967      1.000  16.7891     1.050  196.63
  19   4.6973      1.000  16.7895     0.934  207.43
  20   4.6978      0.960  16.7914     1.074  218.19
  21   4.6962      0.960  16.7934     1.004  229.07
  22   4.6983      0.960  16.7926     0.954  239.83
  23   4.6977      1.040  16.7960     1.016  250.59
  24   4.6980      1.040  16.7952     1.050  261.45
  25   4.6978      1.020  16.7961     0.978  272.23
  26   4.6966      0.980  16.7893     1.016  283.01
  27   4.6992      0.960  16.7910     0.956  293.77
  28   4.6972      0.990  16.7931     0.992  304.62
  29   4.6969      1.020  16.7944     0.934  315.40
  30   4.6982      1.030  16.7983     0.984  326.24
  31   4.6964      0.920  16.7968     0.996  337.07
  32   4.6971      1.020  16.7910     1.042  347.84
  33   4.6968      1.020  16.7951     0.954  358.65
  34   4.6963      1.020  16.7881     1.016  369.49
  35   4.7001      0.990  16.7956     0.932  380.28
  36   4.6981      1.000  16.7930     1.036  391.05
  37   4.6990      0.990  16.7953     0.990  401.85
  38   4.6983      1.020  16.7891     1.036  412.74
  39   4.6975      0.990  16.7937     1.012  423.50
  40   4.6982      0.970  16.7948     1.010  434.30
  41   4.6981      0.990  16.7966     0.984  445.25
  42   4.6973      0.940  16.7880     0.956  456.02
  43   4.6972      0.980  16.7942     1.030  466.78
  44   4.6969      1.020  16.7964     0.850  477.61
  45   4.6972      1.000  16.7862     1.014  488.37
  46   4.6976      0.960  16.7929     1.088  499.14
  47   4.6967      1.020  16.7932     0.942  510.02
  48   4.6971      1.020  16.7907     1.036  520.77
  49   4.6969      1.040  16.7882     0.978  531.61
  50   4.6976      1.030  16.7897     0.978  542.45
  51   4.6979      0.970  16.7908     0.988  553.26
  52   4.6970      0.980  16.7931     0.966  564.06
  53   4.6976      1.000  16.7989     1.086  574.86
  54   4.6975      1.040  16.7972     0.968  585.71
  55   4.6970      0.990  16.7944     0.948  596.49
  56   4.6951      1.030  16.7950     1.010  607.28
  57   4.6974      0.990  16.7951     1.046  618.18
  58   4.6965      1.040  16.7904     1.038  628.97
  59   4.6987      1.020  16.7932     0.952  639.76
  60   4.6964      0.990  16.7916     0.938  650.56
  61   4.6975      1.070  16.7904     1.088  661.34
  62   4.6978      0.990  16.7980     0.984  672.09
  63   4.6952      1.030  16.7943     1.000  682.89
  64   4.6982      0.960  16.7905     1.014  693.77
  65   4.6959      1.070  16.7955     0.976  704.52
  66   4.6972      1.040  16.7949     0.934  715.29
  67   4.6990      0.970  16.7898     0.912  726.15
  68   4.6974      1.020  16.7927     0.988  736.94
  69   4.6979      0.990  16.7912     1.024  747.69
  70   4.6984      1.020  16.7917     0.962  758.55
  71   4.6972      0.980  16.7956     0.930  769.38
  72   4.6984      0.960  16.7948     0.962  780.14
  73   4.6988      1.020  16.7963     0.990  790.93
  74   4.6960      1.010  16.7924     0.944  801.77
  75   4.6994      1.020  16.7836     1.032  812.55
  76   4.6966      0.960  16.7956     0.930  823.32
  77   4.6996      1.020  16.7903     1.068  834.18
  78   4.6984      0.970  16.7939     1.026  844.97
  79   4.6973      1.030  16.7938     0.986  855.73
  80   4.6975      1.030  16.7873     1.096  866.62
  81   4.6977      1.020  16.7927     0.956  877.42
  82   4.6973      1.030  16.7942     1.006  888.19
  83   4.6967      0.990  16.7921     1.024  899.07
  84   4.6975      1.030  16.7969     1.038  909.87
  85   4.6957      1.030  16.7990     1.010  920.68
  86   4.6967      0.960  16.7977     0.940  931.46
  87   4.6982      0.990  16.7907     0.970  942.33
  88   4.6992      1.040  16.7949     0.986  953.11
  89   4.6973      0.970  16.7957     1.032  963.93
  90   4.6977      0.980  16.7934     0.998  974.70
