Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17525417984 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7017      0.750  16.8071     1.052  17.68
   2   4.7016      0.760  16.8009     0.988  33.51
   3   4.7006      0.750  16.8028     0.976  49.17
   4   4.7019      0.750  16.8068     0.926  64.76
   5   4.7004      0.760  16.8005     1.042  80.29
   6   4.7011      0.740  16.8014     0.932  95.91
   7   4.7028      0.730  16.7991     1.050  111.47
   8   4.7024      0.730  16.8055     1.008  127.10
   9   4.7008      0.770  16.8007     1.046  142.69
  10   4.7024      0.770  16.8041     0.902  158.32
  11   4.7025      0.690  16.8007     0.948  173.84
  12   4.7025      0.780  16.8040     0.998  189.40
  13   4.7015      0.750  16.7979     1.012  204.97
  14   4.6999      0.760  16.8112     0.988  220.51
  15   4.7013      0.730  16.8069     0.930  236.13
  16   4.7020      0.720  16.8025     0.960  251.67
  17   4.7028      0.700  16.7969     1.028  267.33
  18   4.7024      0.730  16.8074     0.916  282.90
  19   4.7002      0.740  16.8047     0.986  298.44
  20   4.7025      0.770  16.8062     1.006  314.10
  21   4.7016      0.750  16.8057     0.938  329.64
  22   4.7021      0.710  16.8019     0.972  345.29
  23   4.7007      0.760  16.7975     1.076  360.83
  24   4.7011      0.730  16.8054     0.996  376.48
  25   4.7023      0.770  16.8012     0.988  392.02
  26   4.7028      0.720  16.7980     1.110  407.63
  27   4.7014      0.760  16.8024     1.000  423.18
  28   4.7017      0.750  16.8095     0.982  438.71
  29   4.7014      0.740  16.8040     0.922  454.36
  30   4.7009      0.730  16.8054     0.986  469.89
  31   4.7018      0.790  16.8029     1.002  485.47
  32   4.7021      0.750  16.7984     1.026  501.03
  33   4.7024      0.720  16.8032     0.952  516.56
  34   4.7018      0.780  16.8065     0.984  532.22
  35   4.7011      0.700  16.8031     0.960  547.78
  36   4.7030      0.710  16.8013     1.036  563.43
  37   4.7023      0.760  16.8039     0.966  579.00
  38   4.7013      0.760  16.8023     0.970  594.63
  39   4.7013      0.670  16.8061     1.038  610.18
  40   4.7003      0.750  16.8041     0.932  625.82
  41   4.7014      0.720  16.8011     1.008  641.40
  42   4.7020      0.740  16.8051     0.938  656.94
  43   4.7008      0.740  16.8027     0.944  672.57
  44   4.7028      0.720  16.8033     0.990  688.14
  45   4.7028      0.770  16.7981     1.086  703.78
  46   4.7009      0.750  16.7981     1.044  719.31
  47   4.7012      0.770  16.8039     0.982  734.94
  48   4.7024      0.740  16.8056     0.960  750.50
  49   4.6999      0.750  16.8041     1.002  766.06
  50   4.7016      0.720  16.7949     1.050  781.71
  51   4.7007      0.740  16.7994     1.068  797.23
  52   4.7025      0.720  16.7968     0.996  812.87
  53   4.7013      0.760  16.8042     0.922  828.42
  54   4.7023      0.750  16.8039     0.962  844.02
  55   4.7017      0.750  16.7987     0.890  859.58
  56   4.7017      0.720  16.7980     0.982  875.13
  57   4.7018      0.800  16.7996     1.090  890.75
  58   4.7024      0.730  16.7993     1.022  906.30
  59   4.7016      0.770  16.8032     0.988  921.92
  60   4.7021      0.760  16.7984     0.986  937.45
  61   4.7029      0.750  16.7982     1.088  953.08
  62   4.7013      0.700  16.8059     0.944  968.62
  63   4.7003      0.760  16.8002     1.010  984.18
  64   4.7020      0.780  16.8021     1.048  999.80
  65   4.7023      0.730  16.8030     0.980  1015.36
  66   4.7016      0.740  16.8008     1.022  1030.99
  67   4.7015      0.710  16.8075     1.020  1046.54
  68   4.7000      0.710  16.8002     1.054  1062.18
  69   4.7009      0.770  16.7964     1.016  1077.73
  70   4.7020      0.780  16.8012     0.900  1093.28
  71   4.7027      0.820  16.7988     0.994  1108.90
  72   4.7020      0.720  16.8065     0.960  1124.46
  73   4.7010      0.770  16.8026     0.976  1140.06
  74   4.7015      0.760  16.8053     0.964  1155.60
  75   4.7024      0.750  16.8030     0.984  1171.22
  76   4.7026      0.690  16.8032     0.994  1186.74
  77   4.7015      0.770  16.8021     1.016  1202.29
  78   4.7008      0.700  16.8016     1.028  1217.88
  79   4.7027      0.680  16.7990     0.988  1233.43
  80   4.6999      0.720  16.7996     1.034  1249.09
  81   4.7023      0.710  16.8048     1.040  1264.66
  82   4.7014      0.790  16.8035     1.022  1280.31
  83   4.7017      0.720  16.8009     1.048  1295.87
  84   4.7016      0.730  16.7995     0.976  1311.44
  85   4.7006      0.760  16.8030     1.008  1327.06
  86   4.7033      0.760  16.8027     1.070  1342.63
  87   4.7016      0.780  16.8016     0.996  1358.27
  88   4.7012      0.720  16.8037     0.946  1373.82
  89   4.7025      0.750  16.8033     0.994  1389.44
  90   4.7005      0.750  16.8029     0.990  1404.96
