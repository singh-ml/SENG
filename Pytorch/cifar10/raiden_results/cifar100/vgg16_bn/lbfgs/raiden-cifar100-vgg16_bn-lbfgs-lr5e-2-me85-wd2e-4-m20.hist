Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 18601521664 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6844      0.900  9.7053     1.010  17.91
   2   4.6844      0.870  9.7014     0.992  35.19
   3   4.6845      0.870  9.7063     1.006  52.37
   4   4.6846      0.810  9.7063     0.996  69.56
   5   4.6868      0.900  9.6992     1.012  86.82
   6   4.6850      0.850  9.7060     0.918  103.99
   7   4.6859      0.830  9.7033     0.970  121.21
   8   4.6858      0.900  9.7052     0.956  138.40
   9   4.6856      0.860  9.7078     0.974  155.69
  10   4.6858      0.870  9.6999     0.972  172.87
  11   4.6856      0.960  9.7032     1.044  190.14
  12   4.6852      0.890  9.7020     1.056  207.33
  13   4.6856      0.900  9.7007     0.956  224.56
  14   4.6860      0.870  9.7013     1.010  241.75
  15   4.6842      0.860  9.7000     1.042  258.98
  16   4.6854      0.850  9.7049     1.048  276.16
  17   4.6852      0.880  9.6954     1.050  293.37
  18   4.6858      0.850  9.7026     1.048  310.55
  19   4.6869      0.860  9.7047     0.944  327.82
  20   4.6858      0.830  9.6996     0.970  345.01
  21   4.6852      0.880  9.7020     0.908  362.18
  22   4.6842      0.860  9.6966     0.962  379.41
  23   4.6851      0.830  9.7060     0.996  396.58
  24   4.6867      0.840  9.7034     0.864  413.82
  25   4.6859      0.850  9.7097     1.032  431.02
  26   4.6851      0.950  9.7009     0.976  448.27
  27   4.6854      0.890  9.7051     1.032  465.45
  28   4.6852      0.910  9.6996     1.040  482.66
  29   4.6844      0.870  9.7041     0.948  499.82
  30   4.6855      0.870  9.7010     0.948  517.04
  31   4.6841      0.950  9.7025     1.046  534.20
  32   4.6864      0.870  9.7003     0.978  551.42
  33   4.6848      0.880  9.7061     0.936  568.60
  34   4.6846      0.880  9.7013     0.992  585.78
  35   4.6856      0.870  9.7066     0.924  603.01
  36   4.6858      0.850  9.7091     1.030  620.18
  37   4.6850      0.910  9.7047     0.984  637.35
  38   4.6855      0.880  9.7027     1.044  654.51
  39   4.6860      0.880  9.7014     1.048  671.75
  40   4.6856      0.810  9.7097     0.984  688.93
  41   4.6856      0.890  9.7100     1.044  706.16
  42   4.6865      0.850  9.7000     0.976  723.35
  43   4.6847      0.870  9.7034     1.060  740.60
  44   4.6863      0.840  9.7017     1.046  757.79
  45   4.6853      0.860  9.7071     1.020  775.04
  46   4.6864      0.810  9.7004     1.124  792.22
  47   4.6859      0.960  9.6989     1.124  809.47
  48   4.6858      0.860  9.7033     0.982  826.62
  49   4.6868      0.950  9.7008     0.972  843.79
  50   4.6862      0.900  9.7049     0.990  860.96
  51   4.6847      0.870  9.6995     0.952  878.17
  52   4.6851      0.840  9.7069     1.016  895.42
  53   4.6855      0.900  9.7061     0.970  912.60
  54   4.6839      0.900  9.7024     1.026  929.81
  55   4.6846      0.870  9.7050     1.016  946.99
  56   4.6848      0.830  9.7051     0.958  964.22
  57   4.6837      0.850  9.7084     0.974  981.40
  58   4.6855      0.870  9.7074     1.002  998.65
  59   4.6858      0.840  9.6988     1.052  1015.84
  60   4.6861      0.830  9.7047     0.946  1033.10
  61   4.6868      0.890  9.7078     0.958  1050.29
  62   4.6863      0.820  9.7004     1.024  1067.52
  63   4.6852      0.900  9.7068     0.986  1084.70
  64   4.6855      0.870  9.7075     1.004  1101.90
  65   4.6856      0.890  9.7059     0.994  1119.06
  66   4.6839      0.860  9.6971     1.014  1136.24
  67   4.6849      0.880  9.6968     0.980  1153.49
  68   4.6861      0.910  9.7012     0.980  1170.66
  69   4.6854      0.890  9.7075     1.020  1187.92
  70   4.6864      0.800  9.7018     0.960  1205.06
  71   4.6859      0.890  9.7048     0.986  1222.31
  72   4.6862      0.860  9.7012     1.030  1239.46
  73   4.6860      0.930  9.7005     0.980  1256.71
  74   4.6872      0.870  9.7039     1.012  1273.89
  75   4.6839      0.870  9.6983     1.082  1291.14
  76   4.6861      0.810  9.7039     0.956  1308.32
  77   4.6855      0.930  9.7079     0.934  1325.57
  78   4.6856      0.950  9.6972     1.022  1342.76
  79   4.6852      0.880  9.7027     0.996  1359.97
  80   4.6857      0.840  9.7053     0.984  1377.14
  81   4.6863      0.920  9.7049     1.002  1394.31
  82   4.6853      0.850  9.7023     1.018  1411.55
  83   4.6859      0.920  9.7005     0.974  1428.73
  84   4.6862      0.850  9.7000     0.968  1446.00
  85   4.6863      0.920  9.6989     1.028  1463.18
