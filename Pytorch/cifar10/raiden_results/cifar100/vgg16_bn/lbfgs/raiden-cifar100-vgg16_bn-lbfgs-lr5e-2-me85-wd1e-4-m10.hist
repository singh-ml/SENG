Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17524500480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6701      0.960  7.3189     1.068  17.41
   2   4.6697      0.890  7.3219     1.096  33.39
   3   4.6688      0.970  7.3146     1.002  49.30
   4   4.6696      1.020  7.3119     0.986  65.22
   5   4.6700      0.980  7.3134     1.000  81.07
   6   4.6699      1.010  7.3185     1.088  97.07
   7   4.6699      0.970  7.3159     1.080  112.98
   8   4.6708      0.970  7.3138     1.016  128.88
   9   4.6690      0.970  7.3156     1.050  144.78
  10   4.6691      1.000  7.3145     1.068  160.68
  11   4.6702      0.990  7.3140     1.016  176.59
  12   4.6686      0.990  7.3167     0.946  192.49
  13   4.6700      0.980  7.3138     1.006  208.37
  14   4.6696      0.980  7.3151     1.044  224.26
  15   4.6715      0.930  7.3185     1.052  240.12
  16   4.6702      0.970  7.3146     1.030  256.07
  17   4.6692      0.950  7.3133     0.964  271.97
  18   4.6693      0.970  7.3112     1.094  287.93
  19   4.6687      1.010  7.3187     0.980  303.82
  20   4.6681      0.940  7.3190     1.004  319.77
  21   4.6698      0.950  7.3175     1.096  335.68
  22   4.6699      1.030  7.3185     0.986  351.62
  23   4.6691      0.980  7.3128     0.988  367.50
  24   4.6699      1.030  7.3171     1.024  383.41
  25   4.6689      1.020  7.3195     1.018  399.32
  26   4.6689      0.980  7.3115     1.016  415.18
  27   4.6695      0.970  7.3159     0.970  431.12
  28   4.6700      1.000  7.3162     1.006  447.00
  29   4.6693      0.960  7.3138     0.982  462.87
  30   4.6680      0.920  7.3208     1.084  478.78
  31   4.6695      0.980  7.3159     1.012  494.60
  32   4.6681      0.970  7.3177     0.962  510.56
  33   4.6705      0.980  7.3149     1.008  526.44
  34   4.6687      0.970  7.3128     1.008  542.41
  35   4.6701      0.980  7.3151     1.006  558.29
  36   4.6702      0.920  7.3177     0.978  574.27
  37   4.6693      0.940  7.3178     0.998  590.15
  38   4.6698      0.930  7.3196     0.946  605.96
  39   4.6715      0.960  7.3137     1.048  621.93
  40   4.6698      0.970  7.3115     1.068  637.90
  41   4.6701      1.020  7.3183     0.998  653.86
  42   4.6691      0.980  7.3132     1.052  669.73
  43   4.6704      0.970  7.3189     0.974  685.64
  44   4.6689      0.960  7.3203     1.000  701.53
  45   4.6694      0.980  7.3151     1.090  717.35
  46   4.6706      0.980  7.3172     0.988  733.30
  47   4.6694      0.980  7.3212     1.008  749.16
  48   4.6684      0.980  7.3194     0.982  765.18
  49   4.6689      1.000  7.3144     0.970  781.10
  50   4.6702      0.940  7.3117     1.056  796.98
  51   4.6696      0.980  7.3148     0.976  812.89
  52   4.6695      0.970  7.3152     0.962  828.80
  53   4.6693      1.010  7.3129     0.980  844.79
  54   4.6685      0.990  7.3143     1.020  860.64
  55   4.6693      1.000  7.3183     0.948  876.66
  56   4.6693      0.970  7.3128     1.022  892.62
  57   4.6698      1.040  7.3212     0.966  908.60
  58   4.6699      0.970  7.3195     1.050  924.47
  59   4.6695      0.950  7.3182     1.022  940.45
  60   4.6697      0.980  7.3178     1.096  956.27
  61   4.6686      1.020  7.3235     0.930  972.10
  62   4.6694      1.000  7.3163     1.056  988.06
  63   4.6699      0.970  7.3154     1.008  1003.95
  64   4.6690      0.970  7.3152     0.994  1019.91
  65   4.6692      0.960  7.3167     1.024  1035.75
  66   4.6690      0.940  7.3106     0.966  1051.60
  67   4.6701      0.980  7.3165     0.988  1067.48
  68   4.6690      0.950  7.3086     1.000  1083.38
  69   4.6685      0.960  7.3158     1.030  1099.31
  70   4.6699      0.970  7.3125     1.040  1115.15
  71   4.6695      0.920  7.3174     1.030  1131.14
  72   4.6697      1.010  7.3130     1.040  1147.03
  73   4.6692      1.000  7.3104     1.024  1162.98
  74   4.6699      1.000  7.3140     1.002  1178.87
  75   4.6699      0.970  7.3153     0.942  1194.77
  76   4.6705      0.930  7.3210     1.012  1210.64
  77   4.6689      0.990  7.3144     0.986  1226.53
  78   4.6688      0.980  7.3145     0.980  1242.51
  79   4.6697      1.000  7.3131     1.060  1258.33
  80   4.6681      0.990  7.3131     1.054  1274.21
  81   4.6688      0.970  7.3216     1.018  1290.18
  82   4.6697      0.980  7.3162     0.984  1305.99
  83   4.6698      0.980  7.3180     0.978  1321.90
  84   4.6689      0.950  7.3120     0.958  1337.74
  85   4.6683      0.990  7.3152     0.988  1353.67
