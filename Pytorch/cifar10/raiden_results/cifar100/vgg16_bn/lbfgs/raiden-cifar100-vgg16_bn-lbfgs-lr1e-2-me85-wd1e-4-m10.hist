Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11057673216 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6625      0.760  7.2889     0.982  12.97
   2   4.6617      0.790  7.2834     0.958  24.30
   3   4.6615      0.780  7.2850     1.048  35.67
   4   4.6628      0.790  7.2850     0.972  47.03
   5   4.6624      0.770  7.2889     0.990  58.35
   6   4.6621      0.740  7.2765     0.984  69.70
   7   4.6615      0.730  7.2881     0.982  81.09
   8   4.6606      0.790  7.2852     0.966  92.41
   9   4.6615      0.780  7.2836     1.000  103.79
  10   4.6630      0.750  7.2814     0.946  115.11
  11   4.6624      0.760  7.2858     1.028  126.43
  12   4.6623      0.750  7.2796     1.000  137.73
  13   4.6610      0.740  7.2838     1.076  149.21
  14   4.6614      0.680  7.2833     1.004  160.49
  15   4.6615      0.770  7.2835     0.942  171.87
  16   4.6622      0.780  7.2782     1.030  183.14
  17   4.6604      0.790  7.2848     1.014  194.41
  18   4.6619      0.720  7.2836     0.966  205.70
  19   4.6615      0.760  7.2819     0.982  217.13
  20   4.6616      0.770  7.2771     0.988  228.41
  21   4.6616      0.750  7.2793     0.998  239.68
  22   4.6612      0.720  7.2851     1.060  251.03
  23   4.6621      0.750  7.2807     1.072  262.32
  24   4.6620      0.760  7.2820     0.952  273.66
  25   4.6620      0.780  7.2776     0.934  285.07
  26   4.6608      0.780  7.2853     1.032  296.29
  27   4.6616      0.740  7.2837     1.004  307.58
  28   4.6622      0.790  7.2810     1.048  318.90
  29   4.6628      0.750  7.2843     1.018  330.20
  30   4.6625      0.800  7.2846     0.972  341.47
  31   4.6610      0.770  7.2780     0.982  352.78
  32   4.6620      0.760  7.2845     1.008  364.16
  33   4.6611      0.770  7.2882     0.998  375.45
  34   4.6625      0.770  7.2808     0.990  386.76
  35   4.6615      0.770  7.2836     1.086  398.12
  36   4.6622      0.770  7.2858     1.046  409.47
  37   4.6616      0.760  7.2822     0.956  420.72
  38   4.6626      0.740  7.2764     1.010  432.14
  39   4.6622      0.770  7.2832     0.984  443.53
  40   4.6611      0.790  7.2894     1.024  454.87
  41   4.6620      0.780  7.2849     0.998  466.34
  42   4.6616      0.730  7.2792     1.042  477.65
  43   4.6619      0.780  7.2793     1.084  488.95
  44   4.6617      0.790  7.2808     0.992  500.31
  45   4.6613      0.780  7.2836     0.930  511.56
  46   4.6607      0.740  7.2858     1.004  522.87
  47   4.6626      0.880  7.2843     0.942  534.14
  48   4.6632      0.740  7.2849     1.004  545.52
  49   4.6617      0.780  7.2866     0.966  556.83
  50   4.6617      0.810  7.2820     0.992  568.13
  51   4.6630      0.780  7.2884     0.942  579.53
  52   4.6615      0.760  7.2840     1.098  590.83
  53   4.6622      0.750  7.2876     1.056  602.09
  54   4.6616      0.750  7.2853     1.000  613.54
  55   4.6614      0.770  7.2848     0.896  624.82
  56   4.6607      0.810  7.2793     1.000  636.14
  57   4.6629      0.770  7.2809     0.966  647.62
  58   4.6625      0.780  7.2810     1.004  659.01
  59   4.6611      0.760  7.2877     0.932  670.32
  60   4.6622      0.790  7.2798     0.984  681.71
  61   4.6618      0.770  7.2842     0.978  693.02
  62   4.6629      0.790  7.2781     1.006  704.37
  63   4.6614      0.760  7.2832     0.970  715.73
  64   4.6618      0.790  7.2811     1.062  727.08
  65   4.6620      0.730  7.2858     0.952  738.28
  66   4.6621      0.750  7.2906     0.968  749.62
  67   4.6609      0.790  7.2841     0.906  760.95
  68   4.6609      0.740  7.2840     0.944  772.37
  69   4.6617      0.760  7.2798     1.054  783.72
  70   4.6625      0.800  7.2840     0.980  795.07
  71   4.6613      0.780  7.2848     0.964  806.38
  72   4.6632      0.750  7.2834     1.040  817.71
  73   4.6616      0.820  7.2840     1.006  829.10
  74   4.6627      0.760  7.2832     1.042  840.50
  75   4.6614      0.790  7.2780     1.014  851.77
  76   4.6620      0.770  7.2864     0.950  863.16
  77   4.6631      0.810  7.2814     1.022  874.45
  78   4.6619      0.730  7.2845     0.946  885.86
  79   4.6617      0.740  7.2879     1.006  897.23
  80   4.6620      0.750  7.2826     0.990  908.46
  81   4.6611      0.790  7.2804     1.030  919.81
  82   4.6623      0.780  7.2809     0.992  931.29
  83   4.6626      0.800  7.2851     0.958  942.51
  84   4.6607      0.760  7.2773     0.996  953.86
  85   4.6617      0.780  7.2798     1.058  965.16
