Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6702      0.950  7.3000     1.030  11.94
   2   4.6700      1.030  7.2923     0.890  22.31
   3   4.6696      0.940  7.2937     1.006  32.56
   4   4.6713      0.990  7.2926     1.042  42.87
   5   4.6696      1.030  7.2945     1.030  53.25
   6   4.6693      1.010  7.2901     0.912  63.57
   7   4.6712      1.000  7.2941     0.834  73.87
   8   4.6704      0.970  7.2912     0.992  84.19
   9   4.6713      0.960  7.2901     0.962  94.62
  10   4.6705      0.960  7.2988     0.934  104.93
  11   4.6701      0.920  7.2909     0.978  115.23
  12   4.6708      0.940  7.2969     1.056  125.78
  13   4.6697      0.990  7.2918     0.984  136.07
  14   4.6697      0.970  7.2894     1.110  146.40
  15   4.6718      0.920  7.2938     0.958  156.78
  16   4.6709      0.940  7.2923     0.964  167.02
  17   4.6701      0.990  7.2937     0.878  177.27
  18   4.6707      1.030  7.2891     1.004  187.55
  19   4.6713      0.900  7.2879     0.954  197.92
  20   4.6704      1.040  7.2885     1.030  208.20
  21   4.6704      0.970  7.2976     0.902  218.47
  22   4.6710      0.950  7.2903     0.996  228.86
  23   4.6711      0.930  7.2966     0.968  239.20
  24   4.6706      0.900  7.2901     0.934  249.45
  25   4.6706      0.970  7.2912     1.026  259.74
  26   4.6707      1.000  7.2977     0.952  270.08
  27   4.6711      1.000  7.2911     1.022  280.34
  28   4.6703      1.000  7.2896     1.034  290.64
  29   4.6697      0.900  7.2917     0.992  301.02
  30   4.6707      0.960  7.2851     1.070  311.30
  31   4.6708      0.950  7.2954     1.022  321.64
  32   4.6705      1.010  7.2926     1.028  331.93
  33   4.6713      0.940  7.2925     1.064  342.31
  34   4.6708      1.000  7.3003     1.010  352.62
  35   4.6698      0.960  7.2922     0.960  362.95
  36   4.6710      1.010  7.2946     1.030  373.33
  37   4.6707      0.940  7.2924     0.976  383.59
  38   4.6701      0.970  7.2881     0.956  393.86
  39   4.6706      0.930  7.2886     0.976  404.17
  40   4.6700      0.960  7.2936     0.984  414.60
  41   4.6710      0.980  7.2870     0.994  424.90
  42   4.6707      0.940  7.2930     1.032  435.21
  43   4.6702      1.000  7.2921     1.042  445.62
  44   4.6701      0.920  7.2884     0.998  455.91
  45   4.6715      0.980  7.2940     0.962  466.23
  46   4.6700      1.010  7.2933     0.976  476.62
  47   4.6694      0.950  7.2939     0.986  486.92
  48   4.6702      0.980  7.2954     0.940  497.24
  49   4.6708      0.950  7.2924     0.982  507.53
  50   4.6716      0.940  7.2943     0.962  517.88
  51   4.6712      0.950  7.2969     0.924  528.20
  52   4.6706      1.000  7.2921     0.938  538.50
  53   4.6708      0.910  7.2935     0.952  548.89
  54   4.6691      0.980  7.2955     1.076  559.19
  55   4.6706      0.980  7.2896     1.008  569.50
  56   4.6706      1.000  7.2918     1.054  579.88
  57   4.6720      0.940  7.2901     0.956  590.15
  58   4.6705      0.990  7.2926     0.950  600.43
  59   4.6706      0.970  7.2924     1.078  610.69
  60   4.6699      0.940  7.2887     0.962  621.06
  61   4.6702      0.980  7.2917     1.036  631.36
  62   4.6715      0.900  7.2942     1.054  641.66
  63   4.6708      1.000  7.2874     1.060  652.02
  64   4.6702      0.980  7.2971     1.050  662.31
  65   4.6700      0.980  7.2927     1.054  672.60
  66   4.6706      1.030  7.2910     0.916  682.89
  67   4.6716      0.970  7.2921     0.922  693.27
  68   4.6712      0.910  7.2936     0.978  703.58
  69   4.6704      0.940  7.2889     0.912  713.91
  70   4.6717      0.990  7.2883     1.040  724.27
  71   4.6701      0.960  7.2920     0.972  734.60
  72   4.6699      0.970  7.2975     0.974  744.89
  73   4.6708      0.980  7.2890     1.006  755.13
  74   4.6720      0.900  7.2916     0.918  765.54
  75   4.6698      1.000  7.2870     0.984  775.84
  76   4.6703      0.920  7.2892     0.950  786.14
  77   4.6693      0.980  7.2964     0.994  796.52
  78   4.6707      0.970  7.2931     1.000  806.78
  79   4.6708      0.910  7.2919     0.958  817.09
  80   4.6703      1.010  7.2879     0.946  827.38
  81   4.6699      0.980  7.2905     0.926  837.79
  82   4.6707      0.920  7.2963     0.986  848.10
  83   4.6701      0.910  7.2953     0.958  858.41
  84   4.6699      1.030  7.2954     0.962  868.75
  85   4.6703      0.940  7.2950     1.012  879.07
