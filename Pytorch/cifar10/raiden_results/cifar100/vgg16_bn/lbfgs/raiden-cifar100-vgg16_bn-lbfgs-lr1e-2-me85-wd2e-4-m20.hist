Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11058113536 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6758      0.920  9.6541     0.954  13.19
   2   4.6756      0.920  9.6530     1.022  24.75
   3   4.6754      0.890  9.6566     0.926  36.41
   4   4.6759      0.900  9.6460     0.976  47.87
   5   4.6755      0.910  9.6514     0.994  59.37
   6   4.6755      0.940  9.6521     0.966  70.97
   7   4.6759      0.900  9.6498     0.964  82.64
   8   4.6778      0.910  9.6574     0.958  94.27
   9   4.6747      0.950  9.6471     1.074  105.90
  10   4.6760      0.900  9.6537     1.046  117.60
  11   4.6743      0.900  9.6490     1.010  129.20
  12   4.6783      0.890  9.6499     0.990  140.84
  13   4.6757      0.940  9.6564     0.916  152.52
  14   4.6758      0.900  9.6484     1.042  164.09
  15   4.6759      0.910  9.6517     1.040  175.65
  16   4.6751      0.920  9.6547     0.968  187.31
  17   4.6739      0.920  9.6460     0.986  198.93
  18   4.6756      0.910  9.6446     1.096  210.52
  19   4.6760      0.890  9.6516     0.956  222.18
  20   4.6756      0.870  9.6475     1.040  233.79
  21   4.6765      0.910  9.6525     0.940  245.40
  22   4.6725      0.930  9.6512     0.986  257.04
  23   4.6776      0.950  9.6521     1.024  268.67
  24   4.6746      0.920  9.6474     0.960  280.23
  25   4.6761      0.900  9.6518     1.010  291.90
  26   4.6750      0.920  9.6516     0.994  303.48
  27   4.6748      0.880  9.6439     1.022  315.08
  28   4.6740      0.960  9.6515     1.014  326.79
  29   4.6749      0.890  9.6523     0.986  338.43
  30   4.6768      0.930  9.6482     0.982  350.02
  31   4.6753      0.940  9.6454     0.964  361.70
  32   4.6756      0.910  9.6534     0.998  373.27
  33   4.6768      0.910  9.6506     0.992  384.89
  34   4.6749      0.930  9.6515     0.982  396.53
  35   4.6743      0.930  9.6564     1.028  408.15
  36   4.6746      0.880  9.6489     1.056  419.76
  37   4.6759      0.920  9.6522     1.020  431.40
  38   4.6744      0.930  9.6513     0.952  442.98
  39   4.6767      0.910  9.6440     1.048  454.55
  40   4.6762      0.870  9.6447     1.002  466.22
  41   4.6758      0.870  9.6525     0.972  477.79
  42   4.6745      0.920  9.6478     0.960  489.38
  43   4.6758      0.890  9.6498     1.060  501.05
  44   4.6747      0.920  9.6496     1.000  512.63
  45   4.6753      0.930  9.6518     1.002  524.27
  46   4.6760      0.920  9.6498     0.998  535.92
  47   4.6777      0.940  9.6538     1.044  547.54
  48   4.6758      0.920  9.6519     1.016  559.15
  49   4.6766      0.880  9.6488     1.004  570.85
  50   4.6747      0.900  9.6468     1.020  582.40
  51   4.6760      0.890  9.6501     1.006  594.03
  52   4.6765      0.930  9.6503     0.986  605.60
  53   4.6734      0.940  9.6482     0.942  617.29
  54   4.6741      0.920  9.6478     1.052  628.89
  55   4.6750      0.930  9.6448     0.998  640.47
  56   4.6762      0.980  9.6542     1.002  652.12
  57   4.6758      0.910  9.6516     1.040  663.68
  58   4.6749      0.940  9.6506     0.980  675.24
  59   4.6756      0.900  9.6524     1.018  686.95
  60   4.6761      0.870  9.6514     1.006  698.51
  61   4.6767      0.900  9.6471     0.968  710.09
  62   4.6748      0.930  9.6456     1.004  721.73
  63   4.6753      0.950  9.6497     0.960  733.34
  64   4.6766      0.940  9.6528     0.980  744.96
  65   4.6750      0.930  9.6531     0.992  756.63
  66   4.6754      0.920  9.6447     0.974  768.22
  67   4.6765      0.920  9.6503     0.962  779.87
  68   4.6773      0.880  9.6485     1.028  791.51
  69   4.6757      0.950  9.6484     0.944  803.07
  70   4.6760      0.920  9.6534     0.942  814.63
  71   4.6753      0.900  9.6500     1.064  826.25
  72   4.6754      0.910  9.6494     0.976  837.84
  73   4.6757      0.930  9.6552     0.966  849.45
  74   4.6762      0.920  9.6543     0.982  861.01
  75   4.6754      0.900  9.6517     0.956  872.63
  76   4.6755      0.930  9.6510     0.982  884.22
  77   4.6736      0.950  9.6562     0.936  895.77
  78   4.6758      0.940  9.6460     1.010  907.42
  79   4.6764      0.940  9.6442     0.940  919.01
  80   4.6757      0.940  9.6529     0.936  930.62
  81   4.6744      0.900  9.6486     0.930  942.30
  82   4.6751      0.930  9.6510     0.926  953.91
  83   4.6736      0.950  9.6493     1.074  965.51
  84   4.6732      0.950  9.6540     0.978  977.19
  85   4.6760      0.900  9.6492     1.042  988.77
