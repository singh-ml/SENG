Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901789696 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6669      1.030  16.7343     1.038  12.18
   2   4.6658      0.940  16.7240     1.002  22.46
   3   4.6667      0.960  16.7332     0.974  32.76
   4   4.6668      1.000  16.7355     0.972  43.14
   5   4.6669      1.000  16.7288     0.998  53.36
   6   4.6670      0.970  16.7266     0.954  63.66
   7   4.6677      0.960  16.7348     0.918  73.90
   8   4.6662      0.990  16.7297     0.942  84.20
   9   4.6669      0.930  16.7325     1.018  94.42
  10   4.6654      0.990  16.7249     0.968  104.70
  11   4.6670      0.960  16.7265     1.120  115.08
  12   4.6667      0.940  16.7339     0.966  125.33
  13   4.6656      0.950  16.7265     1.018  135.56
  14   4.6671      1.000  16.7285     0.958  145.82
  15   4.6685      0.990  16.7275     0.962  156.18
  16   4.6669      0.920  16.7260     0.958  166.44
  17   4.6666      0.960  16.7290     0.968  176.69
  18   4.6670      1.040  16.7325     1.018  186.93
  19   4.6662      0.910  16.7265     0.938  197.26
  20   4.6670      1.010  16.7281     1.054  207.49
  21   4.6675      0.940  16.7315     0.936  217.75
  22   4.6675      0.950  16.7287     1.014  228.09
  23   4.6662      0.950  16.7264     1.002  238.33
  24   4.6678      1.030  16.7292     0.946  248.63
  25   4.6671      0.940  16.7296     1.034  258.94
  26   4.6654      1.060  16.7328     1.056  269.20
  27   4.6657      0.950  16.7296     1.110  279.47
  28   4.6667      0.970  16.7331     0.948  289.75
  29   4.6668      1.020  16.7330     0.944  300.02
  30   4.6670      1.000  16.7287     1.026  310.25
  31   4.6659      0.990  16.7286     0.970  320.53
  32   4.6665      1.030  16.7294     0.948  330.87
  33   4.6668      0.960  16.7241     1.020  341.10
  34   4.6676      0.980  16.7294     0.980  351.38
  35   4.6668      1.020  16.7311     1.020  361.68
  36   4.6669      0.930  16.7332     0.968  372.02
  37   4.6672      0.990  16.7280     0.982  382.27
  38   4.6661      0.910  16.7314     0.960  392.54
  39   4.6661      0.940  16.7264     0.978  402.91
  40   4.6677      0.980  16.7327     1.012  413.20
  41   4.6668      0.980  16.7328     1.018  423.44
  42   4.6665      0.890  16.7289     1.000  433.72
  43   4.6684      0.980  16.7304     0.962  444.06
  44   4.6653      0.880  16.7287     0.928  454.34
  45   4.6667      0.980  16.7248     1.014  464.63
  46   4.6659      0.960  16.7252     0.920  474.95
  47   4.6667      0.950  16.7363     1.024  485.23
  48   4.6666      1.000  16.7263     0.984  495.48
  49   4.6675      0.990  16.7294     0.986  505.83
  50   4.6673      0.950  16.7283     0.984  516.09
  51   4.6679      0.930  16.7346     0.996  526.32
  52   4.6670      0.950  16.7203     1.016  536.61
  53   4.6678      0.960  16.7275     1.008  546.94
  54   4.6667      1.010  16.7261     0.974  557.18
  55   4.6661      1.030  16.7332     0.950  567.48
  56   4.6673      0.940  16.7348     0.952  577.73
  57   4.6660      1.010  16.7324     0.978  588.00
  58   4.6666      0.950  16.7288     0.972  598.23
  59   4.6656      1.040  16.7310     1.022  608.51
  60   4.6665      0.970  16.7271     1.002  618.87
  61   4.6676      0.980  16.7317     1.028  629.13
  62   4.6666      0.960  16.7335     0.944  639.42
  63   4.6672      0.990  16.7290     0.996  649.75
  64   4.6673      0.970  16.7277     0.926  660.03
  65   4.6651      0.910  16.7336     0.950  670.29
  66   4.6661      0.900  16.7297     1.042  680.63
  67   4.6679      0.970  16.7308     0.988  690.87
  68   4.6664      0.970  16.7249     0.994  701.09
  69   4.6675      1.010  16.7301     0.970  711.36
  70   4.6662      1.010  16.7300     1.018  721.70
  71   4.6660      0.980  16.7258     1.024  731.99
  72   4.6663      0.890  16.7320     0.948  742.20
  73   4.6677      0.990  16.7252     1.048  752.49
  74   4.6681      0.940  16.7288     1.018  762.77
  75   4.6672      1.070  16.7299     0.940  773.03
  76   4.6670      1.020  16.7299     1.018  783.26
  77   4.6667      0.920  16.7309     1.036  793.61
  78   4.6671      0.960  16.7274     1.056  803.86
  79   4.6658      1.020  16.7303     0.978  814.15
  80   4.6679      0.950  16.7339     0.962  824.54
  81   4.6657      0.940  16.7321     0.918  834.79
  82   4.6672      1.000  16.7286     1.044  845.06
  83   4.6678      1.020  16.7281     0.958  855.40
  84   4.6672      0.960  16.7260     0.930  865.68
  85   4.6667      0.980  16.7300     0.946  875.96
