Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17523845120 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6706      0.950  9.6641     0.892  17.34
   2   4.6715      0.930  9.6656     1.010  32.90
   3   4.6701      0.920  9.6612     0.966  48.49
   4   4.6706      1.020  9.6671     0.942  64.17
   5   4.6710      1.020  9.6643     1.060  79.72
   6   4.6714      1.030  9.6659     0.980  95.33
   7   4.6709      0.920  9.6638     1.042  110.92
   8   4.6703      0.950  9.6627     0.900  126.49
   9   4.6706      0.950  9.6666     0.976  142.13
  10   4.6713      1.010  9.6629     1.010  157.68
  11   4.6701      0.970  9.6662     0.982  173.34
  12   4.6716      1.020  9.6627     1.036  188.92
  13   4.6714      0.970  9.6611     0.986  204.57
  14   4.6715      1.010  9.6644     1.034  220.14
  15   4.6716      1.010  9.6634     0.924  235.74
  16   4.6707      0.880  9.6698     1.012  251.30
  17   4.6699      1.010  9.6626     0.992  266.87
  18   4.6708      0.940  9.6676     0.998  282.53
  19   4.6697      0.870  9.6680     1.048  298.12
  20   4.6718      0.990  9.6572     0.952  313.75
  21   4.6710      0.940  9.6652     1.006  329.33
  22   4.6707      0.940  9.6689     0.926  344.90
  23   4.6714      0.880  9.6598     0.928  360.56
  24   4.6704      0.970  9.6648     1.006  376.15
  25   4.6711      0.990  9.6665     0.990  391.81
  26   4.6712      0.960  9.6662     1.020  407.40
  27   4.6707      0.970  9.6626     0.986  423.10
  28   4.6703      0.920  9.6667     1.064  438.69
  29   4.6706      0.870  9.6641     0.970  454.33
  30   4.6709      1.030  9.6580     1.092  469.90
  31   4.6706      0.940  9.6675     0.964  485.49
  32   4.6708      0.910  9.6636     0.994  501.13
  33   4.6708      1.000  9.6627     0.930  516.71
  34   4.6709      0.870  9.6594     1.018  532.35
  35   4.6706      0.940  9.6645     1.024  547.92
  36   4.6707      1.000  9.6698     0.926  563.50
  37   4.6712      0.930  9.6656     1.010  579.13
  38   4.6700      0.940  9.6630     1.030  594.72
  39   4.6704      1.000  9.6619     0.976  610.33
  40   4.6706      0.930  9.6654     0.958  625.91
  41   4.6703      0.980  9.6631     0.974  641.51
  42   4.6717      0.970  9.6621     1.032  657.08
  43   4.6702      0.990  9.6679     0.980  672.67
  44   4.6709      0.950  9.6658     1.006  688.32
  45   4.6700      0.980  9.6684     1.004  703.89
  46   4.6711      0.980  9.6619     1.102  719.57
  47   4.6710      0.940  9.6616     1.014  735.13
  48   4.6709      0.980  9.6651     0.980  750.69
  49   4.6711      0.990  9.6607     0.978  766.33
  50   4.6709      1.020  9.6670     1.028  781.91
  51   4.6710      0.900  9.6648     0.998  797.53
  52   4.6706      0.930  9.6624     0.936  813.12
  53   4.6703      0.980  9.6679     1.018  828.77
  54   4.6715      0.980  9.6605     1.028  844.33
  55   4.6708      0.920  9.6607     0.944  859.98
  56   4.6712      0.950  9.6598     1.032  875.54
  57   4.6713      0.970  9.6672     0.954  891.13
  58   4.6708      0.980  9.6622     0.998  906.78
  59   4.6712      0.880  9.6598     0.994  922.35
  60   4.6703      0.880  9.6651     1.056  938.00
  61   4.6705      0.950  9.6577     0.986  953.55
  62   4.6707      0.980  9.6632     1.026  969.19
  63   4.6705      1.020  9.6637     0.956  984.77
  64   4.6707      1.040  9.6631     0.996  1000.32
  65   4.6712      0.980  9.6613     0.984  1015.99
  66   4.6706      0.920  9.6650     0.948  1031.58
  67   4.6701      0.950  9.6631     1.024  1047.23
  68   4.6703      0.990  9.6698     1.072  1062.81
  69   4.6704      0.990  9.6651     0.990  1078.44
  70   4.6702      0.980  9.6578     1.018  1094.01
  71   4.6711      0.910  9.6620     0.970  1109.60
  72   4.6713      0.960  9.6620     0.942  1125.23
  73   4.6703      0.960  9.6636     1.012  1140.80
  74   4.6711      0.960  9.6628     1.008  1156.46
  75   4.6709      1.040  9.6647     0.960  1172.04
  76   4.6714      1.060  9.6635     1.020  1187.68
  77   4.6712      1.010  9.6633     0.966  1203.25
  78   4.6711      0.980  9.6621     0.968  1218.81
  79   4.6697      0.960  9.6628     0.936  1234.49
  80   4.6707      1.000  9.6690     0.906  1250.06
  81   4.6693      0.940  9.6647     0.972  1265.70
  82   4.6697      0.920  9.6662     0.918  1281.28
  83   4.6697      0.940  9.6606     0.982  1296.85
  84   4.6702      0.980  9.6663     1.060  1312.51
  85   4.6705      0.990  9.6597     1.028  1328.04
