Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 19678408704 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6948      0.900  16.7894     0.994  18.62
   2   4.6931      0.910  16.7994     0.984  36.08
   3   4.6937      0.900  16.7915     0.936  53.86
   4   4.6932      0.900  16.7959     1.064  71.94
   5   4.6931      0.990  16.7963     1.010  89.92
   6   4.6930      0.930  16.8005     1.050  107.99
   7   4.6930      0.940  16.7963     0.992  126.00
   8   4.6937      0.920  16.8054     0.974  144.00
   9   4.6938      0.960  16.8001     0.972  161.95
  10   4.6938      0.910  16.7979     0.960  180.00
  11   4.6938      0.940  16.8006     1.010  198.01
  12   4.6934      1.000  16.7933     1.062  216.05
  13   4.6944      0.920  16.7926     0.954  234.05
  14   4.6944      0.910  16.7964     1.048  252.08
  15   4.6939      0.890  16.7961     0.980  270.06
  16   4.6940      0.940  16.7962     0.964  288.10
  17   4.6938      0.900  16.8006     1.002  306.09
  18   4.6936      0.930  16.7983     1.006  324.14
  19   4.6937      0.950  16.7945     0.978  342.13
  20   4.6931      0.910  16.8038     0.980  360.19
  21   4.6929      0.920  16.8009     0.988  378.16
  22   4.6946      0.880  16.7999     0.984  396.24
  23   4.6945      0.890  16.7991     0.926  414.21
  24   4.6938      0.910  16.7943     0.992  432.29
  25   4.6948      0.950  16.7938     1.016  450.27
  26   4.6956      0.880  16.7999     1.078  468.32
  27   4.6937      0.930  16.7949     0.940  486.28
  28   4.6940      0.910  16.7991     0.928  504.32
  29   4.6945      0.940  16.8000     0.986  522.30
  30   4.6947      0.900  16.7949     1.036  540.32
  31   4.6932      0.890  16.7989     0.996  558.31
  32   4.6936      0.890  16.7998     0.942  576.36
  33   4.6939      0.880  16.7938     1.022  594.33
  34   4.6952      0.930  16.7978     0.998  612.40
  35   4.6933      0.930  16.7947     1.008  630.38
  36   4.6940      0.920  16.7990     0.988  648.39
  37   4.6938      0.910  16.7962     1.002  666.37
  38   4.6949      0.860  16.7971     0.890  684.46
  39   4.6959      0.930  16.7966     0.974  702.45
  40   4.6943      0.920  16.7935     1.048  720.52
  41   4.6930      0.880  16.8023     1.018  738.50
  42   4.6929      0.930  16.7957     1.034  756.53
  43   4.6948      0.890  16.7991     0.954  774.53
  44   4.6948      0.880  16.7993     1.038  792.56
  45   4.6934      0.910  16.8047     0.992  810.56
  46   4.6943      0.900  16.7950     0.964  828.60
  47   4.6933      0.950  16.7929     0.878  846.58
  48   4.6940      0.970  16.8000     1.022  864.66
  49   4.6942      0.910  16.8048     0.950  882.62
  50   4.6944      0.870  16.7964     0.932  900.69
  51   4.6941      0.890  16.8062     1.024  918.68
  52   4.6939      0.910  16.8000     1.022  936.73
  53   4.6928      0.890  16.7980     1.042  954.72
  54   4.6939      0.910  16.7983     0.964  972.78
  55   4.6944      0.910  16.7969     0.982  990.76
  56   4.6939      0.940  16.7960     0.896  1008.80
  57   4.6916      0.950  16.7964     1.006  1026.76
  58   4.6931      0.930  16.7991     0.986  1044.79
  59   4.6939      0.950  16.8001     1.116  1062.77
  60   4.6931      0.910  16.7979     1.052  1080.79
  61   4.6939      0.900  16.7921     1.024  1098.78
  62   4.6944      0.990  16.7938     1.000  1116.79
  63   4.6930      0.880  16.8027     0.958  1134.78
  64   4.6937      0.880  16.7984     0.984  1152.78
  65   4.6945      0.900  16.8038     0.888  1170.82
  66   4.6933      0.980  16.7980     0.986  1188.77
  67   4.6939      0.920  16.7982     1.026  1206.84
  68   4.6940      0.950  16.7977     0.898  1224.82
  69   4.6943      0.920  16.7974     1.004  1242.88
  70   4.6938      0.880  16.7999     0.926  1260.85
  71   4.6948      0.900  16.7952     0.932  1278.88
  72   4.6932      0.910  16.8028     0.966  1296.86
  73   4.6937      0.920  16.7961     0.962  1314.91
  74   4.6941      0.930  16.7954     0.942  1332.88
  75   4.6945      0.890  16.7981     1.030  1350.93
  76   4.6934      0.920  16.7990     1.052  1368.91
  77   4.6936      0.930  16.8016     0.970  1386.95
  78   4.6954      0.910  16.8058     0.964  1404.92
  79   4.6932      0.890  16.7927     1.056  1422.97
  80   4.6941      0.920  16.8036     0.974  1440.95
  81   4.6930      0.960  16.8030     0.918  1459.00
  82   4.6926      0.910  16.7997     1.100  1477.02
  83   4.6957      0.890  16.7991     0.984  1495.06
  84   4.6939      0.980  16.8001     0.936  1513.02
  85   4.6937      0.950  16.7971     1.026  1531.08
  86   4.6945      0.910  16.8023     0.888  1549.06
  87   4.6940      0.880  16.7940     1.016  1567.11
  88   4.6940      0.880  16.7967     0.960  1585.08
  89   4.6953      0.890  16.7991     0.968  1603.10
  90   4.6956      0.920  16.7957     0.974  1621.09
