Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 12134159360 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6798      0.900  9.6749     1.044  13.43
   2   4.6799      0.890  9.6738     1.006  25.09
   3   4.6799      0.940  9.6683     0.974  36.89
   4   4.6813      0.940  9.6759     1.028  48.62
   5   4.6809      0.940  9.6720     0.986  60.32
   6   4.6805      0.890  9.6701     1.048  72.13
   7   4.6803      0.870  9.6738     0.968  83.89
   8   4.6808      0.910  9.6743     1.072  95.58
   9   4.6793      0.890  9.6724     0.984  107.37
  10   4.6803      0.910  9.6752     1.002  119.19
  11   4.6803      0.940  9.6791     0.906  130.88
  12   4.6814      0.940  9.6763     0.962  142.66
  13   4.6812      0.910  9.6740     1.024  154.35
  14   4.6811      0.910  9.6720     0.924  166.04
  15   4.6806      0.890  9.6752     0.978  177.83
  16   4.6810      0.910  9.6745     0.960  189.53
  17   4.6802      0.920  9.6786     1.006  201.19
  18   4.6802      0.920  9.6784     0.944  212.97
  19   4.6811      0.900  9.6726     0.962  224.66
  20   4.6813      0.920  9.6771     0.942  236.39
  21   4.6801      0.910  9.6781     1.016  248.13
  22   4.6805      0.880  9.6739     0.982  259.91
  23   4.6803      0.880  9.6774     0.966  271.57
  24   4.6812      0.900  9.6798     1.048  283.24
  25   4.6807      0.920  9.6729     0.970  295.00
  26   4.6805      0.910  9.6775     0.934  306.67
  27   4.6802      0.910  9.6740     0.992  318.38
  28   4.6805      0.930  9.6743     0.930  330.18
  29   4.6811      0.920  9.6750     0.994  341.86
  30   4.6802      0.910  9.6759     0.912  353.57
  31   4.6799      0.920  9.6790     0.916  365.40
  32   4.6810      0.900  9.6719     1.054  377.20
  33   4.6800      0.930  9.6749     1.000  388.89
  34   4.6807      0.920  9.6695     1.070  400.68
  35   4.6799      0.960  9.6763     1.044  412.37
  36   4.6808      0.940  9.6751     0.956  424.19
  37   4.6804      0.900  9.6712     0.988  435.96
  38   4.6806      0.890  9.6775     0.968  447.65
  39   4.6810      0.890  9.6774     1.022  459.32
  40   4.6803      0.890  9.6727     1.002  471.12
  41   4.6795      0.900  9.6722     1.018  482.79
  42   4.6809      0.930  9.6733     0.976  494.53
  43   4.6805      0.930  9.6732     0.998  506.36
  44   4.6799      0.870  9.6766     1.018  518.05
  45   4.6803      0.910  9.6849     0.952  529.74
  46   4.6795      0.920  9.6752     1.028  541.52
  47   4.6799      0.910  9.6740     0.976  553.22
  48   4.6806      0.930  9.6790     1.038  564.96
  49   4.6816      0.920  9.6737     0.956  576.82
  50   4.6804      0.920  9.6716     1.044  588.48
  51   4.6806      0.870  9.6714     0.960  600.20
  52   4.6799      0.920  9.6787     0.918  612.00
  53   4.6802      0.930  9.6735     1.060  623.69
  54   4.6815      0.930  9.6722     1.032  635.33
  55   4.6807      0.930  9.6715     1.004  647.07
  56   4.6815      0.930  9.6705     0.978  658.76
  57   4.6816      0.910  9.6745     0.942  670.40
  58   4.6801      0.950  9.6736     0.954  682.17
  59   4.6807      0.920  9.6729     0.966  693.84
  60   4.6803      0.920  9.6726     0.976  705.56
  61   4.6811      0.950  9.6769     0.958  717.29
  62   4.6808      0.920  9.6728     1.018  729.10
  63   4.6798      0.930  9.6736     1.022  740.80
  64   4.6797      0.920  9.6737     0.984  752.59
  65   4.6801      0.900  9.6733     1.020  764.25
  66   4.6805      0.890  9.6781     0.972  775.95
  67   4.6810      0.890  9.6749     0.958  787.76
  68   4.6807      0.950  9.6757     1.006  799.48
  69   4.6806      0.930  9.6777     0.922  811.18
  70   4.6813      0.910  9.6710     1.044  822.95
  71   4.6800      0.920  9.6699     1.004  834.69
  72   4.6809      0.920  9.6762     1.046  846.45
  73   4.6815      0.950  9.6759     0.958  858.22
  74   4.6807      0.950  9.6789     1.092  869.90
  75   4.6802      0.900  9.6784     0.886  881.61
  76   4.6806      0.930  9.6702     1.062  893.33
  77   4.6806      0.930  9.6779     0.900  905.12
  78   4.6805      0.920  9.6745     0.988  916.82
  79   4.6805      0.930  9.6729     1.018  928.49
  80   4.6797      0.920  9.6723     1.030  940.30
  81   4.6801      0.890  9.6779     0.998  952.04
  82   4.6810      0.940  9.6773     0.950  963.82
  83   4.6798      0.890  9.6720     0.988  975.56
  84   4.6806      0.880  9.6760     1.030  987.26
  85   4.6797      0.890  9.6740     1.028  998.99
