Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6739      1.060  7.3090     0.982  12.03
   2   4.6742      1.120  7.3035     1.044  22.46
   3   4.6735      1.130  7.3099     0.958  32.77
   4   4.6741      1.020  7.3075     0.982  43.11
   5   4.6747      1.070  7.3071     0.960  53.53
   6   4.6733      1.040  7.3056     1.046  63.82
   7   4.6735      1.100  7.3022     1.014  74.14
   8   4.6745      1.050  7.3060     1.092  84.52
   9   4.6739      1.110  7.3038     1.022  94.81
  10   4.6739      1.100  7.3053     1.044  105.10
  11   4.6732      1.100  7.3103     1.042  115.42
  12   4.6738      1.060  7.3012     0.948  125.81
  13   4.6732      1.140  7.3078     0.972  136.14
  14   4.6733      1.090  7.3044     1.056  146.48
  15   4.6734      1.060  7.3042     1.028  156.87
  16   4.6738      1.030  7.3032     0.956  167.15
  17   4.6743      1.150  7.2972     0.974  177.46
  18   4.6739      1.090  7.3067     1.016  187.76
  19   4.6737      1.100  7.3096     1.002  198.17
  20   4.6734      1.110  7.3010     1.084  208.53
  21   4.6737      1.130  7.3091     0.992  218.85
  22   4.6745      1.160  7.3099     1.074  229.28
  23   4.6729      1.090  7.3076     1.038  239.60
  24   4.6745      1.070  7.3070     1.014  249.93
  25   4.6735      1.150  7.3054     1.014  260.31
  26   4.6738      1.040  7.3031     0.966  270.62
  27   4.6742      1.070  7.2996     1.070  280.93
  28   4.6737      1.040  7.3013     1.030  291.26
  29   4.6731      1.040  7.3027     1.020  301.71
  30   4.6740      1.090  7.3070     0.974  312.01
  31   4.6740      1.090  7.3080     0.940  322.31
  32   4.6741      1.110  7.3033     1.058  332.70
  33   4.6734      1.160  7.3019     0.990  343.01
  34   4.6749      1.110  7.3015     0.946  353.33
  35   4.6738      1.090  7.3053     0.930  363.68
  36   4.6736      1.070  7.3036     1.092  374.07
  37   4.6726      1.070  7.3077     0.978  384.41
  38   4.6739      1.120  7.3141     0.942  394.70
  39   4.6740      1.140  7.3073     0.968  405.03
  40   4.6740      1.150  7.2996     1.030  415.42
  41   4.6738      1.060  7.3057     1.046  425.73
  42   4.6739      1.100  7.3034     1.008  436.04
  43   4.6734      1.100  7.3024     1.052  446.42
  44   4.6738      1.040  7.3047     0.978  456.73
  45   4.6738      1.110  7.3017     0.918  467.06
  46   4.6742      1.110  7.2963     0.986  477.44
  47   4.6733      1.100  7.3110     0.968  487.79
  48   4.6737      1.060  7.3042     0.998  498.12
  49   4.6749      1.060  7.3065     1.022  508.42
  50   4.6735      1.070  7.3069     1.034  518.78
  51   4.6735      1.080  7.3078     1.062  529.05
  52   4.6740      1.040  7.3005     0.974  539.36
  53   4.6740      1.080  7.3034     0.994  549.68
  54   4.6733      1.060  7.3038     1.026  559.96
  55   4.6741      1.080  7.3084     0.988  570.24
  56   4.6746      1.110  7.3010     0.948  580.56
  57   4.6735      1.080  7.3084     1.020  590.95
  58   4.6737      1.040  7.3051     1.028  601.25
  59   4.6737      1.120  7.3019     0.954  611.55
  60   4.6733      1.090  7.3060     0.996  621.83
  61   4.6738      1.080  7.3042     0.922  632.22
  62   4.6735      1.110  7.3059     0.960  642.53
  63   4.6737      1.080  7.3025     0.984  652.86
  64   4.6743      1.060  7.3086     1.060  663.28
  65   4.6742      1.070  7.3037     1.078  673.59
  66   4.6741      1.130  7.3095     0.952  683.93
  67   4.6741      1.120  7.3063     1.010  694.25
  68   4.6731      1.100  7.3025     1.050  704.62
  69   4.6742      1.100  7.3079     1.018  714.94
  70   4.6734      1.040  7.3046     0.920  725.24
  71   4.6723      1.100  7.3031     1.064  735.59
  72   4.6735      1.110  7.3071     0.994  745.90
  73   4.6732      1.030  7.3116     1.072  756.23
  74   4.6745      1.050  7.3044     0.904  766.53
  75   4.6726      1.050  7.3029     1.004  776.92
  76   4.6742      1.120  7.3107     1.008  787.23
  77   4.6742      1.070  7.3068     0.992  797.52
  78   4.6744      1.060  7.3099     1.024  807.81
  79   4.6733      1.110  7.3066     1.026  818.20
  80   4.6735      1.070  7.2996     1.080  828.49
  81   4.6741      1.100  7.3043     1.040  838.79
  82   4.6735      1.080  7.3073     1.006  849.16
  83   4.6733      1.100  7.3057     0.958  859.46
  84   4.6742      1.070  7.3072     0.982  869.78
  85   4.6726      1.030  7.3059     1.046  880.16
