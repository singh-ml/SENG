Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11058974720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6757      0.950  9.6759     1.008  12.95
   2   4.6762      0.850  9.6737     1.018  24.38
   3   4.6761      0.960  9.6793     1.008  35.81
   4   4.6751      0.940  9.6743     1.010  47.15
   5   4.6759      0.900  9.6797     1.032  58.52
   6   4.6765      0.950  9.6789     0.938  70.03
   7   4.6757      0.920  9.6710     0.978  81.45
   8   4.6765      0.880  9.6777     1.002  92.88
   9   4.6757      0.910  9.6686     0.956  104.21
  10   4.6751      0.910  9.6747     1.076  115.61
  11   4.6761      0.940  9.6750     0.988  126.95
  12   4.6753      0.870  9.6759     1.016  138.36
  13   4.6755      0.980  9.6792     1.050  149.81
  14   4.6763      0.920  9.6785     1.022  161.23
  15   4.6762      0.890  9.6764     1.022  172.59
  16   4.6746      0.860  9.6788     1.014  184.06
  17   4.6748      0.940  9.6816     0.956  195.46
  18   4.6767      0.930  9.6834     0.970  206.91
  19   4.6755      0.880  9.6800     0.970  218.33
  20   4.6747      0.890  9.6787     1.028  229.68
  21   4.6758      0.830  9.6745     1.016  241.07
  22   4.6773      0.900  9.6880     0.942  252.46
  23   4.6767      0.930  9.6811     0.974  263.87
  24   4.6756      0.880  9.6738     1.028  275.30
  25   4.6770      0.940  9.6756     0.974  286.76
  26   4.6760      0.970  9.6766     1.018  298.13
  27   4.6751      0.920  9.6733     0.964  309.57
  28   4.6763      0.890  9.6745     0.974  321.06
  29   4.6746      0.880  9.6770     0.996  332.42
  30   4.6751      0.960  9.6774     0.970  343.81
  31   4.6757      0.930  9.6781     0.978  355.18
  32   4.6755      0.860  9.6786     0.994  366.69
  33   4.6759      0.890  9.6784     1.044  378.11
  34   4.6772      0.990  9.6795     0.946  389.47
  35   4.6749      0.890  9.6755     0.984  400.96
  36   4.6758      0.980  9.6763     0.990  412.39
  37   4.6762      0.930  9.6787     0.942  423.77
  38   4.6757      0.870  9.6852     1.020  435.24
  39   4.6766      0.960  9.6780     0.902  446.57
  40   4.6756      0.920  9.6759     0.962  457.85
  41   4.6758      0.960  9.6774     1.076  469.33
  42   4.6753      0.900  9.6787     0.896  480.77
  43   4.6765      0.980  9.6763     0.988  492.16
  44   4.6755      0.930  9.6756     0.978  503.60
  45   4.6753      0.910  9.6805     0.992  515.06
  46   4.6752      0.890  9.6801     0.966  526.44
  47   4.6757      0.830  9.6853     1.040  537.82
  48   4.6759      0.940  9.6779     0.980  549.24
  49   4.6762      0.970  9.6807     1.020  560.74
  50   4.6752      0.880  9.6755     1.032  572.07
  51   4.6748      0.990  9.6778     0.956  583.52
  52   4.6759      0.960  9.6813     0.960  594.96
  53   4.6758      0.880  9.6824     0.996  606.33
  54   4.6755      0.870  9.6748     1.024  617.82
  55   4.6767      0.920  9.6799     0.980  629.19
  56   4.6751      0.880  9.6785     1.052  640.59
  57   4.6749      0.850  9.6762     0.994  651.96
  58   4.6767      0.940  9.6784     0.990  663.33
  59   4.6756      0.840  9.6824     0.992  674.68
  60   4.6759      0.930  9.6815     0.990  686.18
  61   4.6747      0.980  9.6811     0.976  697.58
  62   4.6767      0.870  9.6778     0.990  708.93
  63   4.6771      0.930  9.6757     0.982  720.40
  64   4.6757      0.930  9.6773     1.000  731.75
  65   4.6765      0.910  9.6770     0.962  743.12
  66   4.6746      0.890  9.6843     0.922  754.50
  67   4.6757      0.830  9.6825     0.976  765.98
  68   4.6759      0.890  9.6739     1.028  777.36
  69   4.6753      0.940  9.6782     0.970  788.75
  70   4.6747      0.880  9.6762     0.976  800.09
  71   4.6749      0.840  9.6780     1.012  811.42
  72   4.6761      0.940  9.6780     1.010  822.88
  73   4.6761      0.850  9.6769     1.090  834.25
  74   4.6758      0.940  9.6786     1.028  845.71
  75   4.6761      0.900  9.6769     0.990  857.12
  76   4.6757      0.860  9.6775     1.068  868.50
  77   4.6760      0.960  9.6790     1.014  879.92
  78   4.6768      0.980  9.6804     0.930  891.26
  79   4.6756      0.950  9.6789     0.970  902.66
  80   4.6754      0.930  9.6772     0.972  914.16
  81   4.6760      0.900  9.6790     1.032  925.54
  82   4.6761      0.950  9.6792     1.046  936.94
  83   4.6759      0.950  9.6750     0.982  948.46
  84   4.6757      0.860  9.6780     0.986  959.83
  85   4.6765      0.890  9.6858     0.936  971.13
  86   4.6757      0.890  9.6848     1.022  982.56
  87   4.6768      0.890  9.6826     0.998  993.90
  88   4.6759      0.850  9.6757     0.954  1005.28
  89   4.6753      0.890  9.6740     0.964  1016.81
  90   4.6764      0.930  9.6857     0.958  1028.16
