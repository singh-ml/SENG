Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 12134561792 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6689      0.840  16.7521     0.966  13.63
   2   4.6688      0.860  16.7488     1.004  25.78
   3   4.6682      0.850  16.7507     0.938  38.00
   4   4.6682      0.870  16.7494     0.944  50.17
   5   4.6681      0.880  16.7469     1.044  62.31
   6   4.6683      0.830  16.7481     0.986  74.52
   7   4.6685      0.870  16.7565     0.964  86.57
   8   4.6684      0.840  16.7502     1.000  98.70
   9   4.6682      0.870  16.7551     0.952  110.85
  10   4.6675      0.850  16.7502     0.896  122.91
  11   4.6692      0.870  16.7494     0.992  135.02
  12   4.6688      0.820  16.7486     1.042  147.13
  13   4.6684      0.840  16.7551     0.970  159.23
  14   4.6690      0.870  16.7515     1.060  171.30
  15   4.6687      0.870  16.7498     0.958  183.47
  16   4.6700      0.830  16.7544     1.032  195.56
  17   4.6689      0.870  16.7459     0.996  207.73
  18   4.6681      0.860  16.7554     0.978  219.98
  19   4.6677      0.860  16.7480     0.966  232.20
  20   4.6694      0.790  16.7547     0.960  244.30
  21   4.6683      0.870  16.7522     1.010  256.34
  22   4.6686      0.890  16.7543     1.004  268.51
  23   4.6694      0.860  16.7502     0.990  280.64
  24   4.6679      0.850  16.7528     0.962  292.89
  25   4.6684      0.860  16.7569     0.952  305.04
  26   4.6675      0.860  16.7499     0.964  317.20
  27   4.6676      0.860  16.7506     0.976  329.34
  28   4.6686      0.850  16.7491     0.952  341.55
  29   4.6690      0.860  16.7482     1.014  353.62
  30   4.6673      0.840  16.7558     0.940  365.82
  31   4.6685      0.830  16.7570     1.016  377.88
  32   4.6685      0.890  16.7528     0.976  390.01
  33   4.6678      0.870  16.7557     0.928  402.16
  34   4.6682      0.860  16.7521     0.972  414.35
  35   4.6680      0.870  16.7519     0.996  426.43
  36   4.6681      0.850  16.7541     0.958  438.64
  37   4.6676      0.860  16.7546     0.964  450.87
  38   4.6680      0.880  16.7504     1.074  463.06
  39   4.6681      0.860  16.7543     0.946  475.18
  40   4.6679      0.870  16.7500     1.018  487.32
  41   4.6681      0.830  16.7543     1.042  499.42
  42   4.6685      0.870  16.7430     0.930  511.59
  43   4.6689      0.860  16.7531     0.932  523.70
  44   4.6678      0.870  16.7534     0.940  535.73
  45   4.6686      0.870  16.7495     0.948  547.94
  46   4.6683      0.850  16.7517     0.994  560.06
  47   4.6679      0.910  16.7555     0.954  572.15
  48   4.6685      0.840  16.7537     0.958  584.35
  49   4.6686      0.810  16.7543     1.012  596.49
  50   4.6681      0.880  16.7529     0.960  608.52
  51   4.6687      0.870  16.7581     0.972  620.65
  52   4.6674      0.860  16.7449     1.010  632.83
  53   4.6679      0.880  16.7510     0.866  644.95
  54   4.6683      0.860  16.7489     1.002  657.03
  55   4.6684      0.850  16.7555     0.926  669.18
  56   4.6672      0.860  16.7478     1.008  681.34
  57   4.6687      0.900  16.7521     0.990  693.43
  58   4.6669      0.870  16.7538     0.956  705.59
  59   4.6680      0.880  16.7507     0.968  717.69
  60   4.6682      0.860  16.7517     0.936  729.82
  61   4.6678      0.850  16.7524     0.994  741.94
  62   4.6687      0.860  16.7483     0.932  754.04
  63   4.6678      0.860  16.7479     0.992  766.11
  64   4.6682      0.890  16.7484     0.986  778.23
  65   4.6689      0.860  16.7484     0.930  790.31
  66   4.6679      0.830  16.7492     1.056  802.48
  67   4.6680      0.880  16.7500     0.998  814.65
  68   4.6679      0.860  16.7532     0.986  826.75
  69   4.6679      0.850  16.7556     0.952  838.97
  70   4.6685      0.820  16.7500     0.982  851.21
  71   4.6686      0.860  16.7545     0.914  863.38
  72   4.6688      0.890  16.7506     1.034  875.50
  73   4.6678      0.830  16.7478     1.008  887.65
  74   4.6676      0.860  16.7484     0.936  899.81
  75   4.6678      0.810  16.7509     0.994  911.92
  76   4.6687      0.840  16.7456     0.956  924.08
  77   4.6683      0.880  16.7511     0.964  936.19
  78   4.6680      0.890  16.7574     0.948  948.32
  79   4.6678      0.860  16.7538     0.974  960.54
  80   4.6684      0.830  16.7601     0.912  972.68
  81   4.6689      0.870  16.7520     0.896  984.77
  82   4.6686      0.850  16.7477     0.926  996.99
  83   4.6678      0.890  16.7547     0.966  1009.15
  84   4.6685      0.820  16.7517     0.934  1021.25
  85   4.6684      0.870  16.7510     0.960  1033.49
  86   4.6688      0.870  16.7555     0.904  1045.65
  87   4.6683      0.880  16.7503     1.008  1057.70
  88   4.6683      0.860  16.7517     1.002  1069.94
  89   4.6683      0.880  16.7526     0.926  1082.10
  90   4.6687      0.830  16.7547     0.938  1094.25
