Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6654      0.890  7.2800     0.982  11.87
   2   4.6654      0.840  7.2804     0.928  22.13
   3   4.6658      0.850  7.2801     0.952  32.40
   4   4.6667      0.900  7.2770     1.004  42.65
   5   4.6660      0.860  7.2714     0.938  52.93
   6   4.6671      0.870  7.2770     1.080  63.23
   7   4.6661      0.850  7.2775     1.064  73.48
   8   4.6657      0.860  7.2797     0.988  83.76
   9   4.6662      0.830  7.2753     1.080  94.12
  10   4.6660      0.900  7.2743     0.992  104.34
  11   4.6656      0.820  7.2772     0.994  114.63
  12   4.6659      0.820  7.2777     0.962  124.98
  13   4.6660      0.870  7.2767     0.956  135.25
  14   4.6653      0.940  7.2789     0.962  145.50
  15   4.6649      0.780  7.2808     1.006  155.74
  16   4.6654      0.850  7.2718     0.976  166.03
  17   4.6654      0.890  7.2761     0.946  176.28
  18   4.6660      0.890  7.2788     1.086  186.55
  19   4.6653      0.890  7.2771     0.958  196.85
  20   4.6658      0.950  7.2785     0.972  207.09
  21   4.6662      0.840  7.2735     0.988  217.34
  22   4.6660      0.880  7.2743     1.012  227.57
  23   4.6666      0.850  7.2748     0.960  237.90
  24   4.6654      0.890  7.2836     1.048  248.14
  25   4.6663      0.860  7.2800     1.018  258.36
  26   4.6657      0.890  7.2746     1.038  268.66
  27   4.6660      0.900  7.2778     0.940  278.90
  28   4.6655      0.910  7.2764     0.994  289.13
  29   4.6654      0.830  7.2781     1.038  299.46
  30   4.6661      0.870  7.2762     1.046  309.69
  31   4.6655      0.820  7.2776     0.974  319.90
  32   4.6657      0.880  7.2739     1.004  330.14
  33   4.6660      0.830  7.2761     1.066  340.45
  34   4.6662      0.880  7.2783     1.010  350.68
  35   4.6666      0.840  7.2764     0.962  360.93
  36   4.6659      0.860  7.2776     0.946  371.17
  37   4.6655      0.850  7.2724     0.974  381.54
  38   4.6657      0.910  7.2739     0.964  391.77
  39   4.6652      0.800  7.2768     0.946  401.99
  40   4.6651      0.810  7.2807     0.934  412.31
  41   4.6659      0.800  7.2813     0.958  422.58
  42   4.6664      0.890  7.2739     1.010  432.79
  43   4.6664      0.840  7.2751     0.982  443.12
  44   4.6653      0.860  7.2816     1.026  453.37
  45   4.6658      0.870  7.2778     1.076  463.57
  46   4.6654      0.870  7.2756     0.964  473.82
  47   4.6659      0.870  7.2747     1.004  484.06
  48   4.6659      0.840  7.2775     0.994  494.32
  49   4.6647      0.870  7.2747     1.004  504.54
  50   4.6662      0.840  7.2739     1.032  514.89
  51   4.6657      0.780  7.2784     0.926  525.13
  52   4.6669      0.770  7.2750     1.026  535.39
  53   4.6660      0.850  7.2809     1.020  545.64
  54   4.6658      0.880  7.2761     1.026  555.95
  55   4.6657      0.830  7.2778     1.020  566.21
  56   4.6661      0.810  7.2758     1.016  576.50
  57   4.6651      0.830  7.2748     1.054  586.69
  58   4.6659      0.860  7.2736     1.004  596.90
  59   4.6654      0.830  7.2729     1.082  607.16
  60   4.6657      0.840  7.2747     0.948  617.45
  61   4.6654      0.840  7.2743     1.002  627.81
  62   4.6653      0.910  7.2764     1.010  638.03
  63   4.6654      0.870  7.2786     0.996  648.21
  64   4.6658      0.870  7.2823     0.998  658.53
  65   4.6655      0.810  7.2812     0.976  668.78
  66   4.6661      0.820  7.2738     1.002  678.98
  67   4.6660      0.830  7.2714     1.032  689.34
  68   4.6658      0.860  7.2763     0.920  699.62
  69   4.6663      0.870  7.2784     1.060  709.86
  70   4.6655      0.810  7.2828     0.980  720.09
  71   4.6661      0.850  7.2758     0.964  730.41
  72   4.6651      0.920  7.2791     0.948  740.65
  73   4.6662      0.860  7.2771     0.998  750.89
  74   4.6655      0.870  7.2748     0.964  761.19
  75   4.6655      0.810  7.2728     0.944  771.40
  76   4.6659      0.840  7.2764     0.996  781.59
  77   4.6652      0.860  7.2749     0.938  791.82
  78   4.6659      0.830  7.2754     1.038  802.17
  79   4.6654      0.890  7.2762     0.952  812.41
  80   4.6653      0.900  7.2761     1.044  822.62
  81   4.6671      0.930  7.2773     0.946  832.96
  82   4.6652      0.770  7.2811     0.946  843.21
  83   4.6667      0.840  7.2726     0.912  853.45
  84   4.6658      0.880  7.2794     1.036  863.73
  85   4.6660      0.850  7.2775     1.102  873.95
