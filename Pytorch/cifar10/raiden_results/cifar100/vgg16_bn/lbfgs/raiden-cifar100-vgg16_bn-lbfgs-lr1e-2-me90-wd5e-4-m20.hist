Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11059367936 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6787      0.900  16.7497     1.042  13.58
   2   4.6776      0.960  16.7468     1.004  25.51
   3   4.6773      0.930  16.7487     0.914  37.37
   4   4.6775      0.940  16.7511     0.936  49.32
   5   4.6764      0.930  16.7493     1.060  61.24
   6   4.6768      0.980  16.7473     1.088  73.21
   7   4.6763      0.940  16.7470     0.980  85.09
   8   4.6772      0.920  16.7512     0.962  96.98
   9   4.6778      0.910  16.7483     1.030  108.97
  10   4.6774      0.860  16.7462     1.092  120.84
  11   4.6768      0.920  16.7497     0.926  132.65
  12   4.6781      0.870  16.7476     1.016  144.58
  13   4.6770      0.910  16.7501     0.968  156.48
  14   4.6780      0.880  16.7502     0.970  168.44
  15   4.6771      0.900  16.7484     1.016  180.45
  16   4.6769      0.940  16.7504     0.970  192.34
  17   4.6770      0.880  16.7516     0.986  204.28
  18   4.6777      0.940  16.7491     0.940  216.23
  19   4.6764      0.970  16.7449     1.006  228.12
  20   4.6769      0.940  16.7445     1.022  240.03
  21   4.6774      0.960  16.7470     0.924  251.86
  22   4.6775      0.960  16.7472     1.038  263.86
  23   4.6768      0.930  16.7489     0.932  275.74
  24   4.6774      0.980  16.7552     0.994  287.61
  25   4.6767      0.950  16.7475     1.010  299.54
  26   4.6777      0.920  16.7493     0.922  311.39
  27   4.6780      0.930  16.7508     0.966  323.28
  28   4.6770      0.960  16.7446     0.948  335.28
  29   4.6765      0.960  16.7496     0.946  347.12
  30   4.6767      0.930  16.7484     0.954  359.04
  31   4.6776      0.930  16.7400     1.024  370.98
  32   4.6783      0.910  16.7477     1.022  382.85
  33   4.6783      0.890  16.7487     0.976  394.67
  34   4.6771      0.970  16.7482     1.068  406.57
  35   4.6763      0.980  16.7473     0.934  418.44
  36   4.6769      0.970  16.7436     1.000  430.24
  37   4.6769      0.900  16.7419     1.012  442.23
  38   4.6784      0.860  16.7498     1.090  454.15
  39   4.6772      0.930  16.7499     0.990  465.99
  40   4.6769      0.980  16.7479     0.986  477.86
  41   4.6767      0.900  16.7461     0.978  489.74
  42   4.6765      0.940  16.7482     1.086  501.67
  43   4.6773      0.880  16.7467     0.960  513.57
  44   4.6779      0.880  16.7509     1.024  525.57
  45   4.6775      0.960  16.7482     1.014  537.43
  46   4.6762      0.920  16.7484     0.994  549.35
  47   4.6772      0.920  16.7493     1.010  561.30
  48   4.6764      0.960  16.7473     0.982  573.21
  49   4.6769      0.960  16.7437     1.066  585.11
  50   4.6766      0.890  16.7459     0.976  596.98
  51   4.6779      0.940  16.7474     1.034  608.87
  52   4.6768      0.910  16.7494     1.028  620.73
  53   4.6770      0.940  16.7527     1.046  632.68
  54   4.6779      0.910  16.7461     1.038  644.56
  55   4.6766      0.920  16.7483     0.986  656.47
  56   4.6765      0.970  16.7400     1.072  668.42
  57   4.6775      0.910  16.7483     1.040  680.30
  58   4.6774      0.930  16.7483     0.984  692.12
  59   4.6767      0.890  16.7458     0.994  704.07
  60   4.6783      0.930  16.7492     0.992  715.94
  61   4.6769      0.970  16.7490     0.974  727.82
  62   4.6766      0.940  16.7441     0.962  739.74
  63   4.6773      0.890  16.7483     1.028  751.65
  64   4.6773      0.950  16.7480     1.018  763.51
  65   4.6760      0.950  16.7462     0.984  775.49
  66   4.6767      0.930  16.7473     1.026  787.43
  67   4.6773      0.920  16.7456     1.072  799.29
  68   4.6760      1.010  16.7487     1.058  811.32
  69   4.6763      1.020  16.7507     1.022  823.24
  70   4.6768      0.880  16.7532     1.058  835.15
  71   4.6777      0.970  16.7388     1.076  847.13
  72   4.6774      0.890  16.7454     0.998  858.99
  73   4.6772      0.940  16.7518     0.934  870.84
  74   4.6771      0.920  16.7464     0.966  882.82
  75   4.6778      0.950  16.7467     1.014  894.71
  76   4.6770      0.930  16.7468     0.958  906.58
  77   4.6777      0.960  16.7473     0.966  918.50
  78   4.6771      0.870  16.7507     0.970  930.39
  79   4.6769      0.960  16.7480     1.062  942.29
  80   4.6770      0.990  16.7505     0.996  954.20
  81   4.6767      1.020  16.7562     0.986  966.11
  82   4.6771      0.900  16.7555     0.912  978.01
  83   4.6765      0.920  16.7521     1.060  989.89
  84   4.6772      0.970  16.7536     0.958  1001.78
  85   4.6766      0.990  16.7500     0.892  1013.59
  86   4.6775      0.930  16.7472     1.078  1025.61
  87   4.6766      0.950  16.7481     1.026  1037.48
  88   4.6769      0.980  16.7471     1.042  1049.32
  89   4.6770      0.940  16.7484     1.004  1061.32
  90   4.6770      0.930  16.7472     1.002  1073.24
