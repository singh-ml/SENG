Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11052814336 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6673      0.930  7.2786     1.002  12.45
   2   4.6669      0.910  7.2842     1.020  23.33
   3   4.6668      0.790  7.2819     1.008  34.09
   4   4.6678      0.850  7.2868     1.006  44.87
   5   4.6678      0.880  7.2829     0.954  55.73
   6   4.6675      0.810  7.2838     0.954  66.47
   7   4.6673      0.800  7.2853     0.964  77.27
   8   4.6661      0.890  7.2851     1.046  88.06
   9   4.6673      0.850  7.2838     0.970  98.93
  10   4.6677      0.800  7.2775     1.020  109.73
  11   4.6675      0.860  7.2829     1.014  120.51
  12   4.6675      0.870  7.2886     0.948  131.37
  13   4.6668      0.880  7.2824     1.084  142.15
  14   4.6666      0.890  7.2816     0.916  152.94
  15   4.6668      0.880  7.2831     1.034  163.80
  16   4.6677      0.880  7.2815     0.944  174.61
  17   4.6668      0.880  7.2806     0.980  185.37
  18   4.6682      0.910  7.2823     1.048  196.12
  19   4.6679      0.900  7.2784     0.990  207.02
  20   4.6667      0.850  7.2837     1.014  217.81
  21   4.6662      0.860  7.2873     0.968  228.60
  22   4.6679      0.870  7.2831     0.946  239.47
  23   4.6674      0.880  7.2871     0.946  250.22
  24   4.6656      0.850  7.2775     0.994  261.03
  25   4.6666      0.920  7.2831     0.988  271.92
  26   4.6667      0.850  7.2861     1.098  282.70
  27   4.6686      0.910  7.2838     0.944  293.48
  28   4.6674      0.880  7.2846     0.944  304.33
  29   4.6681      0.930  7.2798     0.986  315.09
  30   4.6662      0.830  7.2813     1.046  325.83
  31   4.6668      0.860  7.2825     1.028  336.60
  32   4.6664      0.900  7.2778     1.012  347.46
  33   4.6663      0.870  7.2735     1.062  358.28
  34   4.6672      0.880  7.2787     0.996  369.05
  35   4.6667      0.850  7.2887     0.936  379.90
  36   4.6667      0.890  7.2825     0.970  390.70
  37   4.6672      0.810  7.2775     1.106  401.49
  38   4.6668      0.820  7.2786     0.982  412.33
  39   4.6672      0.890  7.2803     1.000  423.12
  40   4.6667      0.890  7.2839     0.964  433.89
  41   4.6664      0.880  7.2800     1.038  444.68
  42   4.6665      0.820  7.2785     0.904  455.51
  43   4.6674      0.840  7.2814     0.986  466.28
  44   4.6667      0.840  7.2779     0.982  477.05
  45   4.6666      0.800  7.2817     1.032  487.91
  46   4.6675      0.820  7.2752     1.110  498.69
  47   4.6670      0.770  7.2835     1.014  509.46
  48   4.6662      0.790  7.2828     0.976  520.33
  49   4.6675      0.920  7.2838     1.030  531.09
  50   4.6661      0.850  7.2849     1.038  541.88
  51   4.6666      0.900  7.2843     0.980  552.66
  52   4.6671      0.860  7.2809     1.044  563.49
  53   4.6667      0.810  7.2818     1.002  574.26
  54   4.6679      0.920  7.2840     1.012  585.02
  55   4.6669      0.830  7.2791     0.956  595.85
  56   4.6668      0.870  7.2834     1.010  606.63
  57   4.6668      0.840  7.2817     1.044  617.38
  58   4.6663      0.850  7.2837     1.012  628.18
  59   4.6654      0.830  7.2833     1.046  639.05
  60   4.6664      0.880  7.2804     0.960  649.85
  61   4.6672      0.840  7.2820     1.078  660.67
  62   4.6671      0.860  7.2860     1.048  671.51
  63   4.6667      0.870  7.2817     1.016  682.30
  64   4.6673      0.920  7.2800     1.018  693.09
  65   4.6677      0.830  7.2817     0.916  703.93
  66   4.6669      0.840  7.2846     1.022  714.72
  67   4.6681      0.870  7.2841     0.988  725.49
  68   4.6670      0.820  7.2844     0.966  736.34
  69   4.6674      0.820  7.2791     0.972  747.14
  70   4.6677      0.780  7.2857     0.974  757.94
  71   4.6666      0.820  7.2871     0.980  768.74
  72   4.6674      0.920  7.2830     1.064  779.62
  73   4.6658      0.840  7.2846     0.940  790.39
  74   4.6670      0.840  7.2843     1.000  801.18
  75   4.6663      0.830  7.2823     1.030  812.03
  76   4.6662      0.860  7.2825     1.076  822.79
  77   4.6683      0.840  7.2852     1.010  833.59
  78   4.6679      0.840  7.2860     1.002  844.46
  79   4.6678      0.890  7.2804     0.938  855.25
  80   4.6673      0.780  7.2856     0.850  866.08
  81   4.6679      0.870  7.2791     0.936  876.92
  82   4.6668      0.850  7.2817     0.942  887.67
  83   4.6661      0.840  7.2784     1.066  898.47
  84   4.6671      0.940  7.2869     0.952  909.25
  85   4.6675      0.930  7.2832     0.948  920.12
  86   4.6676      0.860  7.2805     0.988  930.92
  87   4.6663      0.890  7.2825     1.024  941.71
  88   4.6666      0.860  7.2845     1.008  952.56
  89   4.6667      0.860  7.2849     1.030  963.34
  90   4.6663      0.880  7.2797     1.062  974.16
