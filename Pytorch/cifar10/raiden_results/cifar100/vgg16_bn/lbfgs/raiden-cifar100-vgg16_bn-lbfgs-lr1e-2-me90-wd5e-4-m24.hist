Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11057008640 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6684      0.860  16.7381     1.010  13.04
   2   4.6687      0.910  16.7362     0.946  24.51
   3   4.6688      0.840  16.7305     1.024  35.85
   4   4.6688      0.860  16.7401     0.904  47.33
   5   4.6691      0.850  16.7410     0.980  58.64
   6   4.6677      0.850  16.7464     0.952  70.09
   7   4.6689      0.830  16.7404     0.972  81.51
   8   4.6677      0.900  16.7401     0.990  92.87
   9   4.6682      0.920  16.7370     0.960  104.18
  10   4.6684      0.840  16.7444     1.034  115.57
  11   4.6684      0.910  16.7354     0.996  127.10
  12   4.6678      0.970  16.7393     1.084  138.47
  13   4.6685      0.890  16.7431     0.934  149.86
  14   4.6682      0.790  16.7443     1.006  161.34
  15   4.6682      0.810  16.7435     1.006  172.79
  16   4.6679      0.850  16.7417     0.972  184.13
  17   4.6679      0.860  16.7402     1.002  195.52
  18   4.6682      0.920  16.7365     1.078  206.96
  19   4.6687      0.780  16.7389     0.994  218.40
  20   4.6681      0.820  16.7375     0.980  229.79
  21   4.6683      0.860  16.7440     0.976  241.20
  22   4.6684      0.880  16.7376     1.002  252.63
  23   4.6678      0.830  16.7437     0.962  264.00
  24   4.6687      0.860  16.7415     1.008  275.44
  25   4.6684      0.920  16.7470     0.906  286.82
  26   4.6680      0.910  16.7398     1.004  298.21
  27   4.6682      0.920  16.7375     0.926  309.58
  28   4.6681      0.880  16.7393     0.966  321.01
  29   4.6687      0.860  16.7425     1.050  332.40
  30   4.6680      0.860  16.7353     0.992  343.87
  31   4.6683      0.800  16.7405     1.004  355.32
  32   4.6687      0.820  16.7419     0.896  366.64
  33   4.6685      0.930  16.7373     1.022  378.08
  34   4.6678      0.840  16.7444     0.914  389.49
  35   4.6691      0.830  16.7418     1.030  400.88
  36   4.6689      0.880  16.7388     0.980  412.31
  37   4.6684      0.800  16.7387     0.978  423.78
  38   4.6680      0.850  16.7465     0.936  435.15
  39   4.6687      0.840  16.7413     0.948  446.50
  40   4.6687      0.860  16.7422     0.986  457.97
  41   4.6682      0.920  16.7358     0.994  469.39
  42   4.6677      0.890  16.7411     0.964  480.78
  43   4.6673      0.880  16.7433     0.968  492.18
  44   4.6681      0.870  16.7454     0.980  503.54
  45   4.6685      0.890  16.7430     0.946  514.85
  46   4.6688      0.840  16.7398     1.024  526.26
  47   4.6682      0.890  16.7383     0.998  537.74
  48   4.6689      0.850  16.7410     0.900  549.07
  49   4.6682      0.870  16.7426     0.912  560.49
  50   4.6685      0.870  16.7411     0.980  571.94
  51   4.6679      0.980  16.7380     0.944  583.38
  52   4.6685      0.810  16.7369     0.986  594.80
  53   4.6683      0.850  16.7416     1.014  606.26
  54   4.6684      0.880  16.7389     1.022  617.55
  55   4.6681      0.810  16.7474     0.916  628.90
  56   4.6690      0.940  16.7379     0.934  640.27
  57   4.6683      0.930  16.7430     0.966  651.64
  58   4.6681      0.840  16.7352     1.060  662.97
  59   4.6680      0.930  16.7446     1.012  674.38
  60   4.6678      0.870  16.7391     0.954  685.75
  61   4.6680      0.900  16.7448     0.992  697.18
  62   4.6683      0.890  16.7368     0.970  708.53
  63   4.6687      0.840  16.7365     1.010  720.09
  64   4.6679      0.950  16.7368     1.028  731.48
  65   4.6688      0.890  16.7417     0.984  742.87
  66   4.6683      0.830  16.7413     0.986  754.30
  67   4.6685      0.870  16.7363     1.000  765.72
  68   4.6684      0.890  16.7360     1.014  777.11
  69   4.6681      0.870  16.7384     0.992  788.59
  70   4.6681      0.780  16.7401     1.024  799.96
  71   4.6685      0.930  16.7357     1.018  811.33
  72   4.6692      0.830  16.7423     0.930  822.82
  73   4.6683      0.930  16.7449     0.960  834.23
  74   4.6678      0.910  16.7422     1.010  845.60
  75   4.6686      0.940  16.7445     0.972  856.91
  76   4.6684      0.820  16.7405     0.980  868.31
  77   4.6687      0.850  16.7422     0.914  879.79
  78   4.6684      0.910  16.7397     0.924  891.28
  79   4.6673      0.930  16.7462     0.934  902.71
  80   4.6679      0.830  16.7430     1.026  914.08
  81   4.6693      0.870  16.7412     0.906  925.47
  82   4.6684      0.870  16.7300     1.040  936.83
  83   4.6683      0.850  16.7311     0.966  948.26
  84   4.6683      0.790  16.7425     1.100  959.69
  85   4.6680      0.900  16.7470     0.866  971.26
  86   4.6684      0.890  16.7407     0.942  982.70
  87   4.6680      0.840  16.7409     0.952  994.07
  88   4.6676      0.870  16.7430     0.998  1005.57
  89   4.6683      0.850  16.7409     0.952  1017.00
  90   4.6681      0.880  16.7402     0.962  1028.39
