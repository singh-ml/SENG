Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11057673216 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6798      0.830  16.7592     0.958  12.99
   2   4.6799      0.800  16.7604     1.076  24.19
   3   4.6800      0.810  16.7609     0.954  35.37
   4   4.6798      0.800  16.7589     1.042  46.64
   5   4.6797      0.870  16.7637     0.980  57.77
   6   4.6802      0.880  16.7597     0.980  68.90
   7   4.6801      0.890  16.7572     1.024  80.15
   8   4.6802      0.900  16.7672     0.976  91.30
   9   4.6795      0.900  16.7660     1.014  102.45
  10   4.6796      0.810  16.7645     0.932  113.62
  11   4.6804      0.900  16.7619     0.936  124.89
  12   4.6800      0.840  16.7674     0.998  136.08
  13   4.6805      0.850  16.7604     1.032  147.33
  14   4.6790      0.790  16.7619     0.994  158.56
  15   4.6801      0.810  16.7616     0.952  169.67
  16   4.6804      0.870  16.7589     0.932  180.89
  17   4.6799      0.840  16.7624     1.044  192.09
  18   4.6807      0.840  16.7589     1.012  203.26
  19   4.6787      0.860  16.7646     1.030  214.45
  20   4.6801      0.800  16.7658     0.950  225.77
  21   4.6795      0.870  16.7671     0.910  237.00
  22   4.6798      0.920  16.7620     0.974  248.24
  23   4.6796      0.860  16.7699     0.966  259.61
  24   4.6798      0.900  16.7637     0.998  270.78
  25   4.6799      0.830  16.7596     1.048  282.01
  26   4.6799      0.840  16.7608     0.980  293.31
  27   4.6789      0.840  16.7605     0.994  304.50
  28   4.6800      0.850  16.7608     0.970  315.71
  29   4.6801      0.840  16.7624     1.008  326.93
  30   4.6788      0.860  16.7646     0.934  338.17
  31   4.6801      0.860  16.7599     1.070  349.30
  32   4.6793      0.890  16.7631     1.004  360.52
  33   4.6791      0.800  16.7574     1.076  371.73
  34   4.6798      0.850  16.7591     1.004  382.90
  35   4.6799      0.830  16.7615     1.108  394.08
  36   4.6800      0.910  16.7597     0.906  405.32
  37   4.6799      0.860  16.7544     1.024  416.47
  38   4.6795      0.850  16.7673     0.956  427.66
  39   4.6800      0.860  16.7623     0.992  438.95
  40   4.6803      0.830  16.7629     0.960  450.13
  41   4.6796      0.880  16.7563     1.014  461.30
  42   4.6804      0.940  16.7568     1.006  472.58
  43   4.6795      0.830  16.7563     1.034  483.76
  44   4.6791      0.860  16.7600     0.962  494.94
  45   4.6807      0.860  16.7565     1.048  506.20
  46   4.6799      0.810  16.7637     1.020  517.44
  47   4.6798      0.860  16.7611     1.052  528.67
  48   4.6795      0.890  16.7624     0.982  539.98
  49   4.6796      0.900  16.7625     1.020  551.26
  50   4.6801      0.850  16.7645     0.956  562.43
  51   4.6804      0.870  16.7642     1.002  573.61
  52   4.6792      0.840  16.7604     1.062  584.71
  53   4.6794      0.780  16.7603     1.004  595.95
  54   4.6798      0.830  16.7596     0.934  607.14
  55   4.6800      0.770  16.7617     1.000  618.33
  56   4.6796      0.860  16.7624     1.040  629.63
  57   4.6792      0.810  16.7606     0.896  640.80
  58   4.6788      0.890  16.7579     1.086  651.94
  59   4.6796      0.870  16.7657     1.054  663.18
  60   4.6795      0.910  16.7668     0.968  674.40
  61   4.6787      0.830  16.7574     1.026  685.58
  62   4.6802      0.880  16.7552     1.020  696.83
  63   4.6804      0.860  16.7632     0.982  708.08
  64   4.6798      0.880  16.7652     0.942  719.25
  65   4.6796      0.880  16.7636     0.992  730.48
  66   4.6793      0.810  16.7606     0.998  741.71
  67   4.6804      0.860  16.7643     0.904  752.83
  68   4.6802      0.830  16.7612     1.012  764.03
  69   4.6796      0.860  16.7586     0.988  775.28
  70   4.6800      0.890  16.7638     0.962  786.45
  71   4.6803      0.870  16.7585     1.010  797.67
  72   4.6794      0.830  16.7538     1.014  808.93
  73   4.6812      0.850  16.7663     0.974  820.09
  74   4.6804      0.840  16.7569     0.920  831.30
  75   4.6797      0.860  16.7589     1.042  842.57
  76   4.6801      0.870  16.7634     1.150  853.75
  77   4.6795      0.880  16.7678     0.920  864.98
  78   4.6805      0.890  16.7643     1.002  876.32
  79   4.6803      0.840  16.7624     1.020  887.56
  80   4.6801      0.830  16.7612     1.016  898.77
  81   4.6801      0.860  16.7616     1.086  910.08
  82   4.6798      0.800  16.7590     1.024  921.23
  83   4.6801      0.870  16.7642     1.008  932.36
  84   4.6801      0.840  16.7549     1.060  943.57
  85   4.6794      0.850  16.7619     1.014  954.69
