Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 12137305088 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6785      0.930  7.3117     1.000  14.15
   2   4.6787      0.920  7.3085     0.922  26.70
   3   4.6784      0.930  7.3062     0.974  39.29
   4   4.6783      0.900  7.3106     0.994  51.91
   5   4.6787      0.920  7.3111     1.064  64.39
   6   4.6788      0.930  7.3088     0.980  76.88
   7   4.6794      0.950  7.3067     0.946  89.53
   8   4.6787      0.960  7.3109     0.964  102.04
   9   4.6798      0.940  7.3088     0.998  114.51
  10   4.6792      0.900  7.3036     1.008  127.17
  11   4.6780      0.940  7.3063     0.938  139.64
  12   4.6787      0.900  7.3074     1.040  152.17
  13   4.6792      0.910  7.3079     0.956  164.68
  14   4.6782      0.930  7.3140     1.048  177.24
  15   4.6785      0.910  7.3091     1.018  189.84
  16   4.6782      0.930  7.3075     0.990  202.30
  17   4.6791      0.920  7.3088     1.006  214.86
  18   4.6798      0.950  7.3099     0.976  227.45
  19   4.6782      0.910  7.3101     0.982  240.01
  20   4.6788      0.930  7.3141     0.940  252.53
  21   4.6786      0.920  7.3069     0.958  265.06
  22   4.6790      0.930  7.3123     1.000  277.54
  23   4.6785      0.900  7.3071     1.046  290.08
  24   4.6790      0.920  7.3066     0.944  302.55
  25   4.6785      0.920  7.3051     0.972  315.11
  26   4.6778      0.890  7.3099     0.992  327.58
  27   4.6786      0.930  7.3094     0.968  340.21
  28   4.6790      0.950  7.3072     0.994  352.70
  29   4.6786      0.920  7.3022     1.016  365.25
  30   4.6789      0.910  7.3080     1.008  377.85
  31   4.6783      0.940  7.3077     0.940  390.34
  32   4.6784      0.910  7.3094     1.000  402.85
  33   4.6784      0.910  7.3036     1.068  415.48
  34   4.6785      0.890  7.3128     0.970  427.99
  35   4.6788      0.910  7.3078     0.986  440.41
  36   4.6787      0.880  7.3133     1.082  453.04
  37   4.6788      0.900  7.3062     0.962  465.47
  38   4.6791      0.910  7.3038     1.010  478.01
  39   4.6785      0.930  7.3168     1.032  490.57
  40   4.6791      0.910  7.3049     0.906  503.08
  41   4.6788      0.910  7.3076     0.902  515.57
  42   4.6788      0.900  7.3074     1.038  528.21
  43   4.6782      0.920  7.3102     1.090  540.75
  44   4.6788      0.900  7.3088     0.938  553.38
  45   4.6782      0.940  7.3097     0.990  565.89
  46   4.6780      0.930  7.3030     1.018  578.38
  47   4.6780      0.930  7.3050     0.956  590.99
  48   4.6788      0.950  7.3082     1.038  603.46
  49   4.6791      0.940  7.3062     0.988  615.93
  50   4.6786      0.940  7.3146     0.964  628.49
  51   4.6782      0.920  7.3094     0.968  640.98
  52   4.6787      0.960  7.3054     0.980  653.50
  53   4.6785      0.940  7.3128     0.970  666.06
  54   4.6782      0.900  7.3062     0.972  678.56
  55   4.6781      0.920  7.3073     0.954  691.03
  56   4.6788      0.930  7.3052     1.004  703.67
  57   4.6790      0.940  7.3017     1.036  716.17
  58   4.6795      0.940  7.3077     1.004  728.65
  59   4.6783      0.930  7.3064     1.036  741.21
  60   4.6785      0.920  7.3087     1.062  753.67
  61   4.6785      0.920  7.3099     1.000  766.18
  62   4.6790      0.910  7.3120     1.014  778.85
  63   4.6788      0.910  7.3086     0.960  791.37
  64   4.6791      0.960  7.3094     0.992  803.92
  65   4.6780      0.940  7.3084     0.998  816.51
  66   4.6783      0.910  7.3081     0.962  829.01
  67   4.6792      0.910  7.3094     0.960  841.58
  68   4.6786      0.890  7.3097     0.990  854.15
  69   4.6785      0.910  7.3130     1.006  866.67
  70   4.6800      0.940  7.3101     1.040  879.26
  71   4.6794      0.890  7.3078     0.996  891.81
  72   4.6784      0.900  7.3065     1.030  904.28
  73   4.6792      0.910  7.3078     0.976  916.87
  74   4.6784      0.920  7.3114     0.956  929.41
  75   4.6788      0.930  7.3129     0.946  941.92
  76   4.6782      0.920  7.3055     1.028  954.37
  77   4.6787      0.890  7.3044     1.066  966.91
  78   4.6793      0.930  7.3064     0.946  979.44
  79   4.6781      0.900  7.3034     1.026  992.01
  80   4.6786      0.920  7.3056     0.982  1004.45
  81   4.6797      0.900  7.3091     1.052  1016.96
  82   4.6783      0.910  7.3024     0.932  1029.55
  83   4.6782      0.950  7.3118     1.032  1042.03
  84   4.6785      0.930  7.3080     0.956  1054.52
  85   4.6790      0.890  7.3062     0.984  1067.13
  86   4.6790      0.920  7.3050     0.988  1079.62
  87   4.6789      0.930  7.3088     0.938  1092.10
  88   4.6789      0.930  7.3064     0.978  1104.71
  89   4.6789      0.940  7.3070     1.016  1117.15
  90   4.6785      0.930  7.3060     0.940  1129.69
