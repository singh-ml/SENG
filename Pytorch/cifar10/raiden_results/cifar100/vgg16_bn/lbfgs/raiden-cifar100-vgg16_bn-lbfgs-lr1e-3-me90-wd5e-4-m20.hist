Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8904017920 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6668      0.880  16.7666     1.014  11.97
   2   4.6666      0.930  16.7538     1.026  22.29
   3   4.6672      0.920  16.7554     0.986  32.62
   4   4.6671      0.910  16.7576     0.976  42.96
   5   4.6677      0.900  16.7596     1.046  53.37
   6   4.6668      0.890  16.7539     0.970  63.70
   7   4.6670      0.910  16.7529     1.012  74.00
   8   4.6667      0.920  16.7522     1.068  84.43
   9   4.6666      0.870  16.7619     1.046  94.74
  10   4.6674      0.900  16.7524     0.974  105.08
  11   4.6680      0.900  16.7578     1.018  115.39
  12   4.6673      0.920  16.7520     0.950  125.75
  13   4.6670      0.930  16.7581     0.968  136.10
  14   4.6685      0.880  16.7601     0.958  146.37
  15   4.6661      0.880  16.7569     0.964  156.81
  16   4.6674      0.920  16.7538     0.980  167.08
  17   4.6677      0.870  16.7589     1.022  177.44
  18   4.6675      0.950  16.7506     1.072  187.83
  19   4.6673      0.890  16.7598     0.936  198.12
  20   4.6670      0.940  16.7583     1.038  208.46
  21   4.6670      0.920  16.7546     1.032  218.76
  22   4.6671      0.930  16.7531     1.034  229.14
  23   4.6670      0.960  16.7587     0.988  239.45
  24   4.6676      0.940  16.7506     0.952  249.77
  25   4.6675      0.850  16.7594     0.978  260.08
  26   4.6676      0.880  16.7566     1.000  270.46
  27   4.6683      0.940  16.7591     1.012  280.77
  28   4.6683      0.950  16.7583     1.062  291.08
  29   4.6671      0.920  16.7601     1.010  301.49
  30   4.6668      0.920  16.7581     0.954  311.81
  31   4.6676      0.930  16.7544     1.034  322.14
  32   4.6670      0.950  16.7592     1.018  332.55
  33   4.6670      0.920  16.7542     1.030  342.88
  34   4.6675      0.890  16.7576     1.014  353.22
  35   4.6675      0.890  16.7566     0.966  363.53
  36   4.6667      0.880  16.7568     1.014  373.89
  37   4.6675      0.910  16.7551     1.072  384.20
  38   4.6677      0.890  16.7556     0.970  394.55
  39   4.6673      0.900  16.7589     0.944  404.94
  40   4.6686      0.910  16.7553     1.028  415.26
  41   4.6672      0.920  16.7591     0.976  425.57
  42   4.6675      0.890  16.7555     1.042  435.89
  43   4.6674      0.920  16.7493     1.058  446.26
  44   4.6670      0.850  16.7536     0.988  456.56
  45   4.6664      0.900  16.7579     1.018  466.84
  46   4.6675      0.930  16.7517     1.016  477.29
  47   4.6677      0.890  16.7547     1.008  487.61
  48   4.6674      0.900  16.7527     1.042  497.90
  49   4.6670      0.880  16.7589     0.978  508.20
  50   4.6664      0.880  16.7626     0.910  518.61
  51   4.6668      0.910  16.7523     1.000  528.93
  52   4.6678      0.890  16.7547     1.018  539.23
  53   4.6669      0.950  16.7597     0.964  549.65
  54   4.6668      0.850  16.7600     0.886  559.98
  55   4.6681      0.950  16.7559     1.000  570.28
  56   4.6668      0.890  16.7535     1.020  580.67
  57   4.6667      0.920  16.7574     0.990  590.95
  58   4.6673      0.920  16.7555     0.970  601.30
  59   4.6671      0.930  16.7563     1.008  611.58
  60   4.6668      0.910  16.7507     1.028  622.00
  61   4.6672      0.910  16.7589     0.972  632.30
  62   4.6670      0.920  16.7531     1.018  642.65
  63   4.6670      0.920  16.7566     1.050  652.94
  64   4.6672      0.920  16.7609     0.892  663.33
  65   4.6670      0.850  16.7575     1.006  673.65
  66   4.6669      0.930  16.7587     0.984  683.98
  67   4.6673      0.910  16.7570     0.970  694.40
  68   4.6665      0.960  16.7558     0.996  704.70
  69   4.6667      0.920  16.7548     1.022  714.97
  70   4.6675      0.920  16.7587     1.058  725.28
  71   4.6674      0.900  16.7589     1.010  735.63
  72   4.6665      0.930  16.7532     0.986  745.94
  73   4.6677      0.930  16.7514     0.960  756.28
  74   4.6675      0.910  16.7540     1.032  766.70
  75   4.6665      0.890  16.7618     0.938  777.03
  76   4.6682      0.960  16.7572     1.008  787.31
  77   4.6671      0.930  16.7608     0.956  797.65
  78   4.6669      0.890  16.7528     1.030  808.09
  79   4.6674      0.900  16.7572     0.978  818.43
  80   4.6676      0.890  16.7576     1.020  828.76
  81   4.6669      0.980  16.7523     1.016  839.09
  82   4.6676      0.930  16.7566     0.986  849.40
  83   4.6678      0.970  16.7551     1.034  859.74
  84   4.6670      0.920  16.7566     1.008  870.07
  85   4.6673      0.930  16.7522     0.992  880.47
  86   4.6676      0.960  16.7601     1.038  890.81
  87   4.6665      0.880  16.7551     0.938  901.11
  88   4.6668      0.900  16.7622     1.058  911.49
  89   4.6663      0.910  16.7601     0.980  921.79
  90   4.6673      0.880  16.7584     1.012  932.11
