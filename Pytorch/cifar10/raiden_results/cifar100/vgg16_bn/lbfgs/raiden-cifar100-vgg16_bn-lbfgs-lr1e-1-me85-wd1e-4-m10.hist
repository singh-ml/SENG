Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17523058688 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6617      0.950  7.2681     0.974  17.14
   2   4.6614      0.910  7.2758     0.940  32.80
   3   4.6630      0.870  7.2669     1.006  48.33
   4   4.6618      0.900  7.2676     1.052  63.96
   5   4.6620      0.900  7.2711     1.038  79.46
   6   4.6617      0.910  7.2720     0.984  94.96
   7   4.6626      0.850  7.2772     0.982  110.57
   8   4.6609      0.890  7.2789     0.928  126.07
   9   4.6615      0.880  7.2714     0.982  141.62
  10   4.6616      0.960  7.2693     1.038  157.13
  11   4.6612      0.890  7.2732     0.978  172.76
  12   4.6625      0.880  7.2741     0.948  188.27
  13   4.6617      0.920  7.2687     1.004  203.79
  14   4.6617      0.900  7.2775     1.046  219.38
  15   4.6616      0.910  7.2728     0.996  234.89
  16   4.6628      0.850  7.2724     0.902  250.45
  17   4.6614      0.880  7.2733     1.004  265.94
  18   4.6611      0.920  7.2735     0.990  281.53
  19   4.6613      0.900  7.2765     1.080  297.04
  20   4.6615      0.920  7.2733     0.918  312.64
  21   4.6627      0.890  7.2738     1.084  328.13
  22   4.6613      0.930  7.2712     1.084  343.69
  23   4.6635      0.850  7.2692     0.978  359.21
  24   4.6616      0.910  7.2725     1.010  374.72
  25   4.6612      0.950  7.2747     1.008  390.33
  26   4.6613      0.970  7.2661     1.014  405.83
  27   4.6618      0.910  7.2711     1.004  421.44
  28   4.6625      0.880  7.2746     0.994  436.94
  29   4.6617      0.860  7.2759     0.952  452.42
  30   4.6621      0.870  7.2722     1.018  468.01
  31   4.6611      0.950  7.2793     0.986  483.53
  32   4.6612      0.860  7.2715     0.982  499.15
  33   4.6624      0.950  7.2780     0.954  514.65
  34   4.6618      0.870  7.2686     0.986  530.24
  35   4.6615      0.890  7.2725     0.978  545.77
  36   4.6625      0.950  7.2675     1.030  561.34
  37   4.6617      0.940  7.2743     0.908  576.86
  38   4.6615      0.940  7.2752     1.058  592.44
  39   4.6612      0.910  7.2728     0.996  607.99
  40   4.6619      0.900  7.2725     1.042  623.50
  41   4.6620      0.880  7.2694     0.902  639.13
  42   4.6614      0.930  7.2735     0.954  654.67
  43   4.6621      0.890  7.2725     0.892  670.23
  44   4.6612      0.940  7.2759     0.988  685.77
  45   4.6618      0.930  7.2683     0.972  701.36
  46   4.6614      0.890  7.2747     1.002  716.90
  47   4.6620      0.920  7.2787     0.974  732.44
  48   4.6618      0.890  7.2693     0.940  748.04
  49   4.6626      0.900  7.2777     0.938  763.54
  50   4.6620      0.900  7.2768     0.986  779.17
  51   4.6625      0.880  7.2751     0.986  794.64
  52   4.6627      0.910  7.2743     1.040  810.22
  53   4.6610      0.950  7.2690     1.072  825.71
  54   4.6626      0.930  7.2745     0.936  841.27
  55   4.6625      0.960  7.2726     0.962  856.77
  56   4.6624      0.890  7.2711     1.076  872.39
  57   4.6617      0.940  7.2695     0.996  887.91
  58   4.6609      0.980  7.2670     0.972  903.40
  59   4.6618      0.890  7.2715     0.990  919.01
  60   4.6622      0.880  7.2806     0.966  934.52
  61   4.6618      0.910  7.2713     1.032  950.12
  62   4.6622      0.960  7.2751     1.028  965.62
  63   4.6627      0.880  7.2734     1.030  981.19
  64   4.6625      0.890  7.2785     0.988  996.73
  65   4.6622      0.930  7.2705     0.988  1012.23
  66   4.6629      0.850  7.2685     1.088  1027.83
  67   4.6629      0.910  7.2742     1.002  1043.32
  68   4.6609      0.950  7.2691     1.030  1058.95
  69   4.6612      0.880  7.2710     1.008  1074.45
  70   4.6614      0.950  7.2770     0.954  1090.03
  71   4.6616      0.940  7.2815     0.960  1105.54
  72   4.6615      0.960  7.2731     0.910  1121.09
  73   4.6618      0.960  7.2710     1.044  1136.61
  74   4.6616      0.880  7.2762     0.958  1152.11
  75   4.6614      0.890  7.2718     1.068  1167.75
  76   4.6618      0.940  7.2703     0.978  1183.25
  77   4.6617      0.940  7.2747     0.930  1198.87
  78   4.6628      0.820  7.2738     1.014  1214.43
  79   4.6619      0.900  7.2667     1.086  1230.05
  80   4.6614      0.870  7.2670     0.932  1245.57
  81   4.6626      0.890  7.2637     1.002  1261.07
  82   4.6619      0.870  7.2702     1.048  1276.67
  83   4.6616      0.880  7.2745     0.964  1292.18
  84   4.6612      0.910  7.2680     1.034  1307.75
  85   4.6624      0.910  7.2731     1.020  1323.30
