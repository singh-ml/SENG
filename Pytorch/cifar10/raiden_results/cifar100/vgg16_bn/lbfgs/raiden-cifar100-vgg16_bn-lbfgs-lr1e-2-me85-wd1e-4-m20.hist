Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11058712576 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6733      1.000  7.2888     0.990  12.78
   2   4.6748      1.030  7.2867     0.930  24.02
   3   4.6746      1.010  7.2896     1.074  35.27
   4   4.6743      1.030  7.2903     0.912  46.52
   5   4.6733      0.970  7.2892     0.970  57.79
   6   4.6733      1.070  7.2891     0.906  69.05
   7   4.6742      1.040  7.2890     0.970  80.33
   8   4.6741      1.020  7.2906     0.982  91.59
   9   4.6747      1.010  7.2902     0.968  102.78
  10   4.6748      0.960  7.2843     1.028  114.07
  11   4.6747      1.010  7.2859     0.964  125.27
  12   4.6758      1.040  7.2843     1.044  136.55
  13   4.6737      1.000  7.2853     1.024  147.87
  14   4.6745      1.040  7.2907     1.050  159.06
  15   4.6731      1.050  7.2903     0.854  170.25
  16   4.6731      1.070  7.2924     0.976  181.61
  17   4.6752      1.000  7.2857     0.952  192.86
  18   4.6740      1.010  7.2844     0.978  204.11
  19   4.6754      0.960  7.2909     0.956  215.35
  20   4.6745      1.050  7.2873     0.956  226.57
  21   4.6754      0.980  7.2859     0.926  237.73
  22   4.6745      1.020  7.2830     1.002  248.96
  23   4.6736      1.050  7.2913     0.996  260.15
  24   4.6756      1.000  7.2896     1.088  271.40
  25   4.6752      1.050  7.2851     0.964  282.63
  26   4.6738      1.040  7.2906     0.946  293.96
  27   4.6757      1.020  7.2877     0.994  305.13
  28   4.6738      1.050  7.2915     1.006  316.41
  29   4.6747      1.010  7.2869     0.960  327.73
  30   4.6733      1.050  7.2872     0.954  338.98
  31   4.6741      0.990  7.2876     0.988  350.26
  32   4.6748      0.970  7.2872     0.920  361.51
  33   4.6743      1.070  7.2877     1.018  372.73
  34   4.6749      0.990  7.2836     0.982  383.99
  35   4.6742      1.020  7.2898     1.002  395.21
  36   4.6748      1.050  7.2838     1.028  406.36
  37   4.6748      1.020  7.2900     0.910  417.58
  38   4.6744      1.080  7.2892     0.956  428.92
  39   4.6755      1.060  7.2899     0.958  440.12
  40   4.6742      1.060  7.2900     0.902  451.31
  41   4.6735      0.990  7.2897     0.950  462.56
  42   4.6740      1.030  7.2909     0.970  473.90
  43   4.6752      0.980  7.2892     0.996  485.10
  44   4.6744      1.010  7.2864     0.996  496.30
  45   4.6741      1.000  7.2860     1.006  507.59
  46   4.6747      1.030  7.2868     0.976  518.75
  47   4.6745      1.000  7.2852     0.946  529.93
  48   4.6741      1.010  7.2874     0.956  541.16
  49   4.6743      1.030  7.2839     1.032  552.44
  50   4.6752      1.050  7.2864     0.998  563.64
  51   4.6747      1.000  7.2890     0.960  574.82
  52   4.6746      0.970  7.2857     0.984  586.09
  53   4.6755      1.030  7.2870     1.008  597.23
  54   4.6743      1.010  7.2876     1.012  608.40
  55   4.6744      1.010  7.2876     0.904  619.62
  56   4.6725      0.990  7.2876     1.050  630.84
  57   4.6740      1.040  7.2880     0.998  642.16
  58   4.6754      0.970  7.2896     0.958  653.42
  59   4.6745      1.040  7.2895     0.998  664.62
  60   4.6749      1.000  7.2837     0.986  675.90
  61   4.6744      1.010  7.2845     0.998  687.20
  62   4.6756      0.950  7.2933     0.986  698.39
  63   4.6747      1.040  7.2907     1.040  709.54
  64   4.6757      1.010  7.2871     1.058  720.87
  65   4.6742      1.040  7.2900     0.944  732.04
  66   4.6743      1.000  7.2876     0.994  743.29
  67   4.6758      0.990  7.2876     1.022  754.55
  68   4.6728      1.010  7.2863     0.978  765.75
  69   4.6746      0.990  7.2897     0.972  776.87
  70   4.6764      1.050  7.2811     0.994  788.11
  71   4.6736      1.000  7.2900     0.912  799.27
  72   4.6742      1.060  7.2845     1.038  810.44
  73   4.6745      1.030  7.2906     1.048  821.61
  74   4.6752      1.020  7.2840     0.984  832.93
  75   4.6747      1.050  7.2928     1.060  844.20
  76   4.6753      0.970  7.2901     0.942  855.45
  77   4.6743      1.020  7.2838     0.988  866.78
  78   4.6745      1.040  7.2886     0.948  878.01
  79   4.6749      0.990  7.2870     0.938  889.24
  80   4.6737      1.060  7.2930     0.922  900.43
  81   4.6734      1.030  7.2869     0.968  911.72
  82   4.6744      1.050  7.2909     0.948  922.94
  83   4.6753      0.980  7.2900     0.972  934.16
  84   4.6748      0.970  7.2902     0.980  945.43
  85   4.6748      1.010  7.2883     0.938  956.70
