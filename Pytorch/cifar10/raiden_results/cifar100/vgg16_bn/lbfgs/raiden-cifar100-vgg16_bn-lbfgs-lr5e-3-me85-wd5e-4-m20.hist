Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9979464704 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6731      0.830  16.7652     0.970  12.22
   2   4.6726      0.760  16.7612     1.012  23.05
   3   4.6738      0.810  16.7662     1.010  33.83
   4   4.6728      0.880  16.7608     0.942  44.64
   5   4.6727      0.790  16.7667     1.056  55.50
   6   4.6732      0.790  16.7597     0.986  66.21
   7   4.6727      0.800  16.7661     0.966  77.02
   8   4.6727      0.870  16.7712     0.982  87.82
   9   4.6728      0.770  16.7671     0.938  98.69
  10   4.6726      0.810  16.7696     0.950  109.40
  11   4.6727      0.760  16.7663     1.072  120.22
  12   4.6731      0.830  16.7618     0.978  131.08
  13   4.6724      0.820  16.7689     0.908  141.82
  14   4.6726      0.770  16.7686     1.016  152.57
  15   4.6723      0.780  16.7671     0.952  163.29
  16   4.6727      0.870  16.7645     1.020  174.16
  17   4.6732      0.860  16.7689     1.018  184.96
  18   4.6730      0.810  16.7679     0.966  195.71
  19   4.6731      0.830  16.7680     0.904  206.56
  20   4.6730      0.770  16.7666     0.998  217.34
  21   4.6725      0.800  16.7601     1.010  228.04
  22   4.6728      0.820  16.7666     1.012  238.87
  23   4.6726      0.900  16.7636     1.028  249.61
  24   4.6732      0.850  16.7700     0.962  260.34
  25   4.6731      0.800  16.7666     0.976  271.11
  26   4.6727      0.780  16.7641     0.992  282.04
  27   4.6722      0.810  16.7655     1.002  292.82
  28   4.6733      0.810  16.7695     0.958  303.55
  29   4.6729      0.800  16.7658     0.992  314.42
  30   4.6730      0.800  16.7625     1.010  325.21
  31   4.6742      0.820  16.7649     0.972  335.98
  32   4.6734      0.820  16.7594     1.012  346.79
  33   4.6731      0.800  16.7662     0.942  357.52
  34   4.6730      0.820  16.7690     0.934  368.28
  35   4.6723      0.780  16.7678     0.932  378.97
  36   4.6728      0.810  16.7633     0.906  389.83
  37   4.6730      0.810  16.7639     1.044  400.58
  38   4.6725      0.790  16.7714     0.944  411.34
  39   4.6728      0.820  16.7672     0.998  422.17
  40   4.6730      0.790  16.7649     0.998  432.99
  41   4.6734      0.810  16.7684     0.954  443.70
  42   4.6732      0.790  16.7625     0.912  454.51
  43   4.6726      0.780  16.7612     1.028  465.23
  44   4.6725      0.800  16.7641     0.986  475.99
  45   4.6729      0.740  16.7641     1.040  486.82
  46   4.6730      0.830  16.7677     0.912  497.57
  47   4.6727      0.800  16.7615     0.968  508.27
  48   4.6730      0.810  16.7661     0.982  519.17
  49   4.6723      0.820  16.7659     0.946  529.93
  50   4.6730      0.780  16.7639     1.016  540.68
  51   4.6736      0.840  16.7640     0.920  551.33
  52   4.6725      0.810  16.7647     1.040  562.17
  53   4.6727      0.820  16.7631     0.996  572.93
  54   4.6732      0.780  16.7647     0.992  583.71
  55   4.6727      0.790  16.7656     0.984  594.41
  56   4.6726      0.730  16.7682     1.020  605.24
  57   4.6723      0.790  16.7665     0.988  616.03
  58   4.6729      0.820  16.7657     0.922  626.79
  59   4.6718      0.830  16.7694     1.042  637.57
  60   4.6727      0.810  16.7644     1.006  648.28
  61   4.6730      0.860  16.7635     0.912  659.04
  62   4.6725      0.780  16.7694     0.932  669.81
  63   4.6733      0.830  16.7629     0.926  680.55
  64   4.6729      0.780  16.7627     1.034  691.32
  65   4.6724      0.780  16.7585     1.076  702.16
  66   4.6730      0.830  16.7681     0.992  712.90
  67   4.6734      0.830  16.7735     1.022  723.66
  68   4.6718      0.820  16.7637     1.004  734.37
  69   4.6730      0.840  16.7596     1.080  745.26
  70   4.6730      0.820  16.7652     1.024  755.95
  71   4.6730      0.820  16.7656     0.948  766.71
  72   4.6727      0.790  16.7613     1.004  777.50
  73   4.6727      0.890  16.7637     0.946  788.36
  74   4.6730      0.800  16.7637     1.022  799.16
  75   4.6725      0.850  16.7668     0.930  809.99
  76   4.6723      0.780  16.7642     1.036  820.88
  77   4.6730      0.790  16.7673     0.960  831.64
  78   4.6724      0.780  16.7660     1.036  842.40
  79   4.6737      0.800  16.7716     0.996  853.23
  80   4.6727      0.840  16.7619     1.048  864.02
  81   4.6728      0.890  16.7690     0.946  874.80
  82   4.6729      0.860  16.7690     1.008  885.65
  83   4.6724      0.850  16.7675     1.012  896.41
  84   4.6724      0.780  16.7659     1.020  907.17
  85   4.6726      0.820  16.7670     1.004  917.96
