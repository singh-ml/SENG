Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11056231424 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6794      0.840  9.6715     0.978  12.40
   2   4.6792      0.850  9.6690     1.084  23.14
   3   4.6792      0.760  9.6714     0.962  33.92
   4   4.6787      0.860  9.6693     0.970  44.66
   5   4.6807      0.860  9.6647     1.002  55.40
   6   4.6790      0.860  9.6645     1.004  66.17
   7   4.6793      0.870  9.6663     0.936  77.00
   8   4.6804      0.860  9.6704     0.940  87.74
   9   4.6805      0.870  9.6703     0.980  98.48
  10   4.6804      0.850  9.6658     1.014  109.32
  11   4.6787      0.880  9.6691     0.976  120.06
  12   4.6797      0.920  9.6693     0.954  130.80
  13   4.6790      0.830  9.6734     0.972  141.67
  14   4.6807      0.850  9.6701     0.930  152.43
  15   4.6784      0.840  9.6689     0.988  163.22
  16   4.6793      0.860  9.6644     1.052  174.07
  17   4.6792      0.840  9.6663     0.972  184.84
  18   4.6793      0.880  9.6679     1.038  195.58
  19   4.6780      0.830  9.6725     0.948  206.34
  20   4.6791      0.900  9.6666     1.042  217.18
  21   4.6795      0.840  9.6666     0.926  227.95
  22   4.6802      0.820  9.6633     0.962  238.71
  23   4.6787      0.870  9.6681     0.988  249.51
  24   4.6797      0.900  9.6637     0.974  260.25
  25   4.6801      0.830  9.6658     1.030  271.01
  26   4.6778      0.850  9.6658     0.946  281.83
  27   4.6789      0.870  9.6712     0.990  292.58
  28   4.6797      0.870  9.6662     0.948  303.33
  29   4.6790      0.880  9.6686     0.970  314.10
  30   4.6786      0.860  9.6720     0.950  324.95
  31   4.6791      0.880  9.6699     0.980  335.71
  32   4.6791      0.840  9.6702     1.014  346.44
  33   4.6799      0.860  9.6700     1.050  357.30
  34   4.6791      0.850  9.6703     1.062  368.05
  35   4.6781      0.890  9.6619     1.084  378.82
  36   4.6796      0.860  9.6666     0.956  389.66
  37   4.6782      0.870  9.6676     1.000  400.44
  38   4.6788      0.830  9.6673     0.976  411.22
  39   4.6791      0.850  9.6713     1.024  422.03
  40   4.6795      0.860  9.6697     0.970  432.77
  41   4.6793      0.890  9.6689     1.002  443.53
  42   4.6793      0.880  9.6647     1.014  454.30
  43   4.6796      0.860  9.6636     0.988  465.16
  44   4.6793      0.900  9.6666     1.026  475.90
  45   4.6793      0.870  9.6648     1.032  486.65
  46   4.6788      0.870  9.6663     1.066  497.49
  47   4.6790      0.870  9.6712     0.972  508.23
  48   4.6794      0.820  9.6730     0.970  519.02
  49   4.6793      0.850  9.6654     1.040  529.77
  50   4.6784      0.910  9.6639     0.970  540.61
  51   4.6783      0.890  9.6656     0.946  551.35
  52   4.6794      0.870  9.6684     0.996  562.11
  53   4.6799      0.830  9.6677     0.934  572.96
  54   4.6795      0.850  9.6706     0.944  583.68
  55   4.6796      0.850  9.6665     1.054  594.42
  56   4.6789      0.880  9.6620     0.932  605.26
  57   4.6795      0.870  9.6627     1.134  616.02
  58   4.6783      0.840  9.6686     0.956  626.79
  59   4.6791      0.900  9.6668     0.982  637.65
  60   4.6792      0.870  9.6696     1.012  648.38
  61   4.6797      0.870  9.6652     0.928  659.18
  62   4.6791      0.890  9.6713     0.984  669.95
  63   4.6782      0.890  9.6662     1.008  680.78
  64   4.6798      0.830  9.6698     0.922  691.55
  65   4.6791      0.900  9.6655     0.964  702.30
  66   4.6788      0.850  9.6695     0.946  713.13
  67   4.6789      0.800  9.6638     1.002  723.91
  68   4.6785      0.880  9.6694     0.950  734.67
  69   4.6789      0.850  9.6675     0.956  745.54
  70   4.6805      0.830  9.6657     1.000  756.31
  71   4.6787      0.860  9.6695     1.078  767.05
  72   4.6794      0.880  9.6630     0.924  777.80
  73   4.6792      0.900  9.6633     0.998  788.62
  74   4.6785      0.860  9.6663     1.044  799.37
  75   4.6799      0.820  9.6691     1.024  810.12
  76   4.6794      0.860  9.6671     1.032  820.95
  77   4.6791      0.810  9.6660     1.032  831.74
  78   4.6786      0.840  9.6662     0.906  842.52
  79   4.6790      0.880  9.6672     0.966  853.33
  80   4.6783      0.850  9.6666     1.012  864.06
  81   4.6787      0.840  9.6678     0.944  874.83
  82   4.6791      0.860  9.6717     1.034  885.58
  83   4.6790      0.870  9.6708     1.012  896.41
  84   4.6801      0.870  9.6655     0.980  907.20
  85   4.6792      0.880  9.6721     1.024  917.94
