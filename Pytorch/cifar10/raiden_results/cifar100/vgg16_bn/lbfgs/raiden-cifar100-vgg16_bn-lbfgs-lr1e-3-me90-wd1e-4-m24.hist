Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6690      1.050  7.2917     1.002  11.93
   2   4.6688      1.030  7.2927     0.982  22.24
   3   4.6683      1.060  7.2876     1.004  32.49
   4   4.6692      1.050  7.3006     0.996  42.72
   5   4.6680      1.010  7.2894     1.090  53.04
   6   4.6692      1.150  7.2877     1.086  63.21
   7   4.6687      1.050  7.2949     0.968  73.47
   8   4.6680      1.100  7.2914     0.962  83.71
   9   4.6689      1.080  7.2902     1.008  94.09
  10   4.6686      1.020  7.2870     0.982  104.35
  11   4.6692      0.980  7.2906     1.044  114.66
  12   4.6683      1.040  7.2929     0.972  125.02
  13   4.6685      1.030  7.2905     0.910  135.26
  14   4.6684      1.040  7.2869     0.966  145.52
  15   4.6686      1.010  7.2951     0.980  155.74
  16   4.6683      1.000  7.2905     0.924  166.06
  17   4.6680      1.070  7.2887     1.062  176.27
  18   4.6693      1.070  7.2936     1.028  186.52
  19   4.6677      1.040  7.2926     1.028  196.75
  20   4.6686      1.120  7.2927     0.956  206.99
  21   4.6686      1.050  7.2919     0.944  217.26
  22   4.6687      1.070  7.2925     0.988  227.56
  23   4.6693      1.060  7.2944     1.004  237.92
  24   4.6689      1.020  7.2914     1.098  248.12
  25   4.6689      1.020  7.2898     1.032  258.33
  26   4.6695      1.070  7.2924     0.982  268.67
  27   4.6689      1.120  7.2934     0.984  278.96
  28   4.6679      1.080  7.2874     0.998  289.17
  29   4.6682      1.050  7.2876     0.988  299.40
  30   4.6683      1.030  7.2873     1.002  309.70
  31   4.6680      1.050  7.2877     1.038  319.96
  32   4.6684      1.090  7.2910     0.986  330.22
  33   4.6693      1.020  7.2880     1.018  340.44
  34   4.6691      1.050  7.2929     1.004  350.77
  35   4.6686      1.000  7.2954     0.998  361.03
  36   4.6686      1.010  7.2863     1.050  371.25
  37   4.6683      1.080  7.2953     1.002  381.49
  38   4.6678      1.050  7.2896     1.030  391.70
  39   4.6682      1.080  7.2881     0.940  401.98
  40   4.6688      1.100  7.2912     0.982  412.14
  41   4.6686      1.070  7.2906     1.108  422.53
  42   4.6684      1.060  7.2929     1.004  432.76
  43   4.6681      1.080  7.2848     1.008  443.03
  44   4.6688      1.030  7.2854     0.986  453.37
  45   4.6684      1.070  7.2913     1.014  463.64
  46   4.6684      1.080  7.2920     0.954  473.88
  47   4.6688      1.060  7.2932     0.996  484.18
  48   4.6683      1.060  7.2923     1.008  494.42
  49   4.6680      1.030  7.2937     1.018  504.63
  50   4.6684      1.010  7.2902     1.040  514.92
  51   4.6681      1.010  7.2893     1.010  525.22
  52   4.6680      1.030  7.2907     1.042  535.47
  53   4.6690      1.060  7.2895     0.936  545.73
  54   4.6690      0.990  7.2917     0.922  555.94
  55   4.6682      1.040  7.2935     1.014  566.31
  56   4.6684      1.010  7.2910     1.078  576.57
  57   4.6687      1.050  7.2877     0.978  586.80
  58   4.6684      1.000  7.2916     0.984  597.13
  59   4.6682      0.980  7.2927     0.984  607.34
  60   4.6681      1.070  7.2886     1.010  617.60
  61   4.6689      1.080  7.2965     1.002  627.87
  62   4.6692      1.130  7.2935     0.948  638.20
  63   4.6690      1.120  7.2932     1.050  648.41
  64   4.6682      1.030  7.2941     1.044  658.67
  65   4.6682      1.080  7.2878     1.008  668.98
  66   4.6696      1.100  7.2891     1.022  679.23
  67   4.6695      1.070  7.2922     0.896  689.42
  68   4.6688      1.100  7.2854     1.028  699.65
  69   4.6691      1.100  7.2931     0.996  709.99
  70   4.6684      1.050  7.2945     0.972  720.25
  71   4.6687      1.050  7.2869     1.010  730.53
  72   4.6680      1.060  7.2925     0.996  740.89
  73   4.6691      1.050  7.2877     1.068  751.15
  74   4.6697      1.070  7.2929     0.968  761.43
  75   4.6681      1.060  7.2888     1.014  771.73
  76   4.6689      1.050  7.2906     1.058  782.08
  77   4.6690      1.090  7.2914     1.036  792.35
  78   4.6685      1.010  7.2931     0.994  802.56
  79   4.6688      1.090  7.2971     0.990  812.84
  80   4.6687      1.110  7.2895     1.044  823.09
  81   4.6683      1.050  7.2863     1.036  833.31
  82   4.6689      1.050  7.2886     1.030  843.52
  83   4.6692      0.990  7.2886     1.036  853.86
  84   4.6683      1.100  7.2879     0.980  864.09
  85   4.6685      1.130  7.2957     0.972  874.33
  86   4.6684      1.030  7.2951     1.012  884.66
  87   4.6685      1.030  7.2943     0.994  894.94
  88   4.6686      1.060  7.2909     1.024  905.15
  89   4.6688      1.060  7.2905     0.940  915.35
  90   4.6678      1.040  7.2922     0.994  925.72
