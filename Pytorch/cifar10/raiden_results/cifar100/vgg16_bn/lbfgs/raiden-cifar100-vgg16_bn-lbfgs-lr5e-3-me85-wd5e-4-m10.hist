Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11055182848 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6833      1.050  16.7807     0.988  12.57
   2   4.6838      1.050  16.7812     1.052  23.61
   3   4.6839      1.060  16.7728     0.988  34.63
   4   4.6829      1.090  16.7817     0.958  45.61
   5   4.6840      1.060  16.7791     0.998  56.57
   6   4.6842      1.070  16.7778     1.024  67.67
   7   4.6833      1.050  16.7776     1.046  78.65
   8   4.6840      1.090  16.7851     1.066  89.62
   9   4.6831      1.040  16.7878     0.944  100.65
  10   4.6839      1.070  16.7808     1.058  111.64
  11   4.6824      1.070  16.7792     1.114  122.58
  12   4.6829      1.060  16.7865     0.960  133.65
  13   4.6842      1.060  16.7783     1.034  144.64
  14   4.6839      1.070  16.7777     1.090  155.61
  15   4.6836      1.060  16.7783     0.926  166.69
  16   4.6839      1.070  16.7768     1.014  177.69
  17   4.6849      1.070  16.7801     1.090  188.66
  18   4.6837      1.070  16.7775     1.042  199.64
  19   4.6826      1.090  16.7785     0.982  210.70
  20   4.6835      1.050  16.7774     1.020  221.72
  21   4.6829      1.050  16.7782     0.968  232.70
  22   4.6829      1.040  16.7777     0.984  243.82
  23   4.6829      1.070  16.7776     0.980  254.77
  24   4.6841      1.070  16.7824     1.014  265.75
  25   4.6819      1.060  16.7818     0.980  276.87
  26   4.6829      1.050  16.7821     1.004  287.90
  27   4.6832      1.070  16.7776     0.986  298.87
  28   4.6837      1.090  16.7809     0.952  309.97
  29   4.6837      1.080  16.7801     0.990  320.93
  30   4.6834      1.100  16.7820     1.010  331.90
  31   4.6853      1.060  16.7819     0.976  342.92
  32   4.6842      1.070  16.7856     0.948  353.97
  33   4.6847      1.090  16.7783     1.030  364.96
  34   4.6836      1.060  16.7784     1.008  376.01
  35   4.6832      1.100  16.7823     1.012  387.04
  36   4.6837      1.080  16.7814     1.020  398.02
  37   4.6823      1.060  16.7783     1.014  408.96
  38   4.6835      1.080  16.7870     0.960  419.92
  39   4.6838      1.110  16.7783     0.944  430.88
  40   4.6844      1.070  16.7790     1.058  441.81
  41   4.6838      1.050  16.7862     1.008  452.75
  42   4.6819      1.060  16.7811     1.010  463.81
  43   4.6847      1.110  16.7786     1.044  474.82
  44   4.6835      1.050  16.7823     0.976  485.78
  45   4.6833      1.080  16.7782     1.098  496.81
  46   4.6824      1.060  16.7775     1.026  507.78
  47   4.6840      1.060  16.7795     0.912  518.76
  48   4.6848      1.060  16.7781     1.062  529.86
  49   4.6836      1.070  16.7774     1.000  540.91
  50   4.6851      1.060  16.7811     0.962  551.90
  51   4.6836      1.070  16.7836     0.996  562.86
  52   4.6833      1.060  16.7801     0.932  573.93
  53   4.6844      1.060  16.7804     0.978  584.95
  54   4.6819      1.060  16.7820     0.972  595.94
  55   4.6839      1.070  16.7836     1.000  607.02
  56   4.6834      1.030  16.7728     1.052  618.00
  57   4.6834      1.040  16.7735     1.032  628.97
  58   4.6829      1.090  16.7813     0.906  640.03
  59   4.6839      1.080  16.7817     0.978  651.00
  60   4.6828      1.050  16.7769     1.112  661.96
  61   4.6831      1.090  16.7735     1.056  673.01
  62   4.6842      1.080  16.7788     1.044  684.07
  63   4.6840      1.020  16.7818     0.966  695.07
  64   4.6829      1.060  16.7799     1.008  706.00
  65   4.6824      1.080  16.7776     0.972  717.13
  66   4.6842      1.050  16.7819     0.940  728.11
  67   4.6833      1.070  16.7812     1.024  739.14
  68   4.6827      1.060  16.7813     0.970  750.18
  69   4.6824      1.060  16.7773     1.056  761.07
  70   4.6845      1.080  16.7777     1.008  772.02
  71   4.6848      1.060  16.7854     0.950  783.06
  72   4.6839      1.060  16.7771     1.108  794.02
  73   4.6830      1.050  16.7784     1.050  805.02
  74   4.6844      1.080  16.7749     1.028  815.96
  75   4.6843      1.060  16.7793     1.026  827.05
  76   4.6850      1.020  16.7765     0.998  838.07
  77   4.6845      1.050  16.7823     1.060  849.06
  78   4.6822      1.050  16.7839     1.068  860.12
  79   4.6827      1.050  16.7753     1.034  871.10
  80   4.6831      1.090  16.7811     1.002  882.01
  81   4.6836      1.050  16.7847     0.952  893.07
  82   4.6826      1.040  16.7793     0.892  904.02
  83   4.6825      1.050  16.7741     0.994  915.05
  84   4.6838      1.050  16.7824     1.016  926.05
  85   4.6822      1.080  16.7797     0.960  937.12
