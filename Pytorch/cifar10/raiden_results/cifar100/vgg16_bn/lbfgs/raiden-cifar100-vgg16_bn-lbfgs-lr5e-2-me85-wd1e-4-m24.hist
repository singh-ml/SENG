Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 22916012032 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6570      0.940  7.2806     1.132  21.55
   2   4.6567      0.890  7.2813     0.976  41.93
   3   4.6565      0.880  7.2801     1.028  62.42
   4   4.6555      0.920  7.2801     0.956  82.82
   5   4.6568      0.890  7.2824     0.944  103.29
   6   4.6563      0.910  7.2751     1.064  123.71
   7   4.6573      0.900  7.2824     0.944  144.19
   8   4.6569      0.840  7.2801     0.990  164.58
   9   4.6565      0.910  7.2765     1.108  185.07
  10   4.6569      0.890  7.2783     0.998  205.53
  11   4.6571      0.990  7.2775     0.996  225.93
  12   4.6562      0.880  7.2826     0.960  246.39
  13   4.6562      0.930  7.2837     1.038  266.83
  14   4.6561      0.890  7.2838     1.028  287.30
  15   4.6561      0.890  7.2762     1.044  307.70
  16   4.6572      0.860  7.2815     1.004  328.16
  17   4.6564      0.910  7.2804     0.978  348.58
  18   4.6555      0.920  7.2803     1.046  369.06
  19   4.6565      0.900  7.2857     0.918  389.47
  20   4.6567      0.910  7.2833     1.002  409.92
  21   4.6564      0.990  7.2770     0.946  430.40
  22   4.6566      0.880  7.2806     0.952  450.81
  23   4.6564      0.920  7.2795     0.986  471.29
  24   4.6557      0.930  7.2770     0.996  491.72
  25   4.6572      0.900  7.2841     0.940  512.19
  26   4.6573      0.860  7.2816     1.068  532.59
  27   4.6566      0.930  7.2794     0.922  553.07
  28   4.6565      0.920  7.2819     1.002  573.51
  29   4.6568      0.950  7.2770     1.060  593.92
  30   4.6570      0.940  7.2806     0.990  614.41
  31   4.6562      0.920  7.2820     0.986  634.81
  32   4.6564      0.900  7.2802     1.022  655.30
  33   4.6561      0.940  7.2804     1.018  675.70
  34   4.6560      0.900  7.2783     1.018  696.18
  35   4.6568      0.880  7.2798     1.072  716.56
  36   4.6564      0.900  7.2762     0.982  737.03
  37   4.6567      0.890  7.2772     1.052  757.44
  38   4.6562      0.940  7.2787     0.948  777.88
  39   4.6559      0.920  7.2845     0.988  798.29
  40   4.6568      0.940  7.2872     1.030  818.78
  41   4.6570      0.870  7.2802     1.022  839.26
  42   4.6563      0.920  7.2796     0.936  859.68
  43   4.6561      0.890  7.2811     0.976  880.16
  44   4.6563      0.950  7.2804     0.978  900.57
  45   4.6564      0.900  7.2816     0.902  921.06
  46   4.6552      0.890  7.2824     1.068  941.47
  47   4.6572      0.870  7.2803     1.014  961.93
  48   4.6562      0.930  7.2868     0.928  982.34
  49   4.6570      0.920  7.2836     0.942  1002.81
  50   4.6569      0.880  7.2805     1.032  1023.29
  51   4.6563      0.870  7.2831     1.030  1043.71
  52   4.6567      0.930  7.2811     0.996  1064.19
  53   4.6568      0.900  7.2804     1.022  1084.60
  54   4.6567      0.890  7.2831     1.020  1105.08
  55   4.6566      0.910  7.2795     0.958  1125.49
  56   4.6563      0.880  7.2742     1.028  1145.95
  57   4.6572      0.930  7.2805     1.048  1166.35
  58   4.6565      0.890  7.2765     1.028  1186.82
  59   4.6574      0.920  7.2802     0.986  1207.30
  60   4.6566      0.860  7.2789     1.006  1227.72
  61   4.6565      0.910  7.2769     0.970  1248.19
  62   4.6556      0.920  7.2825     0.962  1268.58
  63   4.6563      0.860  7.2793     1.012  1289.02
  64   4.6557      0.890  7.2808     0.994  1309.45
  65   4.6564      0.840  7.2767     0.946  1329.91
  66   4.6558      0.910  7.2821     0.968  1350.39
  67   4.6570      0.900  7.2785     1.058  1370.80
  68   4.6566      0.880  7.2782     0.970  1391.27
  69   4.6565      0.950  7.2842     1.010  1411.67
  70   4.6561      0.920  7.2744     1.028  1432.12
  71   4.6568      0.910  7.2840     0.930  1452.54
  72   4.6569      0.900  7.2817     1.114  1473.02
  73   4.6566      0.890  7.2761     1.064  1493.44
  74   4.6561      0.920  7.2824     1.030  1513.91
  75   4.6570      0.880  7.2839     0.874  1534.33
  76   4.6566      0.860  7.2800     1.006  1554.80
  77   4.6562      0.920  7.2781     0.916  1575.28
  78   4.6570      0.920  7.2878     0.932  1595.68
  79   4.6566      0.930  7.2791     0.978  1616.17
  80   4.6561      0.920  7.2796     1.010  1636.58
  81   4.6560      0.880  7.2815     0.996  1657.04
  82   4.6570      0.910  7.2804     0.932  1677.46
  83   4.6571      0.900  7.2801     1.016  1697.93
  84   4.6566      0.910  7.2791     0.982  1718.41
  85   4.6561      0.940  7.2832     0.968  1738.81
