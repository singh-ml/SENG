Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 19681161216 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6807      1.030  16.7632     1.084  19.36
   2   4.6806      1.000  16.7690     0.998  37.36
   3   4.6806      1.050  16.7678     1.020  55.43
   4   4.6806      1.030  16.7716     0.930  73.42
   5   4.6802      1.040  16.7755     1.008  91.50
   6   4.6796      1.020  16.7731     1.050  109.50
   7   4.6806      1.000  16.7700     1.024  127.58
   8   4.6812      1.020  16.7685     1.056  145.57
   9   4.6797      1.020  16.7653     1.076  163.61
  10   4.6807      0.980  16.7725     1.008  181.57
  11   4.6801      1.020  16.7658     0.998  199.63
  12   4.6807      0.990  16.7687     1.066  217.62
  13   4.6809      1.110  16.7674     0.972  235.68
  14   4.6794      1.040  16.7673     0.994  253.68
  15   4.6808      0.990  16.7738     1.054  271.73
  16   4.6796      1.090  16.7691     0.968  289.72
  17   4.6809      1.070  16.7692     0.978  307.79
  18   4.6799      1.070  16.7664     0.970  325.80
  19   4.6803      1.010  16.7698     1.054  343.85
  20   4.6817      1.030  16.7649     1.066  361.84
  21   4.6801      1.010  16.7637     1.034  379.84
  22   4.6808      1.090  16.7679     1.016  397.86
  23   4.6811      1.010  16.7657     1.074  415.84
  24   4.6805      1.070  16.7661     0.958  433.91
  25   4.6801      1.100  16.7713     1.046  451.88
  26   4.6800      1.020  16.7720     1.042  469.93
  27   4.6806      1.030  16.7659     1.030  487.91
  28   4.6793      1.040  16.7715     0.944  505.96
  29   4.6805      1.020  16.7664     1.098  523.96
  30   4.6801      1.070  16.7667     1.062  542.00
  31   4.6807      1.050  16.7707     1.060  559.99
  32   4.6811      1.050  16.7752     1.008  578.07
  33   4.6804      1.010  16.7687     0.946  596.08
  34   4.6803      1.030  16.7665     0.994  614.15
  35   4.6800      1.100  16.7718     0.956  632.13
  36   4.6796      1.070  16.7672     1.018  650.20
  37   4.6807      1.030  16.7706     1.022  668.19
  38   4.6808      0.970  16.7709     1.008  686.23
  39   4.6806      1.080  16.7714     1.022  704.22
  40   4.6803      1.030  16.7760     0.966  722.28
  41   4.6807      1.050  16.7703     1.012  740.29
  42   4.6811      1.030  16.7699     0.942  758.33
  43   4.6797      1.030  16.7660     0.950  776.32
  44   4.6800      1.050  16.7660     1.024  794.41
  45   4.6804      0.990  16.7656     1.020  812.39
  46   4.6808      0.970  16.7646     0.970  830.46
  47   4.6810      1.050  16.7647     1.098  848.44
  48   4.6815      1.120  16.7693     0.980  866.50
  49   4.6803      1.070  16.7706     0.992  884.52
  50   4.6806      1.110  16.7701     0.966  902.58
  51   4.6802      0.970  16.7698     0.980  920.56
  52   4.6809      1.000  16.7689     1.050  938.59
  53   4.6811      1.030  16.7659     1.032  956.58
  54   4.6805      0.970  16.7665     1.026  974.63
  55   4.6811      1.020  16.7751     1.036  992.63
  56   4.6808      1.040  16.7719     1.058  1010.69
  57   4.6801      1.010  16.7708     1.024  1028.68
  58   4.6800      1.050  16.7687     1.028  1046.76
  59   4.6817      1.060  16.7651     1.028  1064.75
  60   4.6804      1.060  16.7707     1.066  1082.81
  61   4.6808      1.010  16.7690     1.050  1100.81
  62   4.6806      0.990  16.7715     0.992  1118.92
  63   4.6808      1.080  16.7661     1.106  1136.93
  64   4.6803      1.020  16.7702     0.990  1154.99
  65   4.6808      1.070  16.7703     1.030  1173.02
  66   4.6808      1.020  16.7714     1.014  1191.10
  67   4.6808      0.990  16.7710     1.028  1209.10
  68   4.6803      1.040  16.7667     1.004  1227.10
  69   4.6802      1.040  16.7711     0.992  1245.15
  70   4.6804      0.970  16.7680     0.988  1263.16
  71   4.6801      1.020  16.7674     1.088  1281.28
  72   4.6808      0.990  16.7680     1.032  1299.29
  73   4.6803      1.040  16.7745     0.968  1317.38
  74   4.6814      1.010  16.7712     0.974  1335.47
  75   4.6803      0.960  16.7658     1.078  1353.44
  76   4.6818      1.030  16.7664     0.966  1371.45
  77   4.6801      1.000  16.7686     1.080  1389.48
  78   4.6811      0.980  16.7693     1.032  1407.48
  79   4.6802      1.100  16.7683     1.072  1425.53
  80   4.6804      0.950  16.7751     0.944  1443.53
  81   4.6811      0.970  16.7680     0.972  1461.57
  82   4.6805      1.060  16.7662     1.034  1479.56
  83   4.6807      1.040  16.7705     1.014  1497.61
  84   4.6804      0.990  16.7643     1.016  1515.61
  85   4.6811      1.030  16.7635     0.942  1533.69
