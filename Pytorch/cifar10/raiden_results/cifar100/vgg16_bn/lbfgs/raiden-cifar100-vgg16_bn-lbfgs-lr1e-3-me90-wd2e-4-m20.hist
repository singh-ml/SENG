Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6871      0.820  9.6774     1.016  11.95
   2   4.6869      0.900  9.6786     1.018  22.34
   3   4.6855      0.820  9.6754     0.962  32.59
   4   4.6868      0.870  9.6807     0.978  42.89
   5   4.6866      0.830  9.6740     1.018  53.21
   6   4.6866      0.860  9.6854     0.952  63.57
   7   4.6850      0.880  9.6840     1.028  73.83
   8   4.6863      0.840  9.6766     1.024  84.15
   9   4.6878      0.790  9.6824     0.994  94.54
  10   4.6861      0.850  9.6772     1.010  104.85
  11   4.6878      0.820  9.6810     0.934  115.14
  12   4.6864      0.900  9.6809     1.010  125.53
  13   4.6857      0.810  9.6838     1.026  135.80
  14   4.6849      0.880  9.6817     0.994  146.14
  15   4.6868      0.870  9.6825     1.046  156.41
  16   4.6874      0.870  9.6755     1.004  166.44
  17   4.6872      0.870  9.6765     1.024  176.39
  18   4.6855      0.830  9.6784     0.952  186.36
  19   4.6862      0.860  9.6775     1.030  196.35
  20   4.6860      0.790  9.6782     1.036  206.40
  21   4.6864      0.900  9.6841     0.922  216.39
  22   4.6868      0.810  9.6731     1.046  226.35
  23   4.6876      0.900  9.6750     0.992  236.30
  24   4.6870      0.850  9.6780     1.002  246.31
  25   4.6864      0.870  9.6780     1.010  256.29
  26   4.6849      0.790  9.6811     0.974  266.24
  27   4.6855      0.870  9.6800     0.972  276.20
  28   4.6863      0.850  9.6757     0.982  286.14
  29   4.6852      0.850  9.6778     0.978  296.11
  30   4.6855      0.830  9.6786     0.966  306.05
  31   4.6863      0.860  9.6843     0.964  316.09
  32   4.6863      0.860  9.6829     0.936  326.02
  33   4.6853      0.840  9.6803     1.036  335.98
  34   4.6859      0.860  9.6731     0.994  345.96
  35   4.6871      0.800  9.6761     1.012  356.02
  36   4.6863      0.810  9.6733     1.052  365.98
  37   4.6868      0.810  9.6752     1.042  375.92
  38   4.6858      0.910  9.6792     1.006  385.88
  39   4.6845      0.850  9.6736     0.998  395.83
  40   4.6851      0.840  9.6814     0.940  405.79
  41   4.6866      0.900  9.6741     1.000  415.73
  42   4.6862      0.870  9.6824     1.044  425.79
  43   4.6847      0.870  9.6812     0.944  435.75
  44   4.6857      0.770  9.6808     0.940  445.72
  45   4.6850      0.850  9.6827     1.048  455.67
  46   4.6855      0.810  9.6834     0.962  465.72
  47   4.6861      0.840  9.6775     0.982  475.71
  48   4.6858      0.830  9.6760     0.962  485.69
  49   4.6860      0.810  9.6788     1.084  495.63
  50   4.6856      0.790  9.6823     0.970  505.67
  51   4.6862      0.830  9.6847     0.970  515.65
  52   4.6868      0.790  9.6764     0.956  525.63
  53   4.6868      0.860  9.6781     0.984  535.67
  54   4.6858      0.830  9.6832     1.050  545.62
  55   4.6876      0.870  9.6750     0.976  555.57
  56   4.6862      0.820  9.6778     1.008  565.53
  57   4.6852      0.810  9.6799     0.996  575.61
  58   4.6862      0.840  9.6765     1.030  585.60
  59   4.6865      0.860  9.6822     1.038  595.54
  60   4.6854      0.810  9.6763     0.944  605.56
  61   4.6862      0.840  9.6783     1.060  615.50
  62   4.6876      0.860  9.6755     0.988  625.49
  63   4.6867      0.870  9.6804     0.952  635.43
  64   4.6860      0.820  9.6812     0.996  645.47
  65   4.6861      0.790  9.6774     1.022  655.42
  66   4.6879      0.860  9.6827     0.990  665.37
  67   4.6859      0.860  9.6799     0.970  675.35
  68   4.6854      0.790  9.6872     1.046  685.42
  69   4.6865      0.850  9.6802     1.082  695.39
  70   4.6855      0.910  9.6802     0.914  705.37
  71   4.6861      0.860  9.6815     1.026  715.36
  72   4.6874      0.810  9.6825     1.050  725.43
  73   4.6863      0.820  9.6733     1.044  735.40
  74   4.6853      0.850  9.6737     1.004  745.36
  75   4.6867      0.780  9.6830     1.036  755.44
  76   4.6847      0.890  9.6830     1.040  765.42
  77   4.6857      0.840  9.6790     1.010  775.41
  78   4.6858      0.860  9.6726     1.046  785.39
  79   4.6872      0.880  9.6761     0.996  795.37
  80   4.6860      0.830  9.6813     0.964  805.35
  81   4.6884      0.880  9.6782     1.020  815.31
  82   4.6868      0.860  9.6843     0.938  825.34
  83   4.6867      0.790  9.6793     1.036  835.32
  84   4.6862      0.840  9.6776     0.988  845.29
  85   4.6868      0.800  9.6816     1.032  855.35
  86   4.6847      0.850  9.6829     0.990  865.35
  87   4.6853      0.890  9.6792     1.016  875.31
  88   4.6875      0.850  9.6818     0.896  885.26
  89   4.6874      0.880  9.6757     0.980  895.29
  90   4.6868      0.880  9.6849     1.000  905.28
