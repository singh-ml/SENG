Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17523451904 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6792      1.000  16.7552     0.978  17.21
   2   4.6785      0.950  16.7622     1.036  32.90
   3   4.6781      1.010  16.7604     1.014  48.66
   4   4.6790      0.970  16.7626     1.010  64.34
   5   4.6780      1.010  16.7643     1.012  79.99
   6   4.6782      0.990  16.7574     0.982  95.82
   7   4.6784      0.980  16.7614     0.978  111.53
   8   4.6784      1.000  16.7616     1.042  127.31
   9   4.6780      0.990  16.7619     1.044  142.99
  10   4.6786      0.980  16.7585     0.970  158.72
  11   4.6781      0.990  16.7649     1.008  174.36
  12   4.6791      1.030  16.7647     0.922  190.04
  13   4.6785      0.970  16.7591     1.044  205.81
  14   4.6785      0.940  16.7595     0.970  221.43
  15   4.6795      1.000  16.7644     0.946  237.21
  16   4.6786      0.950  16.7584     1.086  252.87
  17   4.6792      1.030  16.7609     1.020  268.57
  18   4.6776      0.960  16.7672     0.968  284.28
  19   4.6788      1.000  16.7595     0.972  299.97
  20   4.6790      1.030  16.7630     0.966  315.75
  21   4.6782      0.980  16.7620     0.968  331.42
  22   4.6788      1.000  16.7602     1.038  347.16
  23   4.6795      1.000  16.7600     1.032  362.84
  24   4.6786      0.990  16.7616     0.990  378.60
  25   4.6794      0.970  16.7557     1.012  394.22
  26   4.6787      1.010  16.7634     1.000  409.82
  27   4.6789      0.970  16.7567     0.960  425.48
  28   4.6789      1.000  16.7601     1.038  441.08
  29   4.6798      1.020  16.7602     1.024  456.76
  30   4.6787      1.020  16.7625     1.054  472.38
  31   4.6793      0.990  16.7622     0.950  488.03
  32   4.6787      1.010  16.7587     1.030  503.63
  33   4.6779      0.980  16.7558     1.014  519.21
  34   4.6779      0.990  16.7558     1.004  534.89
  35   4.6787      1.010  16.7607     0.976  550.49
  36   4.6786      0.950  16.7593     1.034  566.12
  37   4.6787      0.990  16.7640     0.950  581.72
  38   4.6786      0.970  16.7584     1.070  597.39
  39   4.6786      0.980  16.7680     0.998  612.98
  40   4.6780      0.990  16.7634     0.982  628.57
  41   4.6782      0.940  16.7586     0.956  644.25
  42   4.6780      0.980  16.7594     1.054  659.85
  43   4.6779      1.010  16.7655     1.006  675.55
  44   4.6781      0.980  16.7633     0.970  691.14
  45   4.6785      0.990  16.7673     1.002  706.80
  46   4.6796      1.020  16.7590     1.036  722.42
  47   4.6789      0.970  16.7594     1.054  738.01
  48   4.6781      0.950  16.7600     1.054  753.65
  49   4.6787      0.970  16.7545     0.992  769.25
  50   4.6787      0.970  16.7606     0.956  784.95
  51   4.6784      0.980  16.7632     0.954  800.57
  52   4.6784      0.980  16.7561     1.040  816.25
  53   4.6786      0.980  16.7643     0.976  831.85
  54   4.6788      0.970  16.7599     1.072  847.47
  55   4.6773      1.010  16.7625     1.028  863.17
  56   4.6784      1.010  16.7541     1.062  878.79
  57   4.6782      0.980  16.7591     0.968  894.43
  58   4.6784      0.980  16.7641     1.062  910.04
  59   4.6788      0.970  16.7583     0.984  925.69
  60   4.6794      0.980  16.7594     1.008  941.29
  61   4.6781      0.970  16.7592     1.088  956.89
  62   4.6790      0.940  16.7582     1.022  972.56
  63   4.6780      1.000  16.7607     1.042  988.17
  64   4.6785      0.960  16.7621     0.958  1003.83
  65   4.6787      0.980  16.7622     1.062  1019.46
  66   4.6785      1.000  16.7705     1.014  1035.15
  67   4.6792      0.980  16.7575     0.998  1050.77
  68   4.6792      0.990  16.7602     0.896  1066.43
  69   4.6793      1.020  16.7618     1.018  1082.05
  70   4.6790      0.990  16.7593     0.962  1097.66
  71   4.6796      0.970  16.7576     1.054  1113.31
  72   4.6779      0.990  16.7613     1.082  1128.90
  73   4.6784      0.990  16.7586     0.968  1144.58
  74   4.6786      0.930  16.7646     1.032  1160.19
  75   4.6787      1.010  16.7644     0.992  1175.77
  76   4.6793      0.980  16.7637     0.980  1191.50
  77   4.6796      1.010  16.7640     0.992  1207.09
  78   4.6784      0.990  16.7612     1.002  1222.76
  79   4.6788      1.010  16.7658     0.936  1238.37
  80   4.6783      0.940  16.7607     1.042  1254.07
  81   4.6791      0.990  16.7621     1.008  1269.67
  82   4.6785      0.950  16.7544     1.004  1285.26
  83   4.6781      0.940  16.7617     0.934  1300.92
  84   4.6787      0.960  16.7605     1.022  1316.53
  85   4.6777      0.980  16.7597     1.006  1332.22
  86   4.6799      1.010  16.7561     1.098  1347.82
  87   4.6788      1.020  16.7638     0.978  1363.44
  88   4.6787      0.990  16.7583     0.976  1379.04
  89   4.6786      0.970  16.7645     0.966  1394.64
  90   4.6788      0.960  16.7628     1.052  1410.31
