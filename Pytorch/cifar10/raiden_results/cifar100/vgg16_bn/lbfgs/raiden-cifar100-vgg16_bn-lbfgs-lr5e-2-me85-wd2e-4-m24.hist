Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17526337024 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6707      1.020  9.6603     1.044  17.52
   2   4.6705      1.000  9.6628     1.002  33.89
   3   4.6697      1.020  9.6591     1.066  50.34
   4   4.6707      1.020  9.6645     0.958  66.71
   5   4.6697      0.970  9.6642     1.052  83.14
   6   4.6710      1.020  9.6617     0.994  99.56
   7   4.6700      1.050  9.6579     0.922  116.00
   8   4.6709      1.000  9.6652     1.024  132.37
   9   4.6704      1.040  9.6530     0.974  148.81
  10   4.6708      1.010  9.6606     0.916  165.19
  11   4.6714      1.010  9.6594     1.014  181.62
  12   4.6705      1.030  9.6644     1.000  197.99
  13   4.6707      1.020  9.6612     0.958  214.37
  14   4.6705      0.980  9.6585     1.036  230.81
  15   4.6703      1.030  9.6631     0.968  247.17
  16   4.6707      1.000  9.6649     1.014  263.58
  17   4.6708      1.010  9.6565     1.054  279.98
  18   4.6701      1.000  9.6582     1.034  296.42
  19   4.6710      0.990  9.6613     0.948  312.81
  20   4.6697      1.050  9.6564     0.986  329.25
  21   4.6704      1.020  9.6632     0.960  345.63
  22   4.6703      1.000  9.6601     0.978  362.09
  23   4.6706      1.060  9.6629     1.072  378.47
  24   4.6706      1.020  9.6592     0.970  394.86
  25   4.6706      0.990  9.6623     1.010  411.31
  26   4.6705      1.000  9.6615     0.958  427.67
  27   4.6701      1.010  9.6589     0.954  444.12
  28   4.6702      0.970  9.6600     1.038  460.50
  29   4.6711      1.000  9.6624     1.024  476.93
  30   4.6702      1.010  9.6637     0.978  493.31
  31   4.6711      0.990  9.6578     0.924  509.74
  32   4.6702      1.050  9.6615     0.946  526.12
  33   4.6694      0.960  9.6662     0.946  542.51
  34   4.6707      0.990  9.6613     1.040  558.95
  35   4.6704      0.960  9.6571     0.962  575.33
  36   4.6706      0.990  9.6640     0.990  591.79
  37   4.6720      1.070  9.6612     0.982  608.16
  38   4.6711      1.040  9.6572     1.002  624.58
  39   4.6713      1.060  9.6654     0.934  640.94
  40   4.6711      0.980  9.6636     1.008  657.38
  41   4.6702      0.950  9.6644     0.986  673.76
  42   4.6704      1.010  9.6583     0.924  690.11
  43   4.6710      1.030  9.6608     0.998  706.54
  44   4.6712      1.020  9.6603     1.000  722.94
  45   4.6702      1.030  9.6562     0.960  739.37
  46   4.6707      1.070  9.6619     1.020  755.75
  47   4.6706      1.050  9.6578     1.028  772.18
  48   4.6699      1.050  9.6583     0.928  788.56
  49   4.6701      1.010  9.6660     1.018  805.02
  50   4.6712      1.000  9.6649     0.904  821.40
  51   4.6695      1.000  9.6624     1.058  837.78
  52   4.6701      1.060  9.6630     0.962  854.22
  53   4.6699      1.020  9.6584     0.940  870.60
  54   4.6697      1.040  9.6602     0.946  887.06
  55   4.6710      1.000  9.6637     0.944  903.44
  56   4.6709      1.000  9.6574     0.946  919.88
  57   4.6714      0.970  9.6612     1.006  936.25
  58   4.6697      1.000  9.6630     1.004  952.69
  59   4.6696      0.990  9.6626     0.984  969.06
  60   4.6707      1.040  9.6621     0.914  985.52
  61   4.6695      0.960  9.6620     1.028  1001.91
  62   4.6710      0.960  9.6601     0.990  1018.30
  63   4.6690      1.010  9.6609     0.960  1034.79
  64   4.6711      1.040  9.6619     1.024  1051.15
  65   4.6709      1.030  9.6551     0.974  1067.58
  66   4.6712      1.040  9.6592     0.950  1083.96
  67   4.6701      1.010  9.6619     0.976  1100.41
  68   4.6706      1.040  9.6532     0.946  1116.78
  69   4.6710      0.990  9.6571     0.956  1133.21
  70   4.6700      1.010  9.6576     1.032  1149.58
  71   4.6705      1.040  9.6640     0.942  1165.96
  72   4.6703      0.970  9.6672     0.918  1182.42
  73   4.6707      1.030  9.6603     1.028  1198.81
  74   4.6707      0.990  9.6599     0.968  1215.24
  75   4.6704      1.020  9.6569     0.972  1231.62
  76   4.6705      1.030  9.6658     1.054  1248.06
  77   4.6713      1.010  9.6668     1.010  1264.45
  78   4.6713      1.010  9.6619     0.966  1280.93
  79   4.6708      0.970  9.6580     0.970  1297.30
  80   4.6710      0.980  9.6574     0.960  1313.69
  81   4.6703      1.020  9.6686     0.964  1330.17
  82   4.6707      1.060  9.6588     0.914  1346.56
  83   4.6702      0.990  9.6562     0.978  1363.02
  84   4.6715      1.020  9.6600     0.970  1379.41
  85   4.6702      1.030  9.6628     0.964  1395.85
