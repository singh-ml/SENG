Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8904017920 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6640      0.790  9.6320     1.040  12.03
   2   4.6644      0.750  9.6360     0.976  22.28
   3   4.6642      0.870  9.6386     0.998  32.49
   4   4.6651      0.820  9.6359     0.994  42.76
   5   4.6656      0.840  9.6327     1.006  52.96
   6   4.6644      0.820  9.6394     0.964  63.14
   7   4.6647      0.760  9.6368     0.998  73.40
   8   4.6647      0.820  9.6388     1.008  83.72
   9   4.6642      0.850  9.6364     0.970  93.90
  10   4.6645      0.790  9.6329     1.036  104.07
  11   4.6646      0.840  9.6366     0.988  114.39
  12   4.6644      0.760  9.6358     0.996  124.60
  13   4.6630      0.770  9.6311     0.950  134.86
  14   4.6646      0.790  9.6383     0.920  145.06
  15   4.6643      0.810  9.6368     0.970  155.33
  16   4.6649      0.810  9.6325     0.978  165.56
  17   4.6644      0.780  9.6417     0.946  175.81
  18   4.6646      0.760  9.6380     1.070  186.18
  19   4.6639      0.780  9.6375     0.992  196.38
  20   4.6652      0.790  9.6381     0.950  206.62
  21   4.6648      0.810  9.6330     1.084  216.99
  22   4.6647      0.810  9.6403     0.998  227.22
  23   4.6646      0.780  9.6352     1.042  237.45
  24   4.6641      0.820  9.6360     1.032  247.62
  25   4.6647      0.830  9.6352     1.020  257.86
  26   4.6665      0.820  9.6354     1.034  268.09
  27   4.6640      0.780  9.6380     0.990  278.27
  28   4.6656      0.810  9.6370     1.032  288.59
  29   4.6639      0.840  9.6385     0.990  298.81
  30   4.6635      0.770  9.6403     1.012  309.00
  31   4.6649      0.760  9.6319     1.024  319.22
  32   4.6646      0.780  9.6404     0.980  329.48
  33   4.6639      0.800  9.6330     1.004  339.66
  34   4.6645      0.760  9.6375     1.030  349.86
  35   4.6643      0.810  9.6323     1.086  360.12
  36   4.6648      0.850  9.6375     1.084  370.36
  37   4.6631      0.780  9.6333     1.026  380.59
  38   4.6639      0.750  9.6350     1.002  390.76
  39   4.6641      0.790  9.6371     0.992  401.05
  40   4.6643      0.780  9.6334     0.936  411.29
  41   4.6652      0.830  9.6390     1.044  421.50
  42   4.6645      0.820  9.6323     1.044  431.81
  43   4.6634      0.780  9.6383     0.980  442.01
  44   4.6645      0.760  9.6319     1.028  452.27
  45   4.6637      0.790  9.6342     0.984  462.43
  46   4.6642      0.750  9.6366     0.922  472.72
  47   4.6645      0.820  9.6388     1.026  482.94
  48   4.6655      0.810  9.6338     1.094  493.14
  49   4.6634      0.790  9.6406     0.980  503.37
  50   4.6645      0.760  9.6368     0.998  513.61
  51   4.6632      0.810  9.6368     0.942  523.85
  52   4.6642      0.790  9.6383     0.946  534.13
  53   4.6642      0.830  9.6347     1.016  544.31
  54   4.6651      0.760  9.6372     1.044  554.45
  55   4.6639      0.770  9.6336     0.914  564.67
  56   4.6642      0.780  9.6372     0.992  574.92
  57   4.6645      0.790  9.6342     0.988  585.10
  58   4.6638      0.800  9.6374     0.972  595.35
  59   4.6637      0.790  9.6384     0.998  605.64
  60   4.6646      0.790  9.6351     1.046  615.83
  61   4.6641      0.760  9.6426     0.986  626.02
  62   4.6647      0.800  9.6392     1.056  636.23
  63   4.6649      0.780  9.6364     0.986  646.50
  64   4.6643      0.790  9.6366     0.996  656.65
  65   4.6640      0.800  9.6375     0.986  666.81
  66   4.6643      0.820  9.6356     0.970  677.01
  67   4.6648      0.780  9.6394     1.042  687.23
  68   4.6650      0.840  9.6363     0.998  697.43
  69   4.6643      0.830  9.6363     0.952  707.67
  70   4.6636      0.760  9.6445     0.962  717.96
  71   4.6638      0.810  9.6392     1.004  728.19
  72   4.6643      0.750  9.6397     0.954  738.34
  73   4.6637      0.760  9.6388     1.028  748.50
  74   4.6637      0.780  9.6374     0.956  758.79
  75   4.6634      0.750  9.6352     0.992  768.98
  76   4.6648      0.770  9.6347     0.970  779.20
  77   4.6650      0.810  9.6336     0.994  789.51
  78   4.6635      0.780  9.6384     0.964  799.76
  79   4.6650      0.770  9.6356     0.918  809.92
  80   4.6650      0.810  9.6348     1.020  820.21
  81   4.6647      0.800  9.6358     1.044  830.42
  82   4.6642      0.790  9.6391     0.968  840.63
  83   4.6647      0.760  9.6388     1.016  850.81
  84   4.6637      0.780  9.6351     0.976  861.12
  85   4.6650      0.770  9.6381     0.998  871.33
