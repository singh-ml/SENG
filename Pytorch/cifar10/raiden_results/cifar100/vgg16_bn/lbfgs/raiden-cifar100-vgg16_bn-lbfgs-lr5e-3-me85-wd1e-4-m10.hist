Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9979726848 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6767      1.080  7.3040     1.062  12.76
   2   4.6786      1.000  7.3042     0.972  23.82
   3   4.6782      1.020  7.3059     0.996  34.99
   4   4.6781      1.010  7.3034     0.994  45.85
   5   4.6777      1.040  7.2977     0.972  56.60
   6   4.6791      0.970  7.2983     0.966  67.37
   7   4.6789      0.980  7.2980     1.008  78.20
   8   4.6779      1.070  7.3015     1.034  88.97
   9   4.6779      1.000  7.3000     1.056  99.72
  10   4.6784      1.080  7.2989     0.990  110.57
  11   4.6776      1.020  7.2988     1.034  121.32
  12   4.6778      1.030  7.3067     0.980  132.07
  13   4.6787      0.990  7.3060     1.046  142.89
  14   4.6774      1.040  7.3007     0.990  153.62
  15   4.6767      1.080  7.3048     0.956  164.36
  16   4.6783      0.970  7.3067     0.962  175.12
  17   4.6803      0.960  7.3023     1.026  185.97
  18   4.6781      1.050  7.3012     1.072  196.71
  19   4.6791      1.010  7.3019     1.022  207.47
  20   4.6786      1.060  7.3001     0.968  218.34
  21   4.6777      1.060  7.2994     0.982  229.10
  22   4.6793      0.980  7.3014     1.100  239.89
  23   4.6769      1.110  7.3065     1.012  250.71
  24   4.6786      1.020  7.2972     1.092  261.48
  25   4.6790      1.040  7.3020     1.012  272.25
  26   4.6786      1.030  7.3036     0.956  283.07
  27   4.6783      0.990  7.3030     1.020  293.83
  28   4.6787      0.980  7.2987     0.978  304.57
  29   4.6785      1.000  7.3029     0.988  315.33
  30   4.6784      1.030  7.3035     1.000  326.13
  31   4.6777      1.020  7.3019     0.994  336.87
  32   4.6784      0.990  7.2988     1.012  347.65
  33   4.6785      1.030  7.2980     1.054  358.47
  34   4.6789      1.050  7.2975     1.016  369.21
  35   4.6784      1.010  7.3043     1.024  379.95
  36   4.6786      1.050  7.3029     1.002  390.69
  37   4.6785      1.010  7.3007     0.976  401.54
  38   4.6779      1.010  7.3071     1.068  412.29
  39   4.6781      1.050  7.3053     0.992  423.06
  40   4.6790      1.000  7.2996     1.076  433.92
  41   4.6791      1.000  7.3035     1.024  444.66
  42   4.6781      1.030  7.3043     1.018  455.42
  43   4.6779      1.070  7.2954     0.928  466.25
  44   4.6769      1.070  7.3037     1.022  476.99
  45   4.6794      0.970  7.3065     1.032  487.76
  46   4.6780      1.060  7.2947     1.018  498.58
  47   4.6774      0.990  7.3053     0.876  509.33
  48   4.6787      0.970  7.3042     0.952  520.08
  49   4.6792      1.020  7.3046     0.978  530.82
  50   4.6787      1.020  7.3010     1.024  541.64
  51   4.6791      1.000  7.3027     0.984  552.40
  52   4.6790      1.020  7.3020     0.996  563.18
  53   4.6779      1.060  7.2993     0.994  574.01
  54   4.6789      1.020  7.3044     0.936  584.76
  55   4.6784      1.000  7.3006     1.060  595.51
  56   4.6783      1.000  7.3010     0.972  606.31
  57   4.6790      1.040  7.3009     1.028  617.06
  58   4.6783      1.030  7.3070     0.948  627.85
  59   4.6782      1.080  7.3015     1.000  638.70
  60   4.6779      1.070  7.2937     1.076  649.48
  61   4.6778      1.030  7.3053     0.958  660.19
  62   4.6775      1.020  7.2950     1.004  670.95
  63   4.6784      1.060  7.2974     1.024  681.80
  64   4.6790      0.970  7.2984     1.018  692.55
  65   4.6786      1.060  7.3028     0.996  703.31
  66   4.6784      1.000  7.3049     0.946  714.13
  67   4.6789      1.010  7.2953     1.032  724.87
  68   4.6782      1.040  7.2997     0.998  735.61
  69   4.6779      1.040  7.3057     1.088  746.36
  70   4.6790      1.030  7.3056     1.032  757.13
  71   4.6786      1.000  7.3016     0.994  767.89
  72   4.6791      0.970  7.3066     0.950  778.64
  73   4.6782      1.000  7.3023     0.982  789.43
  74   4.6779      1.050  7.3015     1.052  800.17
  75   4.6775      1.040  7.3066     0.992  810.92
  76   4.6776      1.080  7.2976     1.008  821.77
  77   4.6779      1.020  7.2968     0.980  832.51
  78   4.6787      1.010  7.3002     0.968  843.26
  79   4.6783      1.010  7.3008     1.044  854.11
  80   4.6786      1.010  7.3051     1.054  864.86
  81   4.6774      1.010  7.3034     0.964  875.63
  82   4.6781      1.030  7.2987     1.042  886.40
  83   4.6782      1.040  7.3005     1.044  897.27
  84   4.6776      0.990  7.3097     0.978  908.02
  85   4.6786      1.020  7.2997     0.980  918.75
