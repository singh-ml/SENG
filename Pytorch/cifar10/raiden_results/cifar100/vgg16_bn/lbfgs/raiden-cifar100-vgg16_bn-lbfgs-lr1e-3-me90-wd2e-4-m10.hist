Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6705      0.880  9.6577     0.978  12.05
   2   4.6715      0.900  9.6665     0.926  22.35
   3   4.6714      0.910  9.6616     0.964  32.62
   4   4.6699      0.870  9.6628     0.932  42.84
   5   4.6698      0.900  9.6642     1.022  53.16
   6   4.6693      0.930  9.6590     1.020  63.40
   7   4.6701      0.910  9.6601     0.976  73.63
   8   4.6718      0.940  9.6643     1.054  83.88
   9   4.6706      0.890  9.6530     1.004  94.25
  10   4.6704      0.900  9.6674     0.904  104.52
  11   4.6709      0.920  9.6562     1.002  114.75
  12   4.6705      0.910  9.6611     1.004  125.09
  13   4.6703      0.930  9.6555     1.030  135.35
  14   4.6711      0.920  9.6594     1.002  145.64
  15   4.6694      0.910  9.6614     1.064  155.88
  16   4.6705      0.890  9.6627     1.006  166.22
  17   4.6708      0.870  9.6603     1.014  176.42
  18   4.6707      0.900  9.6634     0.954  186.66
  19   4.6691      0.900  9.6593     0.982  197.02
  20   4.6709      0.910  9.6595     0.922  207.29
  21   4.6701      0.950  9.6592     0.984  217.54
  22   4.6705      0.870  9.6634     0.930  227.76
  23   4.6713      0.900  9.6601     1.004  238.11
  24   4.6706      0.890  9.6609     0.984  248.41
  25   4.6718      0.910  9.6618     0.972  258.67
  26   4.6708      0.900  9.6542     1.040  269.00
  27   4.6716      0.910  9.6623     1.072  279.21
  28   4.6704      0.900  9.6640     0.950  289.50
  29   4.6698      0.960  9.6658     1.010  299.75
  30   4.6701      0.900  9.6597     1.022  310.12
  31   4.6721      0.900  9.6613     0.962  320.38
  32   4.6707      0.890  9.6593     1.020  330.62
  33   4.6698      0.900  9.6648     0.942  340.93
  34   4.6718      0.900  9.6602     0.970  351.21
  35   4.6714      0.910  9.6570     0.958  361.45
  36   4.6711      0.920  9.6598     1.008  371.71
  37   4.6701      0.920  9.6586     0.962  382.03
  38   4.6727      0.890  9.6600     0.898  392.25
  39   4.6718      0.920  9.6622     0.936  402.52
  40   4.6706      0.910  9.6635     0.988  412.83
  41   4.6704      0.880  9.6640     1.036  423.10
  42   4.6701      0.900  9.6635     0.950  433.36
  43   4.6714      0.870  9.6621     0.956  443.69
  44   4.6698      0.900  9.6576     1.012  454.01
  45   4.6717      0.910  9.6574     0.982  464.26
  46   4.6712      0.890  9.6646     0.952  474.47
  47   4.6707      0.900  9.6652     0.986  484.82
  48   4.6712      0.890  9.6658     0.940  495.09
  49   4.6715      0.920  9.6717     0.896  505.31
  50   4.6708      0.900  9.6585     0.930  515.53
  51   4.6699      0.900  9.6622     0.978  525.86
  52   4.6709      0.920  9.6586     1.038  536.11
  53   4.6711      0.900  9.6610     1.018  546.36
  54   4.6718      0.900  9.6627     0.986  556.70
  55   4.6714      0.900  9.6585     0.988  566.99
  56   4.6711      0.930  9.6564     0.926  577.27
  57   4.6720      0.900  9.6568     1.062  587.50
  58   4.6708      0.890  9.6626     1.078  597.82
  59   4.6713      0.910  9.6622     0.992  608.07
  60   4.6703      0.920  9.6598     1.010  618.34
  61   4.6710      0.940  9.6607     1.092  628.71
  62   4.6702      0.890  9.6584     0.988  638.96
  63   4.6705      0.870  9.6611     0.958  649.20
  64   4.6705      0.930  9.6571     0.992  659.54
  65   4.6704      0.900  9.6635     0.918  669.81
  66   4.6698      0.920  9.6618     0.958  680.10
  67   4.6705      0.940  9.6617     1.012  690.39
  68   4.6695      0.860  9.6649     0.948  700.74
  69   4.6702      0.920  9.6637     0.978  711.04
  70   4.6698      0.900  9.6638     0.968  721.30
  71   4.6712      0.890  9.6633     1.038  731.56
  72   4.6693      0.900  9.6628     0.896  741.88
  73   4.6700      0.900  9.6536     0.916  752.15
  74   4.6698      0.850  9.6592     1.074  762.40
  75   4.6697      0.890  9.6605     1.016  772.74
  76   4.6704      0.950  9.6609     0.968  782.93
  77   4.6704      0.900  9.6604     1.028  793.16
  78   4.6709      0.920  9.6565     1.042  803.40
  79   4.6707      0.950  9.6605     1.012  813.77
  80   4.6699      0.900  9.6654     0.934  823.99
  81   4.6711      0.870  9.6603     1.012  834.24
  82   4.6703      0.860  9.6587     0.898  844.60
  83   4.6701      0.910  9.6667     0.974  854.86
  84   4.6709      0.910  9.6634     1.008  865.13
  85   4.6711      0.930  9.6601     0.934  875.41
  86   4.6708      0.930  9.6570     0.996  885.75
  87   4.6703      0.920  9.6631     0.942  895.96
  88   4.6711      0.900  9.6677     0.916  906.24
  89   4.6729      0.930  9.6612     0.992  916.56
  90   4.6704      0.900  9.6615     0.948  926.82
