Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9979333632 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6636      0.980  7.2818     0.994  12.50
   2   4.6637      0.970  7.2771     1.040  23.25
   3   4.6642      1.000  7.2795     0.938  33.99
   4   4.6636      1.010  7.2812     0.992  44.79
   5   4.6640      0.980  7.2799     0.982  55.45
   6   4.6643      0.960  7.2793     0.924  66.17
   7   4.6650      0.980  7.2806     0.926  76.86
   8   4.6642      0.940  7.2859     0.978  87.56
   9   4.6644      0.920  7.2784     0.978  98.24
  10   4.6641      1.000  7.2801     1.016  108.97
  11   4.6638      0.930  7.2787     1.002  119.74
  12   4.6635      0.950  7.2766     1.006  130.44
  13   4.6641      1.010  7.2801     1.002  141.20
  14   4.6644      1.010  7.2766     1.006  151.91
  15   4.6642      0.930  7.2791     1.040  162.65
  16   4.6641      0.980  7.2770     0.994  173.36
  17   4.6641      0.990  7.2808     0.912  184.01
  18   4.6638      0.930  7.2835     1.018  194.79
  19   4.6640      0.990  7.2764     1.062  205.50
  20   4.6638      0.960  7.2843     1.014  216.25
  21   4.6636      0.990  7.2785     1.050  227.01
  22   4.6646      0.990  7.2799     1.018  237.76
  23   4.6640      1.000  7.2830     0.952  248.53
  24   4.6643      1.000  7.2795     1.028  259.24
  25   4.6637      1.000  7.2769     1.046  269.91
  26   4.6644      1.010  7.2816     1.108  280.61
  27   4.6643      0.940  7.2770     0.970  291.37
  28   4.6642      0.990  7.2860     0.974  302.12
  29   4.6639      0.970  7.2832     0.944  312.91
  30   4.6644      1.000  7.2774     0.964  323.65
  31   4.6640      0.920  7.2814     1.054  334.38
  32   4.6637      0.990  7.2813     1.000  345.12
  33   4.6640      1.000  7.2783     1.088  355.74
  34   4.6645      0.970  7.2818     0.988  366.45
  35   4.6633      0.940  7.2812     0.944  377.25
  36   4.6644      1.000  7.2761     0.996  387.97
  37   4.6637      0.930  7.2784     1.038  398.73
  38   4.6643      0.950  7.2777     0.972  409.44
  39   4.6642      0.980  7.2812     0.966  420.08
  40   4.6644      0.930  7.2790     0.898  430.84
  41   4.6632      0.970  7.2753     0.958  441.62
  42   4.6640      0.980  7.2824     0.970  452.31
  43   4.6637      0.980  7.2802     0.994  462.99
  44   4.6641      0.970  7.2780     0.956  473.69
  45   4.6640      0.970  7.2779     0.938  484.36
  46   4.6635      0.950  7.2840     1.030  495.03
  47   4.6644      0.980  7.2811     1.030  505.66
  48   4.6642      0.950  7.2715     1.066  516.41
  49   4.6639      0.970  7.2838     1.100  527.06
  50   4.6635      1.030  7.2812     1.026  537.79
  51   4.6640      0.970  7.2785     1.054  548.54
  52   4.6645      0.930  7.2796     0.972  559.22
  53   4.6642      0.920  7.2887     0.974  569.94
  54   4.6641      0.970  7.2736     1.014  580.63
  55   4.6644      1.000  7.2785     0.992  591.38
  56   4.6644      0.960  7.2753     0.966  602.15
  57   4.6640      0.960  7.2740     1.082  612.83
  58   4.6647      0.980  7.2819     0.990  623.56
  59   4.6642      1.010  7.2826     1.008  634.24
  60   4.6638      1.000  7.2825     0.966  644.93
  61   4.6639      0.960  7.2743     1.018  655.69
  62   4.6638      0.960  7.2765     0.950  666.43
  63   4.6641      0.920  7.2787     0.974  677.09
  64   4.6642      0.970  7.2777     0.970  687.79
  65   4.6644      0.960  7.2755     1.038  698.55
  66   4.6637      0.970  7.2803     1.046  709.32
  67   4.6641      0.930  7.2831     1.020  720.08
  68   4.6643      0.970  7.2740     1.010  730.79
  69   4.6643      0.990  7.2806     0.966  741.51
  70   4.6646      0.990  7.2773     0.992  752.22
  71   4.6641      0.940  7.2808     1.130  762.91
  72   4.6640      0.980  7.2778     0.972  773.63
  73   4.6643      0.960  7.2833     1.040  784.32
  74   4.6635      0.910  7.2780     1.030  795.05
  75   4.6640      0.960  7.2859     0.994  805.89
  76   4.6645      0.930  7.2789     0.952  816.58
  77   4.6642      0.940  7.2856     0.978  827.33
  78   4.6638      0.980  7.2758     1.036  838.10
  79   4.6645      1.010  7.2792     0.990  848.82
  80   4.6638      1.010  7.2798     0.940  859.54
  81   4.6643      0.980  7.2809     0.952  870.27
  82   4.6636      0.930  7.2728     1.054  880.94
  83   4.6642      0.970  7.2830     0.922  891.60
  84   4.6636      0.960  7.2784     1.042  902.33
  85   4.6640      1.000  7.2757     1.040  913.09
  86   4.6650      0.940  7.2796     0.994  923.84
  87   4.6641      0.960  7.2781     1.012  934.57
  88   4.6642      0.980  7.2772     0.992  945.40
  89   4.6640      0.950  7.2772     1.100  956.10
  90   4.6642      1.020  7.2736     1.014  966.83
