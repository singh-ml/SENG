Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9980120064 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6768      0.720  7.3102     0.960  12.67
   2   4.6768      0.710  7.3079     0.940  23.77
   3   4.6776      0.730  7.3107     0.934  34.58
   4   4.6798      0.680  7.3128     0.972  45.33
   5   4.6781      0.700  7.3080     0.982  56.09
   6   4.6777      0.670  7.3106     1.032  66.87
   7   4.6787      0.650  7.3072     0.950  77.67
   8   4.6781      0.670  7.3170     0.942  88.42
   9   4.6770      0.730  7.3083     0.916  99.17
  10   4.6795      0.650  7.3041     0.964  109.89
  11   4.6789      0.670  7.3099     0.896  120.66
  12   4.6773      0.710  7.3066     0.948  131.42
  13   4.6797      0.650  7.3142     1.012  142.17
  14   4.6766      0.690  7.3077     1.032  153.01
  15   4.6789      0.710  7.3149     0.890  163.77
  16   4.6783      0.650  7.3103     0.990  174.52
  17   4.6791      0.690  7.3076     0.902  185.34
  18   4.6794      0.650  7.3096     0.866  196.11
  19   4.6780      0.670  7.3140     1.014  206.88
  20   4.6805      0.650  7.3085     0.990  217.73
  21   4.6780      0.610  7.3084     0.912  228.49
  22   4.6793      0.640  7.3073     0.978  239.23
  23   4.6780      0.710  7.3086     1.026  250.09
  24   4.6775      0.690  7.3115     1.002  260.84
  25   4.6788      0.670  7.3103     0.958  271.59
  26   4.6781      0.650  7.3086     0.914  282.33
  27   4.6789      0.700  7.3125     0.912  293.17
  28   4.6788      0.670  7.3067     1.010  303.89
  29   4.6803      0.560  7.3051     0.950  314.64
  30   4.6789      0.720  7.3065     0.958  325.40
  31   4.6778      0.660  7.3068     0.974  336.24
  32   4.6790      0.660  7.3101     1.026  346.99
  33   4.6766      0.680  7.3079     0.960  357.74
  34   4.6789      0.660  7.3073     0.984  368.55
  35   4.6789      0.590  7.3117     0.996  379.30
  36   4.6783      0.650  7.3134     0.990  390.05
  37   4.6790      0.670  7.3105     0.958  400.84
  38   4.6776      0.720  7.3124     1.008  411.59
  39   4.6784      0.660  7.3109     0.836  422.31
  40   4.6791      0.720  7.3085     0.916  433.14
  41   4.6781      0.700  7.3149     0.980  443.91
  42   4.6788      0.700  7.3082     0.850  454.70
  43   4.6783      0.660  7.3095     0.964  465.45
  44   4.6776      0.660  7.3146     0.946  476.29
  45   4.6786      0.660  7.3051     0.948  487.05
  46   4.6776      0.660  7.3099     1.030  497.81
  47   4.6793      0.650  7.3080     0.916  508.59
  48   4.6800      0.620  7.3045     0.958  519.42
  49   4.6801      0.660  7.3079     0.994  530.19
  50   4.6773      0.680  7.3090     0.936  540.96
  51   4.6804      0.620  7.3043     1.042  551.83
  52   4.6782      0.690  7.3103     0.896  562.58
  53   4.6783      0.750  7.3109     0.944  573.36
  54   4.6775      0.670  7.3042     0.984  584.20
  55   4.6779      0.650  7.3107     0.984  594.96
  56   4.6781      0.680  7.3054     1.024  605.73
  57   4.6790      0.630  7.3063     0.992  616.55
  58   4.6786      0.680  7.3059     1.042  627.32
  59   4.6801      0.610  7.3079     0.950  638.08
  60   4.6763      0.740  7.3094     0.966  648.86
  61   4.6795      0.640  7.3069     0.988  659.61
  62   4.6804      0.640  7.3115     1.104  670.35
  63   4.6780      0.660  7.3108     0.922  681.12
  64   4.6773      0.720  7.3077     0.948  691.96
  65   4.6783      0.730  7.3097     1.018  702.68
  66   4.6775      0.700  7.3073     0.992  713.42
  67   4.6770      0.700  7.3080     0.954  724.26
  68   4.6777      0.660  7.3102     0.946  735.02
  69   4.6804      0.620  7.3096     0.992  745.79
  70   4.6783      0.710  7.3101     0.976  756.64
  71   4.6792      0.690  7.3128     0.958  767.39
  72   4.6796      0.670  7.3096     0.944  778.11
  73   4.6795      0.690  7.3066     0.984  788.86
  74   4.6793      0.670  7.3084     0.904  799.68
  75   4.6787      0.670  7.3114     0.986  810.43
  76   4.6781      0.690  7.3115     0.916  821.17
  77   4.6788      0.650  7.3133     0.890  832.01
  78   4.6774      0.750  7.3129     0.932  842.78
  79   4.6786      0.660  7.3080     1.026  853.53
  80   4.6783      0.670  7.3126     0.922  864.32
  81   4.6795      0.690  7.3062     0.964  875.09
  82   4.6783      0.630  7.3100     0.932  885.87
  83   4.6784      0.650  7.3119     0.968  896.64
  84   4.6797      0.650  7.3076     0.940  907.46
  85   4.6772      0.700  7.3041     0.960  918.37
