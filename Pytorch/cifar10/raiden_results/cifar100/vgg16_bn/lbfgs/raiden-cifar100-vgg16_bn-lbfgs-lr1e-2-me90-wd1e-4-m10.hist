Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11059761152 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6648      0.960  7.2811     1.056  13.24
   2   4.6664      0.970  7.2906     1.006  25.04
   3   4.6672      1.000  7.2897     1.022  36.80
   4   4.6657      0.920  7.2859     0.992  48.60
   5   4.6656      0.910  7.2826     0.992  60.46
   6   4.6666      0.970  7.2855     0.910  72.19
   7   4.6653      0.990  7.2830     1.048  83.89
   8   4.6664      0.960  7.2839     1.012  95.50
   9   4.6668      0.960  7.2838     1.010  107.11
  10   4.6662      1.040  7.2829     0.998  118.66
  11   4.6661      0.990  7.2835     1.078  130.34
  12   4.6662      0.950  7.2888     1.020  141.92
  13   4.6663      0.960  7.2853     1.028  153.50
  14   4.6658      0.950  7.2874     0.970  165.11
  15   4.6656      0.930  7.2890     0.946  176.67
  16   4.6667      0.970  7.2860     1.022  188.27
  17   4.6660      0.920  7.2837     1.002  199.96
  18   4.6661      0.960  7.2913     0.966  211.56
  19   4.6661      0.900  7.2926     0.974  223.10
  20   4.6666      1.010  7.2841     1.060  234.68
  21   4.6664      0.950  7.2866     0.984  246.27
  22   4.6665      1.000  7.2819     1.042  257.84
  23   4.6661      0.960  7.2885     0.944  269.37
  24   4.6670      0.980  7.2824     0.936  281.00
  25   4.6660      0.990  7.2853     0.940  292.58
  26   4.6661      0.950  7.2877     1.040  304.15
  27   4.6660      0.970  7.2862     0.976  315.83
  28   4.6666      0.940  7.2863     0.994  327.38
  29   4.6661      0.930  7.2863     0.996  338.93
  30   4.6666      0.990  7.2818     1.092  350.57
  31   4.6654      0.930  7.2831     1.058  362.12
  32   4.6676      0.910  7.2842     1.000  373.65
  33   4.6670      0.930  7.2909     1.044  385.34
  34   4.6666      0.960  7.2887     0.990  396.90
  35   4.6661      0.950  7.2879     1.058  408.48
  36   4.6661      0.980  7.2808     0.968  420.15
  37   4.6667      0.990  7.2857     1.006  431.75
  38   4.6662      0.990  7.2831     0.974  443.32
  39   4.6663      0.950  7.2821     0.972  454.98
  40   4.6662      1.020  7.2836     1.090  466.55
  41   4.6658      0.970  7.2890     0.972  478.14
  42   4.6666      0.950  7.2831     1.136  489.72
  43   4.6661      0.930  7.2845     1.020  501.27
  44   4.6669      1.000  7.2861     1.002  512.85
  45   4.6667      1.010  7.2829     1.038  524.55
  46   4.6668      0.980  7.2901     1.008  536.14
  47   4.6674      0.920  7.2828     1.058  547.66
  48   4.6663      0.920  7.2880     0.976  559.29
  49   4.6658      0.900  7.2835     1.076  570.85
  50   4.6670      0.990  7.2865     0.956  582.39
  51   4.6664      0.970  7.2828     0.950  594.05
  52   4.6658      0.970  7.2838     1.020  605.61
  53   4.6662      0.990  7.2878     1.060  617.14
  54   4.6662      0.980  7.2831     1.056  628.72
  55   4.6655      0.950  7.2859     0.994  640.35
  56   4.6667      0.960  7.2859     1.012  651.91
  57   4.6667      0.970  7.2910     0.936  663.46
  58   4.6661      1.000  7.2842     0.980  675.15
  59   4.6664      0.940  7.2890     0.978  686.69
  60   4.6658      0.960  7.2848     0.992  698.25
  61   4.6666      0.980  7.2851     1.030  709.90
  62   4.6676      1.000  7.2859     0.986  721.49
  63   4.6666      0.930  7.2876     0.952  733.03
  64   4.6675      1.040  7.2857     1.026  744.70
  65   4.6662      0.950  7.2833     0.952  756.32
  66   4.6660      0.920  7.2862     0.974  767.91
  67   4.6664      0.990  7.2889     0.964  779.57
  68   4.6669      0.890  7.2826     0.992  791.13
  69   4.6658      0.960  7.2854     0.992  802.68
  70   4.6664      1.010  7.2878     1.004  814.30
  71   4.6664      0.930  7.2889     1.054  825.88
  72   4.6671      1.050  7.2872     0.896  837.42
  73   4.6671      0.930  7.2929     0.970  848.97
  74   4.6658      0.970  7.2852     1.004  860.58
  75   4.6664      0.950  7.2896     1.004  872.17
  76   4.6671      0.980  7.2858     1.076  883.75
  77   4.6664      0.890  7.2864     1.032  895.49
  78   4.6673      1.020  7.2874     1.024  907.05
  79   4.6665      0.960  7.2828     1.048  918.59
  80   4.6666      0.980  7.2810     0.940  930.24
  81   4.6658      0.970  7.2843     1.036  941.77
  82   4.6666      0.950  7.2869     1.008  953.37
  83   4.6672      0.970  7.2901     1.010  965.03
  84   4.6662      0.980  7.2859     1.034  976.57
  85   4.6659      0.920  7.2787     1.046  988.19
  86   4.6660      0.950  7.2871     1.056  999.90
  87   4.6661      0.900  7.2887     1.018  1011.47
  88   4.6665      0.960  7.2884     0.984  1023.08
  89   4.6658      0.930  7.2795     1.052  1034.75
  90   4.6663      0.930  7.2806     1.032  1046.36
