Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 11058843648 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6606      0.940  9.6458     0.962  13.47
   2   4.6595      0.950  9.6467     1.006  25.12
   3   4.6602      0.890  9.6458     0.996  36.69
   4   4.6606      0.930  9.6485     0.912  48.35
   5   4.6595      0.960  9.6438     1.024  59.98
   6   4.6594      0.960  9.6459     1.066  71.60
   7   4.6597      1.000  9.6497     0.980  83.17
   8   4.6611      0.970  9.6515     0.956  94.79
   9   4.6597      0.930  9.6521     0.978  106.42
  10   4.6602      0.960  9.6469     0.960  118.04
  11   4.6602      1.010  9.6486     0.990  129.69
  12   4.6604      0.980  9.6515     0.988  141.30
  13   4.6600      0.960  9.6485     1.048  152.92
  14   4.6601      0.980  9.6473     1.036  164.57
  15   4.6601      1.010  9.6543     0.976  176.19
  16   4.6606      0.980  9.6465     1.028  187.74
  17   4.6609      0.960  9.6472     1.022  199.41
  18   4.6599      0.960  9.6455     0.980  211.00
  19   4.6607      1.010  9.6452     1.088  222.57
  20   4.6613      0.960  9.6472     1.028  234.23
  21   4.6595      0.940  9.6407     1.110  245.80
  22   4.6603      1.000  9.6472     1.056  257.35
  23   4.6597      0.990  9.6501     1.006  268.99
  24   4.6607      0.950  9.6472     1.060  280.59
  25   4.6600      0.980  9.6532     0.972  292.14
  26   4.6607      0.960  9.6463     1.044  303.78
  27   4.6612      0.920  9.6461     0.992  315.38
  28   4.6602      0.930  9.6446     1.092  326.98
  29   4.6602      0.930  9.6441     1.050  338.67
  30   4.6611      1.000  9.6533     1.038  350.28
  31   4.6609      0.960  9.6419     1.014  361.89
  32   4.6606      0.950  9.6486     0.974  373.58
  33   4.6600      0.910  9.6487     1.034  385.14
  34   4.6597      0.910  9.6478     0.982  396.73
  35   4.6610      0.960  9.6466     1.022  408.39
  36   4.6608      1.000  9.6477     0.988  419.98
  37   4.6599      0.980  9.6463     0.946  431.55
  38   4.6600      0.940  9.6546     0.962  443.16
  39   4.6601      0.920  9.6489     0.976  454.78
  40   4.6609      0.950  9.6491     0.910  466.34
  41   4.6603      0.960  9.6498     0.938  477.91
  42   4.6599      0.870  9.6491     0.986  489.52
  43   4.6600      0.960  9.6486     0.976  501.15
  44   4.6598      0.980  9.6473     1.040  512.70
  45   4.6602      0.970  9.6514     0.954  524.33
  46   4.6598      0.970  9.6481     0.964  535.89
  47   4.6606      1.000  9.6433     1.020  547.49
  48   4.6616      0.930  9.6425     0.938  559.17
  49   4.6607      0.980  9.6451     1.052  570.80
  50   4.6605      0.920  9.6497     0.922  582.32
  51   4.6605      1.000  9.6433     0.942  593.94
  52   4.6609      0.970  9.6457     1.038  605.55
  53   4.6588      0.950  9.6452     0.986  617.10
  54   4.6600      0.860  9.6520     0.986  628.72
  55   4.6595      0.970  9.6434     1.052  640.29
  56   4.6601      0.960  9.6534     0.982  651.87
  57   4.6597      1.040  9.6513     1.022  663.56
  58   4.6598      0.880  9.6477     0.994  675.13
  59   4.6608      0.990  9.6504     0.966  686.71
  60   4.6598      0.980  9.6521     0.938  698.29
  61   4.6594      0.960  9.6524     1.004  709.97
  62   4.6606      0.980  9.6476     0.936  721.56
  63   4.6597      0.980  9.6453     1.032  733.19
  64   4.6607      0.930  9.6498     1.064  744.81
  65   4.6600      0.970  9.6472     0.924  756.43
  66   4.6597      0.950  9.6478     1.028  768.02
  67   4.6602      0.940  9.6491     0.968  779.71
  68   4.6596      0.880  9.6445     0.946  791.25
  69   4.6608      1.040  9.6446     1.020  802.82
  70   4.6600      0.860  9.6470     0.982  814.44
  71   4.6600      0.970  9.6487     1.060  826.01
  72   4.6597      0.950  9.6475     0.954  837.56
  73   4.6605      0.950  9.6474     0.940  849.20
  74   4.6597      0.980  9.6426     1.016  860.76
  75   4.6610      1.020  9.6431     1.012  872.38
  76   4.6594      0.960  9.6463     0.952  884.08
  77   4.6612      0.950  9.6504     1.044  895.69
  78   4.6596      0.920  9.6459     0.944  907.25
  79   4.6604      0.930  9.6481     0.962  918.83
  80   4.6598      0.990  9.6518     1.060  930.47
  81   4.6595      0.980  9.6494     0.980  942.00
  82   4.6605      0.920  9.6441     1.092  953.55
  83   4.6597      0.930  9.6469     1.004  965.25
  84   4.6595      0.930  9.6517     0.942  976.88
  85   4.6603      0.960  9.6470     0.972  988.45
