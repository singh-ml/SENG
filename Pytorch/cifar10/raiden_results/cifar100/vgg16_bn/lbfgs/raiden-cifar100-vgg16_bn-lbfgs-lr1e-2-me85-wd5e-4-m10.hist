Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 12135076864 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6756      0.870  16.7551     1.054  13.71
   2   4.6741      0.900  16.7450     1.072  25.76
   3   4.6759      0.930  16.7571     0.988  37.85
   4   4.6761      0.900  16.7399     1.022  50.04
   5   4.6744      0.930  16.7461     0.960  62.06
   6   4.6742      0.880  16.7480     1.016  74.17
   7   4.6752      0.890  16.7475     0.988  86.24
   8   4.6743      0.900  16.7520     0.984  98.35
   9   4.6759      0.920  16.7513     0.954  110.53
  10   4.6750      0.900  16.7518     0.892  122.60
  11   4.6749      0.900  16.7412     1.012  134.63
  12   4.6764      0.920  16.7478     1.024  146.73
  13   4.6748      0.900  16.7453     0.986  158.89
  14   4.6756      0.910  16.7459     1.048  170.96
  15   4.6762      0.910  16.7464     1.046  183.07
  16   4.6757      0.900  16.7476     0.950  195.19
  17   4.6758      0.920  16.7500     1.000  207.32
  18   4.6749      0.910  16.7442     0.962  219.42
  19   4.6753      0.900  16.7498     1.008  231.53
  20   4.6758      0.890  16.7440     1.000  243.57
  21   4.6750      0.880  16.7476     1.026  255.73
  22   4.6736      0.930  16.7452     1.086  267.88
  23   4.6744      0.920  16.7487     0.970  279.99
  24   4.6745      0.930  16.7405     0.992  292.10
  25   4.6749      0.900  16.7474     1.028  304.13
  26   4.6759      0.910  16.7486     1.036  316.16
  27   4.6755      0.890  16.7488     1.034  328.31
  28   4.6746      0.910  16.7556     0.952  340.36
  29   4.6759      0.890  16.7543     0.986  352.36
  30   4.6746      0.870  16.7471     0.986  364.52
  31   4.6750      0.920  16.7431     0.998  376.62
  32   4.6740      0.910  16.7430     1.036  388.77
  33   4.6753      0.930  16.7539     1.056  400.84
  34   4.6754      0.910  16.7480     1.016  412.91
  35   4.6751      0.870  16.7505     0.958  425.01
  36   4.6747      0.910  16.7433     1.042  437.20
  37   4.6759      0.880  16.7459     0.972  449.23
  38   4.6755      0.920  16.7494     0.940  461.27
  39   4.6748      0.890  16.7463     1.042  473.39
  40   4.6753      0.900  16.7581     0.958  485.50
  41   4.6758      0.920  16.7484     0.990  497.57
  42   4.6755      0.900  16.7465     1.006  509.80
  43   4.6750      0.860  16.7458     1.008  521.87
  44   4.6741      0.940  16.7508     0.992  534.00
  45   4.6743      0.890  16.7489     0.972  546.15
  46   4.6755      0.900  16.7565     0.998  558.28
  47   4.6759      0.910  16.7473     1.022  570.32
  48   4.6761      0.940  16.7475     0.988  582.44
  49   4.6751      0.900  16.7501     0.984  594.51
  50   4.6748      0.900  16.7504     0.964  606.57
  51   4.6745      0.900  16.7492     1.012  618.72
  52   4.6742      0.910  16.7517     0.944  630.83
  53   4.6747      0.970  16.7464     0.996  642.92
  54   4.6747      0.940  16.7435     1.018  655.08
  55   4.6743      0.880  16.7493     0.976  667.10
  56   4.6748      0.880  16.7514     1.122  679.25
  57   4.6754      0.890  16.7489     0.968  691.29
  58   4.6749      0.910  16.7456     1.030  703.33
  59   4.6747      0.910  16.7471     0.966  715.57
  60   4.6752      0.880  16.7499     0.982  727.70
  61   4.6761      0.930  16.7449     0.924  739.73
  62   4.6750      0.890  16.7554     0.954  751.79
  63   4.6751      0.900  16.7452     1.058  763.87
  64   4.6756      0.910  16.7499     0.974  775.91
  65   4.6762      0.940  16.7482     1.100  788.01
  66   4.6759      0.910  16.7476     0.910  800.13
  67   4.6749      0.910  16.7444     1.002  812.16
  68   4.6743      0.920  16.7438     1.006  824.27
  69   4.6741      0.880  16.7495     1.022  836.38
  70   4.6751      0.920  16.7451     0.966  848.48
  71   4.6746      0.910  16.7517     0.964  860.55
  72   4.6746      0.900  16.7501     1.036  872.65
  73   4.6752      0.890  16.7508     0.936  884.66
  74   4.6761      0.940  16.7506     1.002  896.83
  75   4.6755      0.880  16.7493     1.034  908.89
  76   4.6753      0.910  16.7484     0.974  921.00
  77   4.6751      0.940  16.7538     0.970  933.18
  78   4.6749      0.890  16.7516     0.956  945.39
  79   4.6740      0.920  16.7474     1.050  957.42
  80   4.6746      0.900  16.7544     0.942  969.51
  81   4.6753      0.930  16.7507     1.050  981.58
  82   4.6747      0.920  16.7471     0.956  993.68
  83   4.6758      0.930  16.7476     1.078  1005.79
  84   4.6758      0.900  16.7502     1.000  1017.97
  85   4.6753      0.920  16.7453     0.986  1030.09
