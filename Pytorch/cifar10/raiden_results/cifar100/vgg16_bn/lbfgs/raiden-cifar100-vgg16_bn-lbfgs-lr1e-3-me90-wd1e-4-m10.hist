Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6673      0.870  7.2924     1.058  12.01
   2   4.6682      0.920  7.2937     1.036  22.31
   3   4.6679      0.930  7.2865     1.028  32.55
   4   4.6680      0.950  7.2895     0.986  42.81
   5   4.6674      0.940  7.2853     1.056  53.11
   6   4.6677      0.930  7.2826     1.082  63.38
   7   4.6685      0.910  7.2846     1.008  73.64
   8   4.6673      0.870  7.2924     0.998  83.96
   9   4.6676      0.960  7.2948     0.966  94.24
  10   4.6681      0.910  7.2880     1.056  104.48
  11   4.6676      0.980  7.2923     0.982  114.77
  12   4.6672      0.940  7.2868     0.996  125.11
  13   4.6675      0.860  7.2910     1.020  135.36
  14   4.6677      0.890  7.2880     1.078  145.62
  15   4.6678      0.940  7.2962     1.036  155.92
  16   4.6687      0.940  7.2904     1.024  166.17
  17   4.6678      0.910  7.2958     0.902  176.39
  18   4.6685      0.940  7.2866     0.986  186.64
  19   4.6682      0.890  7.2990     1.002  197.00
  20   4.6679      1.030  7.2943     1.024  207.30
  21   4.6676      0.910  7.2926     1.004  217.55
  22   4.6678      0.940  7.2959     0.954  227.89
  23   4.6673      0.930  7.2934     1.008  238.16
  24   4.6675      0.980  7.2881     1.038  248.46
  25   4.6672      0.940  7.2891     1.036  258.79
  26   4.6673      0.930  7.2890     0.974  269.09
  27   4.6682      0.920  7.2922     0.924  279.34
  28   4.6676      0.950  7.2940     1.032  289.64
  29   4.6667      0.850  7.2944     1.034  300.01
  30   4.6669      0.940  7.2890     1.060  310.32
  31   4.6682      0.940  7.2973     1.052  320.59
  32   4.6679      0.940  7.2974     0.992  331.00
  33   4.6678      0.910  7.2915     1.020  341.26
  34   4.6684      0.940  7.2910     1.084  351.56
  35   4.6677      0.890  7.2947     0.966  361.85
  36   4.6670      0.890  7.2969     1.000  372.23
  37   4.6674      0.890  7.2906     1.014  382.48
  38   4.6672      0.920  7.2892     1.078  392.73
  39   4.6679      0.890  7.2901     1.006  403.09
  40   4.6675      0.940  7.2970     1.008  413.37
  41   4.6680      0.950  7.2899     1.004  423.62
  42   4.6667      0.900  7.2897     1.046  433.86
  43   4.6672      0.910  7.2946     0.946  444.20
  44   4.6676      0.980  7.2906     0.940  454.47
  45   4.6679      0.940  7.2924     1.052  464.78
  46   4.6675      0.940  7.2943     0.988  475.08
  47   4.6674      0.900  7.2896     0.982  485.40
  48   4.6672      0.870  7.2869     1.034  495.59
  49   4.6670      0.900  7.2929     1.012  505.92
  50   4.6676      0.910  7.2923     1.000  516.21
  51   4.6672      0.900  7.2893     0.980  526.50
  52   4.6678      0.930  7.2930     1.020  536.81
  53   4.6679      0.930  7.2989     1.014  547.16
  54   4.6677      0.920  7.2939     0.966  557.47
  55   4.6678      0.940  7.2900     0.970  567.73
  56   4.6678      0.940  7.2897     1.076  577.97
  57   4.6680      0.960  7.2878     1.014  588.36
  58   4.6677      1.010  7.2910     0.988  598.61
  59   4.6683      1.000  7.2918     0.964  608.83
  60   4.6684      0.960  7.2910     0.898  619.16
  61   4.6674      0.960  7.2920     1.036  629.43
  62   4.6674      0.870  7.3000     0.966  639.68
  63   4.6672      0.890  7.2952     1.042  649.95
  64   4.6676      0.890  7.2902     1.044  660.27
  65   4.6681      0.900  7.2910     1.032  670.51
  66   4.6680      0.940  7.2957     1.078  680.73
  67   4.6672      0.910  7.2911     0.954  691.05
  68   4.6671      0.930  7.2889     1.060  701.32
  69   4.6678      0.860  7.2957     1.052  711.56
  70   4.6680      0.900  7.2911     1.064  721.83
  71   4.6677      0.930  7.2950     1.054  732.19
  72   4.6676      0.940  7.2901     1.020  742.44
  73   4.6678      1.000  7.2956     1.066  752.73
  74   4.6675      0.940  7.2978     0.996  763.00
  75   4.6675      0.920  7.2890     1.028  773.36
  76   4.6685      0.960  7.2874     0.900  783.62
  77   4.6673      0.900  7.2883     1.042  793.86
  78   4.6680      0.930  7.2976     1.026  804.22
  79   4.6680      0.960  7.2942     1.026  814.44
  80   4.6675      0.880  7.2985     0.920  824.69
  81   4.6677      1.000  7.2930     0.956  834.95
  82   4.6674      0.900  7.2927     1.014  845.23
  83   4.6670      0.920  7.2938     1.036  855.45
  84   4.6679      0.870  7.2931     1.014  865.71
  85   4.6675      0.910  7.2888     0.968  876.12
  86   4.6671      0.930  7.2911     1.032  886.35
  87   4.6677      0.900  7.2949     0.964  896.58
  88   4.6681      0.900  7.2943     0.994  906.89
  89   4.6676      0.950  7.2925     0.964  917.19
  90   4.6670      0.890  7.2920     1.082  927.48
