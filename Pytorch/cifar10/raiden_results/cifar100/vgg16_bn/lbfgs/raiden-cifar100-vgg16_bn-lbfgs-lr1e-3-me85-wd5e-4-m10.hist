Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8901396480 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6893      1.080  16.7886     1.032  13.54
   2   4.6882      1.050  16.7740     1.006  24.00
   3   4.6896      1.080  16.7836     0.962  34.36
   4   4.6894      1.170  16.7831     0.912  44.75
   5   4.6891      1.020  16.7837     0.956  55.23
   6   4.6888      1.090  16.7784     0.990  65.62
   7   4.6897      1.050  16.7856     0.960  75.98
   8   4.6896      1.090  16.7810     0.998  86.38
   9   4.6891      1.000  16.7833     0.986  96.71
  10   4.6876      1.110  16.7879     0.934  107.09
  11   4.6901      1.020  16.7856     1.000  117.49
  12   4.6881      1.060  16.7834     1.050  127.96
  13   4.6889      1.120  16.7907     0.988  138.31
  14   4.6902      1.010  16.7789     1.034  148.66
  15   4.6894      1.040  16.7836     0.960  159.13
  16   4.6900      1.050  16.7775     1.064  169.51
  17   4.6894      1.030  16.7830     0.952  179.85
  18   4.6883      1.070  16.7861     0.992  190.21
  19   4.6887      1.040  16.7825     0.988  200.66
  20   4.6895      1.090  16.7807     0.982  211.04
  21   4.6868      1.050  16.7826     0.988  221.43
  22   4.6893      1.000  16.7853     0.982  231.85
  23   4.6890      1.100  16.7849     1.024  242.20
  24   4.6882      1.110  16.7800     0.976  252.58
  25   4.6892      1.100  16.7827     0.926  263.07
  26   4.6898      1.040  16.7829     1.032  273.41
  27   4.6887      1.030  16.7800     0.994  283.77
  28   4.6902      1.030  16.7804     0.988  294.13
  29   4.6890      1.050  16.7793     1.012  304.54
  30   4.6882      1.080  16.7795     1.050  314.93
  31   4.6892      1.040  16.7776     0.960  325.30
  32   4.6885      1.050  16.7803     0.946  335.75
  33   4.6883      1.110  16.7811     1.032  346.12
  34   4.6882      1.090  16.7785     1.046  356.51
  35   4.6895      0.990  16.7823     0.972  366.95
  36   4.6874      1.020  16.7810     1.042  377.33
  37   4.6889      1.030  16.7765     1.052  387.69
  38   4.6875      1.140  16.7862     1.012  398.07
  39   4.6887      1.040  16.7790     0.966  408.51
  40   4.6886      1.080  16.7847     1.000  418.87
  41   4.6893      1.050  16.7777     1.026  429.23
  42   4.6889      1.080  16.7861     1.000  439.67
  43   4.6880      1.050  16.7855     0.928  450.05
  44   4.6880      1.070  16.7777     1.016  460.43
  45   4.6888      1.110  16.7826     0.966  470.79
  46   4.6886      1.060  16.7813     0.996  481.27
  47   4.6904      1.000  16.7833     0.962  491.65
  48   4.6884      1.100  16.7848     0.910  502.03
  49   4.6878      1.060  16.7781     0.936  512.49
  50   4.6882      1.050  16.7779     1.038  522.84
  51   4.6885      1.060  16.7744     1.030  533.22
  52   4.6889      1.070  16.7871     1.002  543.59
  53   4.6888      1.070  16.7828     0.962  554.06
  54   4.6890      1.130  16.7853     1.008  564.42
  55   4.6911      1.000  16.7816     0.986  574.78
  56   4.6904      1.030  16.7828     0.998  585.22
  57   4.6886      1.090  16.7878     0.950  595.57
  58   4.6889      1.070  16.7873     0.978  605.92
  59   4.6899      1.070  16.7799     0.972  616.31
  60   4.6898      1.030  16.7836     1.000  626.75
  61   4.6905      1.000  16.7826     0.986  637.12
  62   4.6881      1.060  16.7858     1.006  647.48
  63   4.6881      1.110  16.7841     0.924  657.94
  64   4.6870      1.070  16.7848     0.968  668.32
  65   4.6888      1.050  16.7818     0.898  678.68
  66   4.6889      1.090  16.7830     0.970  689.13
  67   4.6893      1.080  16.7812     0.976  699.52
  68   4.6894      1.010  16.7813     0.920  709.90
  69   4.6902      1.040  16.7829     1.056  720.29
  70   4.6876      1.060  16.7785     0.962  730.72
  71   4.6878      1.050  16.7765     1.004  741.09
  72   4.6894      1.090  16.7799     0.964  751.43
  73   4.6883      1.140  16.7824     0.964  761.80
  74   4.6883      0.970  16.7815     0.996  772.27
  75   4.6881      1.100  16.7820     0.950  782.63
  76   4.6879      1.090  16.7778     1.054  793.02
  77   4.6900      1.010  16.7822     1.006  803.45
  78   4.6888      1.080  16.7831     1.004  813.83
  79   4.6882      1.030  16.7816     0.988  824.21
  80   4.6893      1.050  16.7788     1.038  834.64
  81   4.6883      1.080  16.7846     1.000  845.00
  82   4.6886      1.110  16.7789     1.040  855.38
  83   4.6881      1.050  16.7818     0.998  865.70
  84   4.6887      1.050  16.7776     0.922  876.13
  85   4.6883      1.080  16.7852     1.000  886.48
