Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 9980121600 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.6699      1.000  16.7480     1.092  12.50
   2   4.6707      0.930  16.7529     0.994  23.24
   3   4.6697      1.040  16.7444     0.942  34.07
   4   4.6695      1.010  16.7500     0.978  44.84
   5   4.6695      1.040  16.7454     1.032  55.61
   6   4.6704      0.910  16.7453     1.034  66.37
   7   4.6700      1.020  16.7437     1.052  77.22
   8   4.6700      0.900  16.7453     0.992  87.97
   9   4.6695      0.980  16.7476     0.956  98.73
  10   4.6707      0.960  16.7444     0.940  109.58
  11   4.6715      0.990  16.7481     0.890  120.37
  12   4.6705      0.940  16.7513     0.996  131.11
  13   4.6708      0.920  16.7488     0.956  141.89
  14   4.6701      1.010  16.7519     1.022  152.62
  15   4.6710      0.910  16.7455     0.982  163.38
  16   4.6706      0.970  16.7497     1.072  174.17
  17   4.6703      0.990  16.7426     0.980  184.90
  18   4.6705      0.960  16.7453     0.970  195.62
  19   4.6712      0.930  16.7448     1.030  206.35
  20   4.6702      0.990  16.7511     1.004  217.18
  21   4.6709      0.940  16.7493     1.036  227.92
  22   4.6705      1.000  16.7485     0.988  238.66
  23   4.6704      1.010  16.7487     1.012  249.38
  24   4.6698      1.010  16.7495     0.992  260.14
  25   4.6711      1.000  16.7473     1.010  270.86
  26   4.6702      0.990  16.7492     1.018  281.61
  27   4.6725      1.000  16.7534     1.020  292.43
  28   4.6711      1.030  16.7473     1.030  303.16
  29   4.6713      0.960  16.7487     0.984  313.91
  30   4.6710      0.930  16.7494     0.948  324.70
  31   4.6710      0.950  16.7457     0.974  335.44
  32   4.6703      1.000  16.7458     0.962  346.19
  33   4.6708      0.990  16.7494     1.078  357.00
  34   4.6704      0.990  16.7505     1.024  367.77
  35   4.6711      1.020  16.7406     0.980  378.52
  36   4.6700      0.950  16.7474     0.924  389.27
  37   4.6700      0.970  16.7493     0.926  400.09
  38   4.6709      0.920  16.7479     0.984  410.86
  39   4.6697      0.950  16.7481     0.910  421.64
  40   4.6708      0.910  16.7480     0.986  432.45
  41   4.6713      0.930  16.7485     0.992  443.18
  42   4.6706      0.960  16.7490     1.038  453.91
  43   4.6696      0.960  16.7510     1.008  464.73
  44   4.6710      0.900  16.7496     0.970  475.46
  45   4.6698      0.960  16.7488     0.946  486.22
  46   4.6699      1.050  16.7465     0.988  496.98
  47   4.6717      1.020  16.7446     1.044  507.76
  48   4.6705      1.020  16.7427     1.076  518.47
  49   4.6707      0.940  16.7506     0.952  529.21
  50   4.6714      0.950  16.7456     0.962  540.02
  51   4.6697      1.040  16.7487     0.980  550.74
  52   4.6711      0.990  16.7480     0.912  561.49
  53   4.6702      1.000  16.7423     0.940  572.34
  54   4.6705      0.980  16.7510     0.912  583.08
  55   4.6708      0.980  16.7525     0.890  593.82
  56   4.6703      0.940  16.7455     0.946  604.61
  57   4.6708      1.020  16.7514     0.996  615.35
  58   4.6704      0.900  16.7473     1.032  626.10
  59   4.6707      0.970  16.7533     0.994  636.85
  60   4.6711      0.950  16.7490     0.968  647.67
  61   4.6715      0.950  16.7452     0.900  658.42
  62   4.6707      0.940  16.7513     0.918  669.15
  63   4.6709      0.970  16.7514     1.040  679.97
  64   4.6696      0.990  16.7482     0.984  690.73
  65   4.6707      0.910  16.7486     1.064  701.48
  66   4.6702      1.010  16.7505     0.984  712.23
  67   4.6698      1.030  16.7454     1.102  723.03
  68   4.6705      0.950  16.7501     1.022  733.78
  69   4.6694      1.020  16.7449     1.048  744.50
  70   4.6696      0.970  16.7472     1.086  755.36
  71   4.6717      0.950  16.7438     1.054  766.12
  72   4.6692      1.010  16.7483     1.056  776.85
  73   4.6697      1.000  16.7470     0.994  787.65
  74   4.6706      0.970  16.7476     0.988  798.38
  75   4.6703      0.960  16.7517     0.914  809.14
  76   4.6705      0.970  16.7429     0.980  819.89
  77   4.6698      0.980  16.7447     0.972  830.66
  78   4.6707      0.970  16.7469     0.968  841.40
  79   4.6701      1.020  16.7503     1.034  852.14
  80   4.6706      0.940  16.7396     0.992  863.00
  81   4.6699      1.010  16.7493     1.028  873.73
  82   4.6701      0.930  16.7438     0.948  884.49
  83   4.6709      0.930  16.7499     1.024  895.23
  84   4.6705      0.970  16.7501     0.978  906.05
  85   4.6702      0.940  16.7470     1.024  916.79
