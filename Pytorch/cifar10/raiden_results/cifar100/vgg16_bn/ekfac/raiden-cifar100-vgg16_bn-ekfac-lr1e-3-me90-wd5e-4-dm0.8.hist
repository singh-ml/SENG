Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14294487040 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.3587     18.480  3.9624     9.698  65.48
   2   2.8060     28.920  3.2020    20.644  129.02
   3   2.5356     33.950  2.7902    28.310  192.48
   4   2.2743     39.580  2.5251    33.888  255.87
   5   2.1794     42.130  2.3136    38.474  319.31
   6   2.0448     45.320  2.1497    42.102  382.80
   7   1.9480     47.450  1.9966    45.632  446.24
   8   1.8661     49.730  1.8790    48.170  509.69
   9   1.8068     50.980  1.7595    51.330  573.14
  10   1.7418     52.300  1.6607    53.536  636.61
  11   1.7222     53.070  1.5754    55.918  701.17
  12   1.6623     54.850  1.4930    57.788  764.73
  13   1.6584     54.880  1.4120    59.858  828.15
  14   1.6298     55.640  1.3496    61.524  891.62
  15   1.6094     56.910  1.2838    63.092  955.11
  16   1.5816     57.300  1.2169    65.032  1018.62
  17   1.6070     57.570  1.1716    65.940  1082.38
  18   1.6378     56.970  1.1035    67.696  1146.06
  19   1.5856     58.340  1.0485    69.102  1209.67
  20   1.5716     58.560  1.0032    70.402  1273.17
  21   1.6081     58.320  0.9578    71.244  1336.61
  22   1.5841     58.870  0.9136    72.492  1400.09
  23   1.6191     59.060  0.8692    73.786  1463.57
  24   1.5836     59.590  0.8249    74.960  1527.05
  25   1.6105     59.540  0.7946    75.916  1590.52
  26   1.5924     60.230  0.7545    76.966  1654.12
  27   1.6146     60.590  0.7275    77.864  1717.62
  28   1.6322     60.260  0.6905    78.824  1781.15
  29   1.6427     60.970  0.6458    80.072  1844.60
  30   1.6650     60.090  0.6229    80.674  1908.10
  31   1.6367     61.050  0.5949    81.392  1971.58
  32   1.6684     61.520  0.5638    82.376  2035.14
  33   1.7375     61.070  0.5356    83.326  2098.59
  34   1.7136     61.320  0.5145    83.870  2162.02
  35   1.7558     60.540  0.4909    84.516  2225.46
  36   1.6671     62.790  0.4728    85.068  2288.91
  37   1.7575     62.030  0.4447    85.946  2352.37
  38   1.7509     62.430  0.4227    86.576  2415.82
  39   1.7549     62.140  0.4158    86.772  2479.32
  40   1.7948     62.070  0.3927    87.412  2542.76
  41   1.8050     62.170  0.3795    87.860  2606.23
  42   1.8006     62.600  0.3492    88.744  2669.70
  43   1.8632     61.540  0.3428    88.832  2733.09
  44   1.8418     62.540  0.3288    89.456  2796.56
  45   1.8644     62.710  0.3170    90.070  2859.95
  46   1.8790     62.290  0.3100    90.080  2923.36
  47   1.9059     62.360  0.2958    90.720  2986.82
  48   1.9301     62.130  0.2774    91.066  3050.28
  49   1.9437     61.950  0.2727    91.314  3113.70
  50   1.9664     62.200  0.2630    91.570  3156.23
  51   1.9557     62.400  0.2500    91.888  3219.63
  52   1.9991     61.860  0.2470    92.250  3283.23
  53   1.9586     62.880  0.2330    92.302  3346.67
  54      nan      1.000     nan     7.130  3409.80
