Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar100', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14294487040 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.3008     18.790  3.9661     9.368  65.52
   2   2.7459     29.370  3.1813    21.118  128.78
   3   2.4938     34.400  2.7662    29.002  192.08
   4   2.2874     38.540  2.4996    34.436  255.51
   5   2.1440     42.620  2.2872    39.064  318.83
   6   2.0165     45.190  2.1124    43.324  382.15
   7   1.9345     47.140  1.9769    46.226  445.48
   8   1.8537     49.340  1.8615    48.870  508.80
   9   1.8006     51.120  1.7474    51.350  572.09
  10   1.7634     51.650  1.6509    53.864  635.38
  11   1.7358     53.330  1.5646    55.878  698.88
  12   1.6677     54.480  1.4868    57.976  762.20
  13   1.6679     55.300  1.4145    59.588  825.48
  14   1.6331     55.990  1.3506    61.306  888.79
  15   1.6401     56.300  1.2731    63.132  952.10
  16   1.6001     57.060  1.2174    64.608  1015.36
  17   1.6070     56.890  1.1635    66.094  1078.68
  18   1.6019     58.130  1.1042    67.696  1142.02
  19   1.5731     58.630  1.0569    68.654  1205.40
  20   1.5459     59.250  1.0031    70.368  1268.67
  21   1.5752     59.000  0.9610    71.556  1332.04
  22   1.5675     59.490  0.9195    72.630  1395.44
  23   1.5554     59.510  0.8692    73.726  1458.71
  24   1.6027     59.020  0.8312    74.636  1522.01
  25   1.5946     59.860  0.7990    75.848  1585.36
  26   1.6157     60.060  0.7528    76.920  1648.67
  27   1.5929     61.010  0.7295    77.510  1711.99
  28   1.5881     60.450  0.6811    79.024  1775.39
  29   1.5717     61.330  0.6619    79.718  1838.69
  30   1.6949     59.930  0.6204    80.654  1901.97
  31   1.6571     60.920  0.5944    81.366  1965.30
  32   1.6956     61.030  0.5699    82.016  2028.61
  33   1.6841     60.910  0.5421    83.174  2091.96
  34   1.7050     61.620  0.5064    84.024  2155.18
  35   1.7005     61.070  0.4965    84.276  2218.43
  36   1.7288     60.950  0.4665    85.180  2281.89
  37   1.7385     61.360  0.4445    86.002  2345.18
  38   1.7866     61.330  0.4357    86.244  2408.49
  39   1.7965     61.150  0.4124    86.846  2471.78
  40   1.8144     61.670  0.3926    87.570  2535.12
  41   1.8106     61.100  0.3856    87.920  2598.38
  42   1.8022     61.960  0.3594    88.378  2661.80
  43   1.9045     60.880  0.3442    88.914  2725.18
  44   1.8185     62.460  0.3300    89.432  2788.45
  45   1.8649     61.710  0.3166    89.938  2851.81
  46   1.9110     62.100  0.3040    90.308  2915.11
  47   1.9061     61.580  0.2988    90.444  2978.41
  48   1.9359     61.820  0.2785    91.106  3041.74
  49   1.9481     61.790  0.2697    91.260  3105.20
  50   1.9013     62.170  0.2674    91.610  3147.68
  51   1.9948     61.940  0.2477    92.010  3210.95
  52   1.9636     62.720  0.2450    92.178  3274.28
  53   1.9415     61.930  0.2381    92.370  3337.70
  54   1.9907     62.820  0.2239    92.826  3401.14
  55   2.0041     62.710  0.2202    92.898  3464.56
  56   1.9805     62.810  0.2148    93.186  3527.99
  57   1.9709     62.640  0.2087    93.514  3591.24
  58      nan      1.000     nan    14.816  3654.29
