Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.1065     96.530  0.3555    88.585  25.53
   2   0.1099     96.760  0.1299    96.018  48.64
   3   0.1110     96.590  0.1105    96.578  71.73
   4   0.1187     96.300  0.1070    96.683  94.78
   5   0.0931     97.210  0.0985    97.000  117.87
   6   0.1062     96.890  0.0971    97.032  140.96
   7   0.0855     97.710  0.0982    97.078  164.03
   8   0.1120     96.560  0.0959    97.277  187.11
   9   0.0893     97.270  0.0905    97.403  210.21
  10   0.0756     97.630  0.0923    97.355  233.28
  11   0.1045     96.750  0.0865    97.540  256.30
  12   0.0698     97.630  0.0803    97.628  279.33
  13   0.1615     95.620  0.0794    97.715  302.44
  14   0.0658     97.760  0.0734    97.795  325.53
  15   0.0665     97.770  0.0714    97.843  348.61
  16   0.1213     96.140  0.0701    97.985  371.70
  17   0.0735     97.670  0.0675    98.000  394.80
  18   0.0751     97.600  0.0667    98.050  417.89
  19   0.0669     97.810  0.0645    98.120  441.00
  20   0.0619     98.010  0.0623    98.175  464.07
  21   0.0799     97.590  0.0632    98.112  487.21
  22   0.0589     98.210  0.0592    98.280  510.31
  23   0.0735     97.840  0.0593    98.232  533.41
  24   0.0646     98.180  0.0593    98.238  556.54
  25   0.0363     98.920  0.0554    98.330  579.67
  26   0.0699     98.020  0.0578    98.253  602.76
  27   0.0521     98.380  0.0534    98.492  625.88
  28   0.0761     97.800  0.0536    98.482  649.00
  29   0.0542     98.240  0.0526    98.452  672.12
  30   0.0389     98.640  0.0537    98.433  695.24
  31   0.0468     98.570  0.0493    98.533  718.37
  32   0.0506     98.320  0.0505    98.485  741.50
  33   0.0462     98.540  0.0481    98.592  764.64
  34   0.0680     97.910  0.0495    98.525  787.78
  35   0.0665     98.110  0.0496    98.522  810.88
  36   0.0398     98.770  0.0494    98.515  834.00
  37   0.0620     98.000  0.0462    98.608  857.12
  38   0.0409     98.680  0.0460    98.637  880.27
  39   0.0538     98.320  0.0474    98.595  903.42
  40   0.0532     98.280  0.0475    98.600  926.54
  41   0.0440     98.780  0.0458    98.647  949.67
  42   0.0370     98.840  0.0458    98.638  972.79
  43   0.0462     98.490  0.0452    98.633  995.93
