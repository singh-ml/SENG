Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.1619     94.840  0.4216    86.055  25.47
   2   0.1154     96.610  0.1381    95.713  48.61
   3   0.1216     96.440  0.1172    96.425  71.76
   4   0.1157     96.050  0.1061    96.767  94.94
   5   0.1137     96.480  0.0995    96.965  118.12
   6   0.0743     97.690  0.0974    97.123  141.32
   7   0.0745     97.740  0.0922    97.243  164.49
   8   0.0942     97.060  0.0898    97.337  187.62
   9   0.0764     97.580  0.0922    97.217  210.86
  10   0.0572     98.280  0.0828    97.550  234.05
  11   0.0709     97.750  0.0781    97.592  257.21
  12   0.0638     97.910  0.0778    97.708  280.37
  13   0.0727     97.770  0.0746    97.808  303.57
  14   0.0746     97.640  0.0742    97.782  326.72
  15   0.0486     98.480  0.0670    98.032  349.86
  16   0.0609     98.130  0.0644    98.135  373.03
  17   0.0641     98.120  0.0688    97.968  396.18
  18   0.0689     97.760  0.0640    98.068  419.34
  19   0.0916     97.160  0.0620    98.175  442.48
  20   0.0614     97.990  0.0572    98.307  465.61
  21   0.0549     98.130  0.0609    98.183  488.76
  22   0.0541     98.340  0.0594    98.257  511.93
  23   0.0710     97.850  0.0570    98.357  535.15
  24   0.0527     98.370  0.0550    98.423  558.32
  25   0.0837     97.450  0.0540    98.377  581.47
  26   0.0465     98.740  0.0535    98.447  604.64
  27   0.0591     98.210  0.0535    98.418  627.80
  28   0.0519     98.430  0.0533    98.438  650.93
  29   0.0494     98.510  0.0537    98.432  674.08
  30   0.0476     98.560  0.0511    98.530  697.25
  31   0.0501     98.500  0.0500    98.495  720.41
  32   0.0400     98.740  0.0495    98.523  743.57
  33   0.0631     98.050  0.0466    98.617  766.71
  34   0.0646     97.810  0.0457    98.695  789.84
  35   0.0615     98.110  0.0478    98.590  813.01
  36   0.0576     98.170  0.0500    98.558  836.15
  37   0.0574     98.190  0.0473    98.628  859.31
  38   0.0443     98.620  0.0463    98.627  882.48
  39   0.0491     98.470  0.0492    98.575  905.62
  40   0.0615     98.250  0.0452    98.668  928.77
  41   0.0496     98.510  0.0449    98.600  951.92
