Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.5307     82.320  1.1422    63.485  25.43
   2   0.1856     94.000  0.2430    92.497  48.62
   3   0.4216     86.140  0.1805    94.473  71.75
   4   0.1111     96.510  0.1467    95.465  94.88
   5   0.1362     95.810  0.1302    96.070  118.03
   6   0.0661     98.070  0.1142    96.480  141.16
   7   0.0967     96.990  0.1036    96.870  164.30
   8   0.1311     96.000  0.0983    96.982  187.45
   9   0.0832     97.310  0.0980    96.930  210.56
  10   0.0753     97.580  0.0939    97.208  233.71
  11   0.0745     97.650  0.0900    97.282  256.85
  12   0.1332     95.750  0.0879    97.293  279.95
  13   0.1095     96.680  0.0874    97.315  303.10
  14   0.0803     97.550  0.0877    97.360  326.21
  15   0.0828     97.130  0.0859    97.422  349.40
  16   0.0927     97.010  0.0827    97.445  372.55
  17   0.1389     95.290  0.0845    97.460  395.68
  18   0.0981     96.990  0.0808    97.587  418.86
  19   0.0839     97.230  0.0820    97.483  441.98
  20   0.0761     97.570  0.0829    97.413  465.08
  21   0.0898     97.050  0.0777    97.595  488.23
  22   0.0800     97.600  0.0784    97.537  511.35
  23   0.0778     97.490  0.0794    97.563  534.47
  24   0.1323     95.660  0.0776    97.538  557.66
  25   0.1145     96.300  0.0782    97.592  580.83
  26   0.0601     98.080  0.0784    97.608  603.95
  27   0.0943     97.030  0.0769    97.665  627.10
  28   0.0635     97.910  0.0758    97.687  650.23
  29   0.1073     96.480  0.0766    97.643  673.36
  30   0.0796     97.450  0.0738    97.768  696.50
  31   0.0729     97.630  0.0735    97.767  719.60
  32   0.0659     97.860  0.0760    97.657  742.73
  33   0.0745     97.630  0.0747    97.708  765.85
  34   0.0597     98.010  0.0743    97.707  788.96
  35   0.0702     97.690  0.0744    97.810  812.12
  36   0.0784     97.520  0.0765    97.623  835.26
  37   0.0956     96.840  0.0735    97.757  858.39
