Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.1295     95.770  0.3583    88.423  25.40
   2   0.0984     96.840  0.1300    96.035  48.45
   3   0.0917     97.410  0.1189    96.408  71.51
   4   0.0950     97.040  0.1070    96.737  94.55
   5   0.0840     97.260  0.0955    97.147  117.59
   6   0.0659     97.960  0.0880    97.342  140.60
   7   0.0604     98.260  0.0784    97.620  163.64
   8   0.0883     97.170  0.0773    97.682  186.67
   9   0.0866     97.080  0.0702    97.893  209.74
  10   0.0559     98.110  0.0663    97.993  232.79
  11   0.0746     97.540  0.0634    98.068  255.81
  12   0.0849     97.400  0.0603    98.200  278.86
  13   0.0510     98.360  0.0589    98.212  301.90
  14   0.0520     98.300  0.0576    98.232  324.96
  15   0.0417     98.760  0.0544    98.335  348.03
  16   0.0440     98.690  0.0524    98.415  371.13
  17   0.0760     97.540  0.0511    98.440  394.20
  18   0.0488     98.420  0.0519    98.443  417.28
  19   0.0564     98.170  0.0488    98.548  440.36
  20   0.0581     98.100  0.0487    98.502  463.39
  21   0.0510     98.280  0.0474    98.563  486.46
  22   0.0584     98.180  0.0467    98.562  509.49
  23   0.0390     98.760  0.0447    98.657  532.55
  24   0.0564     98.190  0.0447    98.637  555.59
  25   0.0379     98.830  0.0447    98.615  578.65
  26   0.0633     97.910  0.0441    98.625  601.71
  27   0.0405     98.690  0.0437    98.653  624.77
  28   0.0617     97.950  0.0424    98.703  647.87
  29   0.0397     98.630  0.0445    98.633  670.92
  30   0.0451     98.510  0.0411    98.728  694.03
  31   0.0364     98.810  0.0395    98.800  717.12
  32   0.0350     98.930  0.0411    98.733  740.22
  33   0.0437     98.570  0.0407    98.763  763.37
  34   0.0378     98.680  0.0399    98.768  786.45
  35   0.0359     98.840  0.0393    98.773  809.52
  36   0.0361     98.820  0.0388    98.838  832.59
  37   0.0459     98.500  0.0400    98.787  855.68
  38   0.0380     98.830  0.0376    98.815  878.74
  39   0.0417     98.690  0.0383    98.802  901.87
  40   0.0464     98.520  0.0379    98.797  924.95
  41   0.0396     98.810  0.0377    98.850  948.02
  42   0.0465     98.530  0.0375    98.878  971.13
  43   0.0432     98.630  0.0374    98.885  994.22
  44   0.0529     98.230  0.0363    98.902  1017.28
  45   0.0461     98.540  0.0362    98.880  1040.38
  46   0.0354     98.820  0.0363    98.885  1063.48
  47   0.0524     98.390  0.0365    98.858  1086.52
  48   0.0479     98.540  0.0347    98.957  1109.59
  49   0.0391     98.790  0.0356    98.890  1132.67
  50   0.0449     98.540  0.0362    98.900  1155.74
  51   0.0343     98.940  0.0350    98.923  1178.84
  52   0.0348     98.930  0.0365    98.880  1201.95
  53   0.0415     98.640  0.0342    98.938  1225.00
  54   0.0407     98.630  0.0352    98.925  1248.08
  55   0.0427     98.580  0.0333    98.948  1271.18
  56   0.0381     98.810  0.0331    98.958  1294.24
  57   0.0347     98.960  0.0340    98.938  1317.34
  58   0.0374     98.890  0.0339    98.933  1340.42
  59   0.0344     98.880  0.0330    98.982  1363.47
  60   0.0366     98.690  0.0328    98.992  1386.59
  61   0.0477     98.420  0.0344    98.940  1409.65
  62   0.0423     98.700  0.0345    98.898  1432.72
  63   0.0405     98.830  0.0322    99.005  1455.75
  64   0.0327     98.950  0.0345    98.932  1478.78
  65   0.0359     98.870  0.0325    98.973  1501.84
  66   0.0568     98.240  0.0320    99.002  1524.91
  67   0.0365     98.790  0.0333    98.963  1547.99
  68   0.0299     99.100  0.0329    98.963  1571.03
  69   0.0511     98.390  0.0331    98.975  1594.10
  70   0.0343     98.910  0.0321    99.027  1617.17
  71   0.0346     98.960  0.0330    98.983  1640.23
  72   0.0326     98.990  0.0314    99.020  1663.32
  73   0.0426     98.700  0.0338    98.980  1686.43
  74   0.0456     98.490  0.0327    99.012  1709.50
  75   0.0338     99.010  0.0319    99.027  1732.58
  76   0.0305     99.020  0.0313    98.980  1755.89
  77   0.0364     98.950  0.0326    99.053  1778.96
  78   0.0482     98.480  0.0312    99.027  1802.04
  79   0.0419     98.720  0.0316    99.010  1825.11
  80   0.0433     98.700  0.0328    98.975  1848.18
  81   0.0326     98.950  0.0305    99.055  1871.29
  82   0.0412     98.700  0.0322    99.028  1894.37
  83   0.0375     98.790  0.0328    98.993  1917.43
  84   0.0360     98.890  0.0304    99.050  1940.54
  85   0.0313     98.980  0.0314    99.023  1963.63
