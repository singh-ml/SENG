Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.1161     96.330  0.3771    87.578  25.43
   2   0.0868     97.310  0.1464    95.497  48.53
   3   0.2085     93.760  0.1298    95.968  71.62
   4   0.1040     97.000  0.1193    96.353  94.74
   5   0.1336     95.600  0.1070    96.715  117.83
   6   0.0769     97.790  0.0983    97.027  140.91
   7   0.1141     96.490  0.0894    97.307  164.05
   8   0.0880     97.280  0.0814    97.513  187.17
   9   0.0737     97.820  0.0777    97.713  210.27
  10   0.0532     98.470  0.0740    97.860  233.40
  11   0.0939     97.230  0.0682    97.977  256.50
  12   0.1021     96.950  0.0651    98.093  279.61
  13   0.0660     98.010  0.0646    98.033  302.74
  14   0.0450     98.550  0.0607    98.162  325.86
  15   0.0455     98.530  0.0589    98.257  349.00
  16   0.0717     97.800  0.0586    98.340  372.17
  17   0.0926     97.150  0.0586    98.288  395.27
  18   0.0729     97.640  0.0557    98.328  418.41
  19   0.0597     98.110  0.0543    98.417  441.53
  20   0.0503     98.560  0.0540    98.397  464.65
  21   0.0701     97.990  0.0533    98.420  487.76
  22   0.0410     98.760  0.0529    98.473  510.83
  23   0.0551     98.270  0.0509    98.483  533.93
  24   0.0620     98.140  0.0506    98.497  557.01
  25   0.0855     97.570  0.0488    98.575  580.10
  26   0.0456     98.730  0.0488    98.538  603.19
  27   0.0485     98.540  0.0492    98.540  626.33
  28   0.0495     98.430  0.0488    98.552  649.43
  29   0.0566     98.300  0.0473    98.630  672.55
  30   0.0453     98.730  0.0481    98.568  695.68
  31   0.0531     98.340  0.0476    98.585  718.79
  32   0.0670     97.870  0.0462    98.620  741.91
  33   0.0774     97.560  0.0466    98.543  765.00
  34   0.0598     98.160  0.0467    98.602  788.09
  35   0.0452     98.620  0.0446    98.668  811.22
  36   0.0404     98.820  0.0454    98.652  834.31
  37   0.0423     98.750  0.0458    98.695  857.40
  38   0.0432     98.590  0.0434    98.705  880.53
  39   0.0596     98.220  0.0437    98.710  903.63
  40   0.0416     98.660  0.0441    98.663  926.75
  41   0.0664     97.870  0.0436    98.705  949.86
  42   0.0318     99.060  0.0431    98.752  972.94
  43   0.0388     98.750  0.0442    98.742  996.06
