Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.1422     95.660  0.4670    84.158  25.44
   2   0.1243     96.100  0.1315    95.903  48.47
   3   0.1054     96.650  0.1162    96.412  71.50
   4   0.0636     97.860  0.0986    96.997  94.56
   5   0.0900     97.300  0.0891    97.247  117.59
   6   0.0885     97.280  0.0850    97.457  140.63
   7   0.0760     97.690  0.0802    97.565  163.65
   8   0.0633     97.980  0.0803    97.593  186.70
   9   0.0834     97.330  0.0713    97.917  209.77
  10   0.0631     98.080  0.0689    97.967  232.82
  11   0.0743     97.780  0.0626    98.145  255.86
  12   0.0593     98.140  0.0623    98.133  278.92
  13   0.0615     97.960  0.0619    98.215  301.94
  14   0.0751     97.580  0.0615    98.187  324.99
  15   0.0582     98.180  0.0577    98.333  348.06
  16   0.0664     98.030  0.0573    98.293  371.11
  17   0.0633     98.050  0.0549    98.388  394.17
  18   0.0352     98.930  0.0520    98.490  417.23
  19   0.0537     98.380  0.0524    98.477  440.26
  20   0.0550     98.300  0.0491    98.577  463.32
  21   0.0744     97.820  0.0496    98.503  486.38
  22   0.0727     97.810  0.0490    98.528  509.39
  23   0.0483     98.600  0.0486    98.577  532.44
  24   0.0504     98.350  0.0490    98.577  555.49
  25   0.0384     98.800  0.0459    98.607  578.52
  26   0.0499     98.490  0.0431    98.705  601.57
  27   0.0456     98.560  0.0426    98.713  624.64
  28   0.0609     98.150  0.0446    98.692  647.69
  29   0.0516     98.530  0.0472    98.597  670.75
  30   0.0407     98.770  0.0416    98.807  693.80
  31   0.0481     98.440  0.0400    98.775  716.85
  32   0.0430     98.690  0.0402    98.835  739.91
  33   0.0432     98.620  0.0410    98.802  762.95
  34   0.0412     98.700  0.0405    98.797  786.00
  35   0.0567     98.400  0.0384    98.885  809.11
  36   0.0414     98.810  0.0409    98.798  832.17
  37   0.0394     98.830  0.0386    98.872  855.81
  38   0.0451     98.620  0.0351    98.922  878.88
  39   0.0552     98.340  0.0398    98.823  901.93
  40   0.0434     98.620  0.0392    98.847  924.97
  41   0.0304     99.140  0.0382    98.880  948.05
  42   0.0545     98.410  0.0365    98.918  971.12
  43   0.0521     98.440  0.0368    98.922  994.16
  44   0.0550     98.350  0.0358    98.892  1017.21
  45   0.0661     98.030  0.0360    98.940  1040.32
  46   0.0505     98.510  0.0363    98.923  1063.41
  47   0.0441     98.790  0.0347    98.990  1086.46
  48   0.0397     98.790  0.0353    98.953  1109.51
  49   0.0339     98.860  0.0339    99.000  1132.54
  50   0.0452     98.580  0.0340    98.965  1155.60
  51   0.0394     98.710  0.0353    98.932  1178.65
