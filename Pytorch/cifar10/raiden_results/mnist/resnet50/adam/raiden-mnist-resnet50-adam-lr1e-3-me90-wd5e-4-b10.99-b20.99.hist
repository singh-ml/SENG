Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.2062     93.210  0.4345    85.693  25.37
   2   0.1187     96.500  0.1500    95.383  48.49
   3   0.1092     96.600  0.1256    96.165  71.57
   4   0.0860     97.350  0.1146    96.558  94.67
   5   0.1157     96.600  0.1059    96.812  117.76
   6   0.0877     97.260  0.0928    97.173  140.89
   7   0.0716     97.870  0.0851    97.455  163.98
   8   0.0564     98.080  0.0785    97.543  187.11
   9   0.0723     97.870  0.0738    97.747  210.22
  10   0.0572     98.340  0.0692    97.908  233.32
  11   0.0698     98.030  0.0696    97.910  256.46
  12   0.0671     97.760  0.0652    98.030  279.58
  13   0.0461     98.710  0.0606    98.162  302.68
  14   0.0581     98.080  0.0602    98.162  325.79
  15   0.0642     97.990  0.0581    98.242  348.89
  16   0.0538     98.330  0.0565    98.295  372.00
  17   0.0481     98.690  0.0542    98.422  395.13
  18   0.0428     98.650  0.0545    98.365  418.21
  19   0.0508     98.460  0.0535    98.367  441.33
  20   0.0495     98.630  0.0539    98.383  464.45
  21   0.0630     98.070  0.0501    98.505  487.55
  22   0.0736     97.710  0.0504    98.440  510.69
  23   0.0445     98.750  0.0499    98.523  533.82
  24   0.0709     97.880  0.0488    98.545  556.92
  25   0.0563     98.090  0.0476    98.568  580.06
  26   0.0760     97.830  0.0477    98.567  603.18
  27   0.0542     98.290  0.0458    98.622  626.29
  28   0.0699     97.810  0.0468    98.555  649.39
  29   0.0410     98.710  0.0444    98.678  672.54
  30   0.0631     98.030  0.0458    98.620  695.64
  31   0.0495     98.340  0.0479    98.567  718.72
  32   0.0577     98.160  0.0464    98.558  741.85
  33   0.0376     98.770  0.0458    98.612  764.96
  34   0.0572     98.130  0.0419    98.723  788.09
  35   0.0423     98.690  0.0441    98.647  811.19
  36   0.0617     98.140  0.0435    98.738  834.30
  37   0.0517     98.350  0.0425    98.720  857.43
  38   0.0388     98.700  0.0434    98.692  880.52
  39   0.0699     97.690  0.0435    98.693  903.66
  40   0.0389     98.740  0.0413    98.758  926.78
  41   0.0634     98.150  0.0406    98.813  949.87
  42   0.0335     98.940  0.0398    98.798  973.00
