Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 13828049408 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   0.1544     94.690  0.5117    82.662  25.64
   2   0.1026     96.590  0.1321    95.848  48.98
   3   0.1190     96.420  0.0990    96.917  72.28
   4   0.0683     97.790  0.0881    97.298  95.57
   5   0.0700     97.580  0.0844    97.487  118.87
   6   0.0658     97.880  0.0774    97.668  142.17
   7   0.0682     97.910  0.0793    97.588  165.49
   8   0.1144     96.520  0.0759    97.725  188.80
   9   0.0561     98.250  0.0744    97.732  212.08
  10   0.0592     98.210  0.0697    97.938  235.38
  11   0.0736     97.760  0.0696    97.937  258.67
  12   0.0766     97.840  0.0729    97.875  281.98
  13   0.0753     97.590  0.0720    97.933  305.31
  14   0.0526     98.540  0.0732    97.863  328.58
  15   0.0664     97.950  0.0681    97.993  351.94
  16   0.0973     97.090  0.0695    98.010  375.24
  17   0.0783     97.460  0.0707    97.947  398.53
  18   0.0588     98.340  0.0630    98.193  421.80
  19   0.0496     98.560  0.0622    98.170  445.12
  20   0.0712     97.860  0.0596    98.222  468.44
  21   0.0511     98.580  0.0611    98.247  491.72
  22   0.0465     98.630  0.0562    98.410  514.99
  23   0.0654     98.020  0.0604    98.238  538.27
  24   0.0550     98.200  0.0568    98.352  561.60
  25   0.0516     98.460  0.0575    98.327  584.88
  26   0.0592     98.370  0.0541    98.407  608.18
  27   0.0733     97.760  0.0519    98.523  631.45
  28   0.0490     98.520  0.0484    98.628  654.70
  29   0.0707     98.030  0.0509    98.537  678.03
  30   0.0547     98.380  0.0523    98.470  701.30
  31   0.0735     97.750  0.0530    98.425  724.57
  32   0.0581     98.310  0.0503    98.555  747.89
  33   0.0616     98.140  0.0490    98.613  771.18
  34   0.0400     98.840  0.0461    98.665  794.46
  35   0.0805     97.710  0.0467    98.612  817.76
  36   0.0664     97.910  0.0446    98.740  841.07
  37   0.0638     98.080  0.0463    98.647  864.35
  38   0.0580     98.260  0.0440    98.692  887.66
  39   0.0474     98.610  0.0438    98.718  910.93
  40   0.0549     98.230  0.0446    98.698  934.24
  41   0.0405     98.620  0.0432    98.725  957.54
  42   0.0432     98.510  0.0396    98.857  980.81
  43   0.0503     98.550  0.0425    98.758  1004.09
