Use GPU: 0 for training
==> Running with ['main_adahessian.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 32181790208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5191     54.320  1.8654    34.167  132.95
   2   0.7597     75.080  1.1849    62.303  261.26
   3   0.4450     85.750  0.6215    80.117  389.65
   4   0.3322     89.330  0.4006    86.900  518.08
   5   0.2330     92.460  0.2948    90.563  646.48
   6   0.1865     93.840  0.2237    92.745  774.86
   7   0.1495     94.970  0.1783    94.337  903.20
   8   0.1332     95.690  0.1431    95.508  1031.59
   9   0.1136     96.290  0.1188    96.198  1160.10
  10   0.1011     96.700  0.1039    96.750  1288.40
  11   0.0874     97.000  0.0909    97.183  1416.80
  12   0.0885     97.130  0.0819    97.422  1545.12
  13   0.0702     97.640  0.0717    97.730  1673.57
  14   0.0642     97.880  0.0673    97.908  1801.83
  15   0.0682     97.720  0.0628    98.033  1930.23
  16   0.0512     98.370  0.0561    98.272  2058.61
  17   0.0596     98.030  0.0544    98.342  2187.02
  18   0.0531     98.330  0.0498    98.392  2315.55
  19   0.0489     98.430  0.0487    98.468  2443.91
  20   0.0540     98.420  0.0458    98.552  2572.42
  21   0.0532     98.370  0.0448    98.565  2700.87
  22   0.0505     98.300  0.0420    98.650  2829.26
  23   0.0371     98.830  0.0416    98.730  2957.63
  24   0.0404     98.780  0.0386    98.775  3085.97
  25   0.0458     98.490  0.0371    98.833  3214.44
  26   0.0360     98.860  0.0370    98.823  3342.70
  27   0.0448     98.580  0.0339    98.943  3471.10
  28   0.0502     98.380  0.0343    98.902  3599.38
  29   0.0542     98.270  0.0322    98.922  3727.77
  30   0.0525     98.550  0.0328    98.940  3856.15
  31   0.0380     98.810  0.0316    98.945  3984.64
  32   0.0410     98.720  0.0303    99.038  4112.95
  33   0.0419     98.740  0.0299    99.012  4241.33
  34   0.0379     98.720  0.0282    99.082  4369.78
  35   0.0497     98.430  0.0276    99.130  4498.25
  36   0.0487     98.410  0.0274    99.133  4626.66
  37   0.0468     98.500  0.0265    99.127  4755.08
  38   0.0367     98.830  0.0246    99.190  4883.35
  39   0.0351     98.860  0.0231    99.225  5011.81
  40   0.0360     98.740  0.0232    99.242  5140.11
  41   0.0341     98.840  0.0220    99.260  5268.44
  42   0.0336     98.950  0.0213    99.305  5396.95
  43   0.0400     98.730  0.0218    99.297  5525.27
  44   0.0446     98.650  0.0209    99.297  5653.77
  45   0.0391     98.750  0.0191    99.367  5782.15
  46   0.0405     98.780  0.0183    99.347  5910.58
  47   0.0353     98.770  0.0192    99.360  6038.87
  48   0.0344     98.960  0.0186    99.342  6167.23
  49   0.0475     98.480  0.0161    99.470  6295.69
  50   0.0365     98.710  0.0168    99.457  6424.13
  51   0.0342     98.880  0.0148    99.493  6552.50
  52   0.0302     99.060  0.0148    99.477  6680.84
  53   0.0357     98.900  0.0151    99.452  6809.28
  54   0.0389     98.850  0.0125    99.613  6937.73
  55   0.0377     98.860  0.0128    99.573  7066.08
  56   0.0385     98.880  0.0132    99.573  7194.48
  57   0.0343     99.000  0.0114    99.605  7322.84
  58   0.0355     98.920  0.0108    99.632  7451.23
  59   0.0370     98.930  0.0104    99.663  7579.66
  60   0.0360     98.950  0.0101    99.677  7708.10
  61   0.0354     98.930  0.0098    99.637  7836.51
  62   0.0385     98.960  0.0087    99.715  7964.96
  63   0.0375     98.930  0.0080    99.725  8093.30
  64   0.0394     98.820  0.0088    99.712  8221.79
  65   0.0424     98.650  0.0077    99.738  8350.14
  66   0.0334     99.050  0.0091    99.697  8478.61
  67   0.0290     99.120  0.0071    99.758  8606.91
  68   0.0428     98.810  0.0071    99.738  8735.38
  69   0.0388     98.920  0.0060    99.813  8863.86
  70   0.0360     99.030  0.0066    99.792  8992.20
  71   0.0386     98.850  0.0060    99.807  9120.69
  72   0.0338     99.010  0.0057    99.842  9248.95
  73   0.0331     99.050  0.0053    99.825  9377.38
  74   0.0345     99.050  0.0051    99.835  9505.72
  75   0.0349     98.980  0.0047    99.852  9634.20
  76   0.0362     99.010  0.0049    99.840  9762.70
  77   0.0376     99.030  0.0047    99.852  9891.04
  78   0.0329     99.110  0.0047    99.855  10019.51
  79   0.0387     98.990  0.0042    99.870  10147.83
  80   0.0373     99.000  0.0044    99.845  10276.26
  81   0.0394     98.930  0.0042    99.880  10404.66
  82   0.0350     99.030  0.0040    99.867  10533.03
  83   0.0351     99.090  0.0041    99.873  10661.51
  84   0.0330     99.060  0.0037    99.883  10789.78
  85   0.0384     99.060  0.0037    99.897  10918.14
