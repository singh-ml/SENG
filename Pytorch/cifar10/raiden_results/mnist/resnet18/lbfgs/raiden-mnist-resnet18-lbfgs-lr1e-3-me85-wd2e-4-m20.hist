Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 3842544128 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.4058      9.130  4.9076     9.135  9.26
   2   2.4055      9.120  4.9083     9.097  16.92
   3   2.4052      9.090  4.9069     9.113  24.57
   4   2.4055      9.130  4.9072     9.120  32.56
   5   2.4068      9.130  4.9077     9.085  40.15
   6   2.4052      9.130  4.9078     9.100  47.88
   7   2.4052      9.140  4.9087     9.123  55.52
   8   2.4059      9.110  4.9075     9.110  63.18
   9   2.4051      9.110  4.9086     9.157  70.82
  10   2.4052      9.110  4.9081     9.132  78.63
  11   2.4057      9.110  4.9075     9.138  86.29
  12   2.4058      9.090  4.9080     9.155  93.90
  13   2.4068      9.120  4.9079     9.158  101.49
  14   2.4056      9.120  4.9082     9.112  109.01
  15   2.4056      9.110  4.9074     9.105  116.64
  16   2.4057      9.110  4.9076     9.067  124.25
  17   2.4063      9.120  4.9073     9.073  131.88
  18   2.4061      9.100  4.9074     9.162  139.50
  19   2.4059      9.110  4.9067     9.097  147.19
  20   2.4061      9.110  4.9078     9.155  154.89
  21   2.4053      9.130  4.9073     9.113  162.39
  22   2.4055      9.130  4.9074     9.150  169.95
  23   2.4056      9.100  4.9071     9.170  177.60
  24   2.4063      9.100  4.9071     9.057  185.25
  25   2.4062      9.120  4.9066     9.083  192.82
  26   2.4053      9.090  4.9076     9.160  200.39
  27   2.4050      9.110  4.9082     9.112  207.92
  28   2.4053      9.130  4.9074     9.107  215.56
  29   2.4048      9.120  4.9078     9.085  223.28
  30   2.4046      9.110  4.9075     9.103  230.81
  31   2.4053      9.110  4.9085     9.110  238.43
  32   2.4044      9.110  4.9071     9.090  246.01
  33   2.4050      9.100  4.9076     9.085  253.62
  34   2.4052      9.120  4.9068     9.162  261.35
  35   2.4058      9.110  4.9074     9.123  269.01
  36   2.4055      9.100  4.9077     9.118  276.72
  37   2.4055      9.110  4.9073     9.118  284.37
  38   2.4058      9.130  4.9076     9.083  292.38
  39   2.4051      9.120  4.9078     9.107  299.98
  40   2.4056      9.140  4.9071     9.130  307.63
  41   2.4060      9.090  4.9074     9.133  315.25
  42   2.4048      9.110  4.9075     9.090  322.87
  43   2.4056      9.120  4.9079     9.110  330.60
  44   2.4054      9.090  4.9076     9.055  338.13
  45   2.4054      9.150  4.9068     9.143  345.83
  46   2.4062      9.110  4.9075     9.062  353.56
  47   2.4059      9.100  4.9074     9.083  361.35
  48   2.4052      9.110  4.9076     9.060  369.01
  49   2.4058      9.110  4.9080     9.077  376.56
  50   2.4055      9.090  4.9067     9.155  384.18
  51   2.4046      9.100  4.9073     9.080  391.95
  52   2.4056      9.110  4.9082     9.108  399.61
  53   2.4055      9.120  4.9081     9.085  407.22
  54   2.4051      9.120  4.9070     9.080  414.75
  55   2.4051      9.130  4.9069     9.095  422.32
  56   2.4060      9.110  4.9070     9.115  430.10
  57   2.4051      9.110  4.9080     9.168  437.71
  58   2.4061      9.100  4.9072     9.157  445.24
  59   2.4047      9.120  4.9080     9.083  452.92
  60   2.4046      9.110  4.9071     9.128  460.53
  61   2.4057      9.120  4.9072     9.107  468.09
  62   2.4053      9.130  4.9078     9.073  475.69
  63   2.4060      9.110  4.9069     9.100  483.33
  64   2.4053      9.130  4.9073     9.128  491.11
  65   2.4048      9.130  4.9081     9.108  498.88
  66   2.4055      9.110  4.9082     9.120  506.46
  67   2.4053      9.090  4.9082     9.092  514.01
  68   2.4055      9.110  4.9083     9.093  521.55
  69   2.4056      9.100  4.9079     9.127  529.11
  70   2.4055      9.110  4.9075     9.085  536.86
  71   2.4055      9.100  4.9067     9.127  544.57
  72   2.4056      9.120  4.9079     9.077  552.13
  73   2.4047      9.130  4.9078     9.138  559.64
  74   2.4053      9.120  4.9068     9.145  567.21
  75   2.4052      9.080  4.9076     9.100  575.01
  76   2.4049      9.090  4.9072     9.152  582.63
  77   2.4056      9.140  4.9080     9.115  590.25
  78   2.4048      9.120  4.9082     9.107  597.83
  79   2.4055      9.110  4.9086     9.068  605.49
  80   2.4048      9.130  4.9080     9.088  613.07
  81   2.4058      9.120  4.9072     9.108  620.71
  82   2.4052      9.100  4.9076     9.102  628.39
  83   2.4050      9.110  4.9068     9.143  636.23
  84   2.4056      9.120  4.9080     9.163  643.82
  85   2.4059      9.100  4.9077     9.113  651.44
