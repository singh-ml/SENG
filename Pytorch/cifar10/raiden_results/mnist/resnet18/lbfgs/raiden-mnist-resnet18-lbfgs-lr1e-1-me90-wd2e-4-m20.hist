Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 4701547008 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7299     39.950  4.5879    23.358  10.90
   2   0.6370     80.300  4.1823    56.642  20.37
   3   10.1528     11.360  4.2652    75.920  29.64
   4   0.4376     85.420  3.0078    83.047  39.27
   5   1.2609     56.000  3.0716    83.888  48.84
   6   0.3538     88.310  3.4593    81.583  58.21
   7   0.3173     89.710  2.8539    88.103  67.49
   8   0.4753     82.400  3.1640    86.457  76.81
   9   0.2781     91.250  2.8937    87.612  86.10
  10   0.2788     91.230  2.8006    90.145  95.27
  11   0.2784     91.270  2.8017    90.097  104.28
  12   0.2780     91.310  2.8003    90.133  113.23
  13   0.2789     91.150  2.8006    90.120  122.19
  14   0.2763     91.290  2.8011    90.240  131.30
  15   0.2792     91.110  2.8004    90.172  140.25
  16   0.2779     91.210  2.7983    90.167  149.25
  17   0.2785     91.330  2.8002    90.133  158.28
  18   0.2779     91.180  2.7976    90.175  167.44
  19   0.2790     91.110  2.8015    90.190  176.46
  20   0.2800     91.110  2.8014    90.148  185.52
  21   0.2805     91.030  2.8014    90.142  194.67
  22   0.2777     91.240  2.7997    90.193  203.67
  23   0.2775     91.290  2.8004    90.078  212.63
  24   0.2776     91.280  2.8010    90.178  221.61
  25   0.2774     91.310  2.7999    90.225  230.81
  26   0.2785     91.190  2.8003    90.140  239.84
  27   0.2767     91.320  2.8034    90.202  248.89
  28   0.2783     91.140  2.8012    90.153  257.91
  29   0.2790     91.330  2.8016    90.155  267.34
  30   0.2789     91.070  2.8043    90.138  276.44
  31   0.2771     91.230  2.8005    90.238  285.42
  32   0.2774     91.260  2.7981    90.248  294.45
  33   0.2786     91.250  2.8000    90.140  303.57
  34   0.2774     91.220  2.7998    90.233  312.82
  35   0.2768     91.330  2.7995    90.207  321.85
  36   0.2776     91.260  2.8015    90.098  330.95
  37   0.2770     91.270  2.8024    90.153  339.93
  38   0.2784     91.170  2.7999    90.147  348.92
  39   0.2777     91.250  2.8033    90.128  357.94
  40   0.2782     91.210  2.8012    90.128  366.95
  41   0.2779     91.120  2.7999    90.195  376.10
  42   0.2772     91.180  2.8006    90.193  385.15
  43   0.2781     91.120  2.7997    90.222  394.26
  44   0.2775     91.240  2.8026    90.153  403.14
  45   0.2785     91.150  2.7997    90.127  412.25
  46   0.2768     91.240  2.7986    90.198  421.33
  47   0.2778     91.160  2.7994    90.290  430.36
  48   0.2769     91.310  2.7993    90.190  439.36
  49   0.2781     91.300  2.8001    90.245  448.55
  50   0.2783     91.350  2.7993    90.148  457.56
  51   0.2777     91.300  2.7997    90.103  466.50
  52   0.2778     91.230  2.7999    90.155  475.55
  53   0.2787     91.150  2.7988    90.288  484.66
  54   0.2798     91.150  2.7997    90.103  493.72
  55   0.2773     91.350  2.8011    90.217  502.78
  56   0.2782     91.250  2.8007    90.135  511.82
  57   0.2771     91.280  2.7979    90.217  520.92
  58   0.2778     91.310  2.7999    90.072  530.26
  59   0.2777     91.280  2.8007    90.180  539.24
  60   0.2782     91.300  2.8011    90.095  548.36
  61   0.2778     91.290  2.7999    90.170  557.32
  62   0.2794     91.150  2.7990    90.207  566.34
  63   0.2780     91.190  2.8017    90.215  575.32
  64   0.2776     91.210  2.8004    90.285  584.73
  65   0.2777     91.290  2.8024    90.087  593.79
  66   0.2764     91.280  2.7993    90.297  602.80
  67   0.2778     91.250  2.8010    90.223  611.86
  68   0.2784     91.190  2.7991    90.290  620.83
  69   0.2776     91.260  2.7995    90.278  629.85
  70   0.2776     91.360  2.8003    90.262  638.79
  71   0.2774     91.200  2.8020    90.203  648.12
  72   0.2800     91.060  2.8017    89.985  657.32
  73   0.2762     91.250  2.8000    90.215  666.35
  74   0.2776     91.190  2.8002    90.170  675.30
  75   0.2777     91.320  2.7992    90.195  684.33
  76   0.2784     91.220  2.8005    90.210  693.58
  77   0.2781     91.390  2.7989    90.223  702.62
  78   0.2778     91.240  2.7991    90.152  711.61
  79   0.2775     91.180  2.8008    90.198  720.72
  80   0.2772     91.170  2.8014    90.100  729.85
  81   0.2778     91.330  2.7991    90.223  738.86
  82   0.2782     91.300  2.7994    90.232  747.89
  83   0.2774     91.090  2.8003    90.307  756.93
  84   0.2783     91.280  2.7999    90.237  765.92
  85   0.2770     91.270  2.7976    90.218  774.98
  86   0.2790     91.070  2.7979    90.187  783.94
  87   0.2765     91.250  2.7986    90.327  793.07
  88   0.2781     91.220  2.8001    90.212  802.04
  89   0.2782     91.250  2.7983    90.178  811.07
  90   0.2784     91.050  2.7990    90.205  820.04
