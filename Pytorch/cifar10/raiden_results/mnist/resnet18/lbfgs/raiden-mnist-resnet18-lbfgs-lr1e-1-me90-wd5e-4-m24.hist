Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 5058858496 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.0590     28.290  8.4708    16.703  11.16
   2   4.6124     13.720  8.2259    37.845  20.87
   3   0.4735     83.790  7.0700    70.422  30.39
   4   0.4199     85.790  6.5534    83.062  40.02
   5   0.4852     84.720  7.2799    82.328  49.59
   6   0.3422     89.310  6.3316    87.200  59.21
   7   0.3752     87.630  6.2493    88.422  68.79
   8   0.2748     91.430  6.1302    89.845  78.44
   9   0.2854     91.240  6.0182    88.617  88.00
  10   3.1527      9.420  583.8522    39.230  97.56
  11   24571591.8000     10.280  112260968.1322    14.737  107.17
  12   16074.2292      9.450  11453365.9098    10.043  116.95
  13   7798.1750      4.120  148641.5200     9.750  126.64
  14   7333.8075      9.190  145738.3351     9.108  136.18
  15   8397.5348     10.350  145134.5978    10.055  145.78
  16   16429882.7500      9.890  145612.0089     9.930  155.48
  17   6122156.0875     11.350  2905790563.5003    10.623  165.09
  18   115735.7156     10.100  135878384.4899     9.840  174.71
  19   1589542.1508     15.730  1742934213.8298    11.068  184.70
  20   217156.1257      7.960  11866403.2585    10.472  194.49
  21   156435.5866     12.350  3400482.8330    10.435  204.20
  22   104810.0947      9.580  3305507.9181     9.800  213.86
  23   86701.3032      8.940  3298556.9351    10.292  223.48
  24   68906.5101      8.750  3286418.7298    11.008  233.24
  25   98029.2860     14.830  3286460.4617    11.623  242.87
  26   83885.7163     10.460  3247892.5755    12.075  252.51
  27   109576.1587      9.580  3216075.8691    11.248  262.28
  28   673036.8222     12.580  3213933.5574    10.980  272.01
  29   257303.0975      8.160  3180059.2447     9.958  281.65
  30   94598.8465      5.380  3178489.6500     8.690  291.24
  31   274424.6583      9.860  3159447.6436     8.667  300.69
  32   237275.3878      6.410  3179620.7245     8.773  310.03
  33   153681.7457      5.100  3175585.8074     8.822  319.30
  34   158231.7751      8.430  3180605.9926     8.885  328.66
  35   168424.6263      5.480  3180948.2628     8.798  337.94
  36   151562.6478      6.270  3177944.3489     8.802  347.19
  37   225509.0958      8.720  3179285.9755     8.793  356.52
  38   179970.8534      6.220  3169763.8968     8.728  366.17
  39   246379.9571      5.850  3178675.7394     8.765  375.49
  40   174907.4360      5.410  3182155.5979     8.900  384.79
  41   368882.5579     11.060  3168233.7426     8.855  394.19
  42   193559.2324      8.320  3177074.8074     8.662  403.67
  43   152486.4313      7.900  3171691.5702     8.755  413.01
  44   140919.9775      5.250  3175081.7691     8.830  422.34
  45   229356.3934      7.190  3166987.0798     8.673  431.75
  46   111588.2390      5.500  3176443.5989     8.727  441.07
  47   169814.9287      7.410  3179240.6064     8.587  450.27
  48   253820.1350     11.110  3172981.7830     8.623  459.67
  49   222025.9184      7.420  3172023.8532     8.808  468.93
  50   197775.7560      6.260  3174311.7160     8.678  478.28
  51   162908.5455      7.070  3191730.3457     8.893  487.55
  52   183245.4741      6.460  3179886.8479     8.877  496.82
  53   192373.3362      6.860  3181009.4404     8.730  506.52
  54   174579.1581      5.370  3179338.6830     8.755  515.92
  55   202032.7017      5.500  3181204.9915     8.710  525.20
  56   150893.9722      5.790  3184496.1319     8.750  534.51
  57   191034.9526      7.300  3176982.4840     8.837  544.01
  58   246512.9833      8.530  3188303.2745     8.837  553.30
  59   237799.8232      7.390  3179050.0915     8.763  562.61
  60   156504.6737      6.680  3179661.2702     8.727  571.91
  61   169094.0957      6.330  3179582.3043     8.685  581.31
  62   185763.7915      5.720  3172848.8511     8.723  590.55
  63   174270.9617      5.980  3184111.0766     8.778  599.88
  64   129980.1762      5.280  3187783.6809     8.808  609.21
  65   133883.8838      5.500  3188856.6128     8.763  618.69
  66   290764.3817     10.890  3183349.6383     8.822  627.99
  67   224531.9675      7.670  3172347.9947     8.733  637.24
  68   308135.0555     11.190  3177042.8309     8.760  646.71
  69   198845.1546      7.360  3180291.5798     8.780  656.02
  70   210864.5871      7.800  3183103.9362     8.870  665.50
  71   272380.3579      7.520  3175150.5681     8.640  674.71
  72   190873.0165      6.930  3164409.1830     8.698  684.13
  73   242594.8525     10.750  3170878.9691     8.730  693.51
  74   224505.8568      5.930  3180826.7191     8.570  702.85
  75   235927.1254      9.160  3185121.6287     8.527  712.14
  76   181524.9144      5.610  3175364.4862     8.620  721.58
  77   127229.0844      5.610  3181832.5798     8.837  730.86
  78   215801.8548      7.100  3185240.4298     8.903  740.20
  79   158293.9795      5.470  3189117.8032     8.658  749.55
  80   191689.7568      5.470  3181569.7830     8.668  758.94
  81   190908.1609      5.770  3174726.1202     8.637  768.42
  82   274311.1500      7.840  3184983.9883     8.938  777.75
  83   200207.8022      5.040  3172566.1926     8.617  787.06
  84   178059.7801      6.220  3186801.2872     8.830  796.49
  85   161992.4598      5.150  3174765.3862     8.703  806.17
  86   218576.4049      6.260  3179401.0436     8.618  815.72
  87   101934.5402      5.480  3173842.1691     8.703  825.09
  88   122849.7741      5.200  3173977.0511     8.788  834.46
  89   205425.1128      5.350  3183791.3138     8.673  843.81
  90   239667.2430     10.690  3175224.0287     8.840  853.14
