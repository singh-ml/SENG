Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '20', '--trainset', 'mnist', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Memory peak: 4706003456 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   95.3404      9.800  4.5588    26.243  10.69
   2   25.0379      9.800  6320.0156    14.198  19.93
   3   125.9195      9.800  146.3241    11.212  29.11
   4   2.6417     15.060  8.7311    11.997  38.29
   5   2.4433     16.000  4.3397    13.710  47.40
   6   1496033925529.6001      8.920  6.7660    15.685  56.68
   7   2.1135     22.420  9.8467    21.163  65.92
   8   1.9392     27.210  4.1175    22.618  75.12
   9   168.6520     10.280  59215.5739    28.245  84.31
  10   2.3108     17.440  1160.8006    10.875  93.60
  11   2.0177     25.320  5.0935    18.940  102.83
  12   2.1266     17.700  39.6442    18.622  112.05
  13   1.3408     52.460  55.7980    25.248  121.42
  14   0.7774     73.020  3.6547    62.800  131.00
  15   0.5193     81.810  3.7524    65.970  140.11
  16   7411037.5500      9.580  103990551018.3326    41.300  149.29
  17   17776308.4500     10.620  51593438990.7532    12.825  158.55
  18   984476.6047     11.130  54610902.8085    13.882  167.86
  19   1004862.2516     11.060  51274810.7064    14.237  177.30
  20   1074846.3203     11.560  51347766.9957    14.288  186.49
  21   962752.8078     10.920  49412828.4596    14.347  195.60
  22   1163328.3848     13.670  50555381.7872    14.283  204.90
  23   823920.4605     12.090  50277756.9362    14.922  214.15
  24   817555.3723     11.740  49242111.4723    14.907  223.36
  25   835650.4824     11.530  49412349.6170    15.330  232.71
  26   809253.1969     11.840  49741423.1660    15.098  241.73
  27   720012.2723     11.640  49143576.2894    15.227  250.63
  28   793477.9449     11.700  48900236.1702    15.108  259.52
  29   769237.7973     12.580  48786203.8128    14.992  268.48
  30   781499.0211     11.840  48261759.4553    15.042  277.39
  31   652331.3166     11.820  48866670.9106    15.072  286.46
  32   799429.9955     12.010  49958877.6000    14.928  295.35
  33   815997.0141     11.810  48504300.4085    15.083  304.41
  34   857932.9086     11.920  49666507.9660    15.028  313.32
  35   987135.7672     12.120  50587257.4809    15.200  322.32
  36   853390.8691     11.670  48591032.9362    15.108  331.27
  37   752626.7053     11.820  48789235.7957    15.068  340.31
  38   801679.7199     11.640  48941785.6511    15.040  349.24
  39   849666.6143     12.200  49599760.0000    14.998  358.14
  40   763115.6041     12.050  49256972.1872    15.108  367.14
  41   901413.9824     12.040  49502571.5234    15.162  375.99
  42   814176.1895     11.890  48861746.8936    15.065  384.92
  43   869838.6307     12.330  50725176.2213    15.125  393.81
  44   777522.9713     11.760  49109478.6043    15.148  402.78
  45   851658.3938     11.820  51014463.0128    15.050  412.01
  46   723945.1248     11.660  50422810.6043    15.000  420.95
  47   945736.5352     12.420  49197308.0511    15.012  429.86
  48   778851.7727     11.410  50723288.6128    15.138  438.76
  49   662779.8100     11.590  49001303.7106    15.093  447.84
  50   939278.0664     12.110  48221295.2000    15.183  456.77
  51   788805.6521     11.830  49972045.4638    15.008  465.66
  52   745143.3209     11.890  49427629.2085    14.955  474.60
  53   817297.2680     12.000  50017140.5106    14.920  483.74
  54   768719.5311     11.800  50321608.9362    14.968  492.69
  55   853509.3266     11.800  48655944.8681    15.047  501.61
  56   955087.2262     11.860  49805881.7191    15.155  510.52
  57   794250.9605     11.680  48645993.9404    15.002  519.85
  58   730630.2740     11.750  49597808.1532    15.022  528.73
  59   951915.1344     12.040  50554847.6426    15.168  537.63
  60   747525.7602     12.010  48372648.0681    14.980  546.60
  61   745014.3520     11.830  48559812.9872    15.002  555.62
  62   827123.7367     12.170  48183354.8255    15.110  564.50
  63   786699.9754     12.080  48192285.1234    15.080  573.57
  64   786518.1199     11.690  48973586.5362    14.978  582.43
  65   747805.8414     12.020  50046164.3064    14.940  591.46
  66   849602.8230     12.090  48038259.8128    15.007  600.36
  67   882790.2992     11.530  49980728.6638    14.932  609.24
  68   808296.5594     11.990  50039947.7617    15.115  618.06
  69   761565.0531     12.080  49842560.9362    15.095  627.39
  70   757721.8002     11.640  49959791.0468    15.087  636.25
  71   728573.5869     11.800  49229568.4255    15.145  645.21
  72   771693.8885     11.980  49849856.3915    14.895  654.19
  73   719632.3467     11.870  49389068.4085    15.052  663.21
  74   830451.6293     11.600  48734219.0638    14.983  672.17
  75   905015.9961     11.860  48835152.4596    15.143  681.39
  76   766257.7141     11.890  49732204.2553    15.003  690.47
  77   875800.1156     11.900  48188535.2170    14.953  699.41
  78   834165.9297     12.150  50026803.8128    14.962  708.34
  79   894406.2789     12.050  49991678.9957    14.988  717.30
  80   790747.9859     11.710  49089143.7277    15.007  726.31
  81   806885.5334     12.060  49264165.3617    15.208  735.20
  82   926033.5969     12.120  49836023.9489    15.068  744.08
  83   771266.8766     11.920  49214792.5957    14.977  752.96
  84   705961.2195     11.760  49216469.4638    14.962  761.97
  85   769316.2994     11.610  49648004.9872    14.922  770.86
  86   840217.4242     11.990  49194690.1277    15.000  780.05
  87   893757.9516     11.920  49804724.2383    15.027  788.95
  88   811371.6844     12.160  49914108.8851    14.993  797.92
  89   847804.0164     11.760  49455807.5064    14.967  807.19
  90   706448.9301     11.670  48519561.9234    15.037  816.16
