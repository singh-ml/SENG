Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7829     26.890  2.3813    16.100  7.08
   2   1.4448     46.040  1.6219    36.432  12.49
   3   1.5331     48.070  1.2862    53.052  17.87
   4   1.0485     63.990  1.0587    63.164  23.26
   5   0.8746     70.180  0.9031    68.974  28.79
   6   0.8179     72.760  0.8164    72.270  34.17
   7   0.8028     74.080  0.7408    75.140  39.55
   8   0.7179     76.370  0.6767    77.772  44.97
   9   0.6695     78.730  0.6357    79.018  50.41
  10   0.9549     76.820  0.5904    80.550  55.83
  11   0.6624     79.070  0.5623    81.606  61.27
  12   0.5778     81.260  0.5473    81.932  66.80
  13   0.6315     79.990  0.4946    83.694  72.20
  14   0.6714     80.440  0.4902    84.088  77.62
  15   0.5743     81.720  0.4459    85.348  83.02
  16   0.5975     80.580  0.4648    84.962  88.41
  17   0.4942     83.930  0.4189    86.160  93.79
  18   0.6088     81.290  0.3952    86.790  99.19
  19   0.5459     83.590  0.4220    86.254  104.72
  20   0.5929     81.050  0.3609    88.066  110.13
  21   0.5484     83.000  0.3566    88.216  115.55
  22   0.5044     83.570  0.3636    88.126  120.96
  23   0.6805     80.740  0.3217    89.420  126.37
  24   0.5183     83.550  0.3110    89.784  131.78
  25   0.5010     84.570  0.3042    90.048  137.19
  26   0.5213     84.940  0.2961    90.376  142.73
  27   0.5094     85.030  0.2778    90.896  148.13
  28   0.4535     85.720  0.2674    91.162  153.55
  29   0.4756     85.420  0.2670    91.206  158.95
  30   0.5976     84.700  0.2516    91.808  164.35
  31   0.5128     84.200  0.2488    91.744  169.78
  32   0.5298     84.220  0.2363    92.122  175.21
  33   0.4716     86.450  0.2372    92.210  180.75
  34   0.4608     86.480  0.2292    92.536  186.15
  35   0.5354     85.480  0.2341    92.504  191.55
  36   0.4981     85.980  0.2050    93.204  196.95
  37   0.5231     85.600  0.2044    93.346  202.36
  38   0.4915     86.310  0.1990    93.540  207.76
  39   0.4686     86.900  0.1851    93.828  213.18
  40   0.4372     87.660  0.1904    93.684  218.70
  41   0.7724     80.040  0.1856    93.950  224.09
  42   0.4818     86.790  0.1767    94.272  229.51
  43   0.4857     86.850  0.1785    94.260  234.91
  44   0.4656     86.730  0.1642    94.634  240.31
  45   0.5009     87.110  0.1653    94.586  245.71
  46   0.4597     86.970  0.1581    94.864  251.21
  47   0.4775     86.680  0.1556    94.964  256.64
  48   0.4658     87.420  0.1590    94.798  262.08
  49   0.4554     87.450  0.1538    95.050  267.47
  50   0.5054     86.780  0.1476    95.220  272.86
  51   0.4811     87.870  0.1420    95.466  278.29
  52   0.4680     87.920  0.1413    95.574  283.72
  53   0.4663     87.840  0.1390    95.526  289.17
  54   0.4708     88.120  0.1291    95.824  294.57
  55   0.4603     87.740  0.1337    95.708  299.99
  56   0.4508     87.600  0.1292    95.912  305.39
  57   0.4560     87.610  0.1301    95.820  310.82
  58   0.4662     87.480  0.1256    95.906  316.23
  59   0.4968     86.960  0.1250    95.998  321.63
  60   0.4953     87.170  0.1244    96.044  327.11
  61   0.4641     88.080  0.1195    96.104  332.53
  62   0.4605     88.230  0.1150    96.358  337.93
  63   0.4390     88.480  0.1180    96.166  343.36
  64   0.4513     88.200  0.1110    96.334  348.78
  65   0.4686     88.240  0.1111    96.442  354.17
  66   0.4928     87.920  0.1134    96.368  359.60
  67   0.4896     88.300  0.1085    96.464  365.00
  68   0.4746     88.380  0.1079    96.466  370.56
  69   0.4656     88.210  0.1038    96.670  375.97
  70   0.4821     88.320  0.1034    96.708  381.38
  71   0.4941     87.740  0.1056    96.624  386.77
  72   0.4978     87.770  0.1021    96.796  392.18
  73   0.4847     88.710  0.1066    96.632  397.58
  74   0.5771     86.220  0.0983    96.970  403.00
  75   0.4255     89.290  0.0994    96.820  408.53
  76   0.4455     88.520  0.1000    96.800  413.93
  77   0.4499     88.980  0.0951    96.970  419.34
  78   0.5501     87.360  0.0958    96.960  424.78
  79   0.4574     88.710  0.0958    96.868  430.21
  80   0.4728     88.930  0.0913    97.088  435.64
  81   0.4578     88.360  0.0922    97.084  441.20
  82   0.4186     89.620  0.0920    97.104  446.61
  83   0.4581     89.200  0.0886    97.312  452.04
  84   0.4215     89.240  0.0878    97.136  457.47
  85   0.5095     88.530  0.0883    97.204  462.88
