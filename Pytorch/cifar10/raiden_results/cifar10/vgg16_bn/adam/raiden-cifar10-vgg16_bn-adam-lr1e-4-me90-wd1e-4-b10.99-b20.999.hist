Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8182     27.420  2.2092    18.192  7.09
   2   1.5568     38.570  1.7429    30.444  12.49
   3   1.3594     47.630  1.5061    41.842  17.92
   4   1.2254     53.640  1.3503    49.016  23.33
   5   1.1103     58.150  1.2197    55.230  28.73
   6   1.0022     65.070  1.0873    61.418  34.26
   7   0.9278     67.410  0.9804    65.456  39.71
   8   0.8194     71.890  0.9010    68.720  45.12
   9   0.7721     73.350  0.8235    71.590  50.51
  10   0.7353     74.530  0.7610    73.950  55.92
  11   0.7025     75.710  0.7083    75.800  61.34
  12   0.6631     77.440  0.6609    77.606  66.77
  13   0.6536     77.960  0.6060    79.562  72.30
  14   0.6479     77.860  0.5849    80.276  77.71
  15   0.5960     79.630  0.5573    81.302  83.14
  16   0.5766     80.850  0.5228    82.432  88.56
  17   0.5559     81.440  0.5006    83.390  93.99
  18   0.5578     81.240  0.4794    84.068  99.41
  19   0.5374     82.110  0.4535    84.722  104.83
  20   0.5386     82.040  0.4354    85.510  110.39
  21   0.5057     83.070  0.4197    85.958  115.82
  22   0.5163     83.070  0.3987    86.938  121.24
  23   0.5116     82.630  0.3781    87.602  126.67
  24   0.5073     83.330  0.3600    88.062  132.10
  25   0.5002     83.530  0.3470    88.414  137.52
  26   0.5013     83.390  0.3384    88.738  142.94
  27   0.4919     83.720  0.3270    89.266  148.35
  28   0.4799     84.210  0.3122    89.586  153.90
  29   0.4731     84.070  0.2961    90.330  159.29
  30   0.4714     84.170  0.2949    90.284  164.72
  31   0.4545     85.130  0.2787    90.676  170.11
  32   0.4824     84.800  0.2634    91.250  175.52
  33   0.4883     84.100  0.2499    91.628  180.93
  34   0.4812     84.560  0.2490    91.660  186.34
  35   0.4728     85.050  0.2297    92.314  191.86
  36   0.4735     84.930  0.2289    92.380  197.27
  37   0.4619     85.610  0.2176    92.544  202.71
  38   0.4698     85.310  0.2115    92.866  208.17
  39   0.4721     85.790  0.2026    93.348  213.58
  40   0.4607     85.550  0.1944    93.506  218.99
  41   0.4738     85.580  0.1972    93.494  224.39
  42   0.4653     86.210  0.1879    93.690  229.97
  43   0.5292     84.130  0.1770    94.084  235.40
  44   0.4850     85.550  0.1804    93.982  240.83
  45   0.4731     86.030  0.1721    94.148  246.27
  46   0.4893     85.440  0.1658    94.502  251.69
  47   0.5087     86.210  0.1584    94.786  257.13
  48   0.4971     86.040  0.1524    94.954  262.55
  49   0.4948     85.900  0.1480    95.080  268.10
  50   0.4758     86.240  0.1454    95.242  273.52
  51   0.5182     86.020  0.1427    95.232  278.93
  52   0.4945     86.560  0.1388    95.354  284.33
  53   0.5054     86.490  0.1314    95.726  289.75
  54   0.4992     86.680  0.1218    95.926  295.15
  55   0.5161     86.590  0.1217    95.986  300.71
  56   0.5213     85.940  0.1206    96.040  306.13
  57   0.5294     86.120  0.1306    95.658  311.54
  58   0.4976     86.550  0.1228    96.074  316.96
  59   0.5104     86.380  0.1171    96.138  322.36
  60   0.5447     86.680  0.1112    96.262  327.79
  61   0.5312     86.840  0.1053    96.532  333.19
  62   0.5600     86.060  0.1053    96.550  338.75
  63   0.5555     86.500  0.1022    96.632  344.15
  64   0.5616     86.510  0.0974    96.700  349.58
  65   0.5400     86.560  0.1009    96.770  355.03
  66   0.5660     86.560  0.0958    96.916  360.47
  67   0.5477     87.180  0.0937    96.916  365.91
  68   0.5583     87.030  0.0857    97.106  371.32
  69   0.5657     87.310  0.0819    97.254  376.80
  70   0.6100     85.560  0.0846    97.194  382.21
  71   0.5371     87.130  0.0884    97.042  387.64
  72   0.5520     87.240  0.0831    97.322  393.06
  73   0.5906     86.540  0.0821    97.252  398.52
  74   0.5580     86.800  0.0844    97.184  403.93
  75   0.5682     87.020  0.0861    97.180  409.32
  76   0.5826     87.200  0.0779    97.420  414.89
  77   0.5348     87.210  0.0725    97.560  420.32
  78   0.5795     87.030  0.0671    97.774  425.78
  79   0.5790     87.660  0.0695    97.718  431.22
  80   0.6154     86.380  0.0703    97.698  436.67
  81   0.5685     87.180  0.0734    97.548  442.06
  82   0.6196     86.880  0.0660    97.884  447.49
  83   0.5846     87.610  0.0658    97.826  453.01
  84   0.5324     86.990  0.0641    97.834  458.44
  85   0.5609     86.920  0.0684    97.840  463.85
  86   0.5392     87.760  0.0689    97.724  469.26
  87   0.5602     87.350  0.0739    97.566  474.65
  88   0.5518     87.020  0.0647    97.800  480.10
  89   0.6093     87.350  0.0648    97.880  485.50
  90   0.5924     87.540  0.0586    98.086  491.05
