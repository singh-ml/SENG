Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8290     25.260  2.3672    15.588  7.04
   2   1.3444     47.710  1.6420    35.986  12.46
   3   1.1909     58.270  1.3141    51.636  17.89
   4   0.9694     66.180  1.0833    61.928  23.48
   5   0.8576     70.310  0.9275    68.062  28.90
   6   0.8277     72.120  0.8221    71.956  34.33
   7   0.8932     71.520  0.7426    75.074  39.78
   8   0.6820     77.520  0.6808    77.216  45.20
   9   0.8896     73.760  0.6359    79.022  50.61
  10   0.6545     77.730  0.5924    80.560  56.03
  11   0.5839     81.410  0.5566    81.900  61.45
  12   0.6389     79.190  0.5238    82.742  66.98
  13   0.5698     81.690  0.4957    83.712  72.42
  14   0.6030     80.990  0.4813    84.136  77.84
  15   0.6342     80.300  0.4396    85.334  83.25
  16   0.5804     81.160  0.4219    85.932  88.68
  17   0.5052     83.370  0.4528    85.804  94.11
  18   0.4810     84.170  0.3760    87.492  99.53
  19   0.5674     82.100  0.3698    87.838  105.08
  20   0.4783     84.920  0.3825    87.700  110.50
  21   0.5098     84.820  0.3915    87.650  115.93
  22   0.4563     85.600  0.3223    89.278  121.34
  23   0.5472     83.340  0.3119    89.694  126.77
  24   0.4899     85.070  0.3097    89.888  132.20
  25   0.4982     84.560  0.2918    90.356  137.59
  26   0.4943     85.500  0.3026    90.052  143.15
  27   0.4381     86.540  0.2703    91.096  148.58
  28   0.4754     85.770  0.2952    90.520  154.01
  29   3.6774     78.780  0.2917    90.848  159.41
  30   0.4103     87.160  0.2602    91.428  164.83
  31   0.4790     86.040  0.2313    92.394  170.26
  32   0.4824     85.480  0.2321    92.232  175.69
  33   0.4834     86.350  0.2882    91.198  181.20
  34   0.4248     87.160  0.3610    90.230  186.62
  35   0.4391     87.070  0.1972    93.476  192.01
  36   0.4644     85.930  0.2067    93.188  197.43
  37   0.4646     86.080  0.2074    93.244  202.85
  38   0.4888     86.080  0.2005    93.348  208.27
  39   0.6229     84.400  0.1969    93.582  213.73
  40   0.5277     86.120  0.2438    92.510  219.28
  41   0.4813     87.150  0.1979    93.750  224.71
  42   0.4620     86.610  0.2299    92.836  230.15
  43   0.4837     86.320  0.1712    94.330  235.56
  44   0.4305     87.100  0.1704    94.322  240.96
  45   0.5151     86.070  0.1724    94.290  246.39
  46   0.4742     86.830  0.1583    94.740  251.91
  47   0.4626     87.190  0.1553    94.844  257.32
  48   0.4643     86.860  0.1484    94.940  262.77
  49   0.5328     86.140  0.1704    94.690  268.21
  50   0.4862     87.070  0.1402    95.390  273.64
  51   0.4472     88.090  0.1757    94.616  279.04
  52   0.4770     87.240  0.1354    95.518  284.46
  53   0.5006     86.350  0.1459    95.228  289.85
  54   0.4744     87.050  0.1291    95.732  295.42
  55   0.4623     86.900  0.1291    95.874  300.81
  56   0.4508     87.530  0.1250    95.912  306.23
  57   0.5195     86.900  0.1227    95.924  311.64
  58   0.5165     86.390  0.1179    96.120  317.09
  59   0.4873     87.840  0.1187    96.216  322.52
  60   0.4837     87.650  0.1143    96.336  327.95
  61   0.4848     87.620  0.1138    96.214  333.39
  62   0.4685     88.250  0.1110    96.448  338.97
  63   0.5030     87.320  0.1068    96.536  344.35
  64   0.5120     87.620  0.1115    96.406  349.80
  65   0.4554     88.080  0.1022    96.672  355.21
  66   0.4590     88.240  0.1032    96.568  360.65
  67   0.4621     88.390  0.0986    96.712  366.10
  68   0.4911     87.630  0.0985    96.722  371.66
  69   0.5318     87.630  0.0948    96.958  377.12
  70   0.4554     88.180  0.0941    96.900  382.56
  71   0.5154     87.430  0.0900    97.076  387.99
  72   0.4895     87.890  0.0923    96.996  393.39
  73   0.5713     86.810  0.0900    97.034  398.82
  74   0.5186     88.090  0.0866    97.202  404.25
  75   0.5317     87.640  0.0825    97.302  409.81
  76   0.5141     87.680  0.0848    97.252  415.25
  77   0.4691     88.930  0.0835    97.278  420.67
  78   0.5028     87.240  0.0862    97.240  426.09
  79   0.4790     88.340  0.0867    97.292  431.51
  80   0.4563     88.990  0.0824    97.274  436.92
  81   0.4647     88.390  0.0806    97.402  442.37
  82   0.5071     88.480  0.0779    97.450  447.90
  83   0.5270     87.810  0.0783    97.542  453.35
  84   0.4632     88.480  0.0798    97.392  458.80
  85   0.4858     88.490  0.0793    97.428  464.22
