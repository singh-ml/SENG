Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2230     14.800  2.5032    12.198  7.07
   2   2.0664     18.070  2.1817    15.042  12.50
   3   1.8685     25.050  1.9880    19.920  17.92
   4   1.7708     28.730  1.8657    24.498  23.31
   5   1.5851     37.000  1.7401    29.912  28.74
   6   1.4715     43.700  1.5480    39.596  34.13
   7   1.3267     49.800  1.4090    46.778  39.52
   8   1.1699     57.320  1.2609    52.668  44.92
   9   1.0823     61.340  1.1618    57.282  50.31
  10   0.9714     65.090  1.0590    62.342  55.73
  11   0.8920     67.890  0.9544    66.148  61.27
  12   0.8576     70.350  0.8888    68.720  66.69
  13   0.8434     71.650  0.8117    71.806  72.07
  14   0.7411     74.590  0.7652    73.758  77.50
  15   0.7809     74.280  0.7070    75.894  82.92
  16   0.7418     76.210  0.6820    77.152  88.32
  17   0.6340     78.660  0.6437    78.332  93.71
  18   0.6476     79.200  0.6064    80.100  99.26
  19   0.6530     78.890  0.5767    81.000  104.66
  20   0.6003     80.630  0.5495    81.790  110.05
  21   0.6007     81.890  0.5296    82.532  115.44
  22   0.5438     82.170  0.5036    83.340  120.89
  23   0.5715     82.330  0.4742    84.374  126.29
  24   0.5844     81.600  0.4779    84.324  131.70
  25   0.5040     83.960  0.4505    85.128  137.24
  26   0.5444     84.160  0.4193    86.142  142.64
  27   0.5273     83.260  0.4186    86.332  148.08
  28   0.5751     84.440  0.3910    87.132  153.52
  29   0.5347     82.940  0.4531    85.984  158.90
  30   0.4983     84.910  0.3926    87.162  164.31
  31   0.5339     83.030  0.3717    87.880  169.73
  32   0.4982     85.110  0.3695    88.016  175.27
  33   0.4694     84.830  0.3577    88.400  180.69
  34   0.4703     85.000  0.3167    89.454  186.08
  35   0.4744     85.590  0.3031    89.884  191.52
  36   0.4479     86.230  0.2885    90.586  196.93
  37   0.4537     86.460  0.2776    90.850  202.33
  38   0.4508     85.990  0.2712    91.144  207.73
  39   0.4745     85.850  0.2641    91.336  213.17
  40   0.4339     87.250  0.2472    91.836  218.56
  41   0.4268     87.120  0.2412    92.082  223.97
  42   0.4741     86.260  0.2270    92.578  229.38
  43   0.4387     86.840  0.2182    92.748  234.82
  44   0.4667     86.410  0.2226    92.742  240.22
  45   0.4374     86.960  0.2140    93.046  245.65
  46   0.4558     86.680  0.2012    93.482  251.17
  47   0.4305     87.520  0.1959    93.616  256.60
  48   0.4371     87.870  0.1881    93.854  262.01
  49   0.5037     86.920  0.1800    94.162  267.45
  50   0.4474     86.800  0.1792    94.236  272.83
  51   0.4279     87.790  0.1728    94.316  278.26
  52   0.4307     87.930  0.1765    94.186  283.76
  53   0.4496     88.180  0.1613    94.660  289.18
  54   0.4530     87.910  0.1647    94.516  294.61
  55   0.4726     87.600  0.1564    94.952  300.04
  56   0.4446     87.740  0.1455    95.248  305.45
  57   0.4433     87.890  0.1492    95.028  310.84
  58   0.4094     88.220  0.1519    94.992  316.26
  59   0.4683     87.720  0.1373    95.582  321.77
  60   0.4350     87.840  0.1386    95.526  327.17
  61   0.4424     88.250  0.1320    95.744  332.63
  62   0.4899     87.860  0.1261    95.932  338.04
  63   0.4366     88.780  0.1234    96.152  343.44
  64   0.4439     88.540  0.1211    95.984  348.85
  65   0.4576     88.600  0.1149    96.162  354.26
  66   0.4397     88.870  0.1086    96.466  359.83
  67   0.4466     88.510  0.1194    96.046  365.27
  68   0.4530     88.630  0.1092    96.426  370.70
  69   0.5182     87.710  0.1049    96.518  376.11
  70   0.4527     88.350  0.1068    96.492  381.53
  71   0.4692     88.060  0.0993    96.790  386.97
  72   0.4660     88.760  0.1022    96.736  392.37
  73   0.4802     89.010  0.0961    96.930  397.88
  74   0.4827     88.430  0.1008    96.714  403.29
  75   0.4969     88.540  0.0952    96.916  408.70
  76   0.4489     89.190  0.1026    96.706  414.14
  77   0.5114     88.030  0.1000    96.772  419.53
  78   0.4758     88.550  0.0922    97.110  424.93
  79   0.5059     88.810  0.0886    97.214  430.32
  80   0.4625     89.040  0.0853    97.280  435.72
  81   0.4600     88.940  0.0856    97.206  441.23
  82   0.4757     89.350  0.0928    97.116  446.66
  83   0.4892     88.780  0.0865    97.204  452.06
  84   0.4596     89.170  0.0875    97.220  457.46
  85   0.4900     88.650  0.0814    97.446  462.89
  86   0.4831     89.310  0.0800    97.366  468.34
  87   0.5051     88.510  0.0763    97.506  473.73
  88   0.4657     89.230  0.0790    97.530  479.26
  89   0.4716     89.310  0.0753    97.600  484.67
  90   0.4642     89.010  0.0801    97.452  490.12
