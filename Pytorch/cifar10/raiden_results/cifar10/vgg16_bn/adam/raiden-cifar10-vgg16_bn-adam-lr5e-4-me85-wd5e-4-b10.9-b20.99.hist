Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.9196     18.800  2.6839    13.696  7.14
   2   1.7115     30.180  1.8368    24.866  12.56
   3   1.5176     43.340  1.5139    40.808  17.95
   4   1.4564     59.200  1.2464    55.504  23.48
   5   1.5437     54.980  1.0649    63.628  28.92
   6   4.8717     55.900  0.9276    68.610  34.32
   7   0.7736     75.190  0.8517    71.464  39.76
   8   0.7668     74.550  0.8075    73.474  45.16
   9   0.7639     74.250  0.7018    77.030  50.57
  10   0.6567     77.720  0.7306    76.802  55.97
  11   0.6875     77.450  0.6655    78.802  61.54
  12   0.6435     78.750  0.5838    81.296  66.94
  13   0.7991     74.400  0.6378    79.912  72.35
  14   0.5934     81.030  0.7272    78.434  77.76
  15   0.5737     81.280  0.5209    83.240  83.19
  16   0.6774     77.620  0.6420    81.290  88.59
  17   0.5811     81.810  0.5560    82.436  93.99
  18   0.6211     81.370  0.4614    85.302  99.51
  19   0.5866     81.970  0.5582    83.348  104.94
  20   0.6602     81.160  0.4461    85.746  110.35
  21   0.5522     82.170  0.4216    86.480  115.78
  22   0.6722     79.360  0.5909    83.434  121.21
  23   0.6031     82.220  0.4757    85.342  126.66
  24   0.4978     84.470  0.3836    87.496  132.21
  25   0.4616     85.280  0.3876    87.644  137.62
  26   5.2097     76.030  0.3940    87.518  143.04
  27   0.4661     85.620  0.3814    87.926  148.50
  28   0.6640     80.500  0.3512    88.932  153.91
  29   0.4885     84.660  0.3355    89.238  159.32
  30   0.4775     85.750  0.3293    89.560  164.73
  31   0.5031     85.400  0.3697    88.634  170.16
  32   0.4948     85.460  0.3047    90.130  175.75
  33   0.4963     84.870  0.3003    90.330  181.18
  34   0.5536     84.280  0.3014    90.388  186.62
  35   0.4360     86.990  0.2833    91.014  192.06
  36   0.4543     86.320  0.2752    91.200  197.50
  37   0.5303     85.320  0.2753    91.282  202.90
  38   0.4745     85.960  0.2652    91.624  208.34
  39   0.4001     88.010  0.2581    91.834  213.78
  40   0.4505     86.720  0.2517    91.986  219.21
  41   0.5497     84.370  0.2423    92.230  224.64
  42   0.5932     83.670  0.2393    92.286  230.07
  43   0.5371     85.850  0.2361    92.572  235.47
  44   0.4497     87.410  0.2288    92.868  240.88
  45   0.4783     86.120  0.2202    92.938  246.42
  46   0.4247     87.850  0.2175    93.030  251.84
  47   0.4104     88.180  0.2079    93.422  257.29
  48   0.4345     87.450  0.2126    93.326  262.73
  49   0.4075     87.870  0.2097    93.568  268.16
  50   0.4998     86.340  0.2036    93.666  273.57
  51   0.4049     88.500  0.1997    93.732  279.00
  52   0.4150     88.150  0.2001    93.692  284.55
  53   0.4477     87.540  0.1883    94.128  289.95
  54   0.4270     88.390  0.1885    94.080  295.39
  55   0.4752     86.900  0.1842    94.160  300.82
  56   0.4268     88.240  0.1826    94.244  306.23
  57   0.4351     88.270  0.1788    94.424  311.65
  58   0.4088     88.350  0.1754    94.480  317.19
  59   0.4904     87.520  0.1724    94.566  322.60
  60   0.4238     88.310  0.1719    94.580  328.03
  61   0.4067     89.220  0.1708    94.584  333.46
  62   0.3766     89.140  0.1619    94.988  338.87
  63   0.4392     88.210  0.1629    94.820  344.29
  64   0.4412     88.110  0.1645    94.888  349.69
  65   0.4006     89.160  0.1571    95.158  355.16
  66   0.4027     88.980  0.1588    95.004  360.58
  67   0.4539     87.950  0.1568    95.022  366.00
  68   0.4833     86.730  0.1588    95.096  371.41
  69   0.5013     87.030  0.1545    95.234  376.85
  70   0.4018     88.940  0.1543    95.148  382.30
  71   0.5161     86.580  0.1479    95.394  387.75
  72   0.4269     88.610  0.1562    95.158  393.20
  73   0.3895     89.000  0.1450    95.458  398.66
  74   0.3765     89.420  0.1432    95.574  404.08
  75   0.3682     90.130  0.1415    95.568  409.50
  76   0.4557     87.870  0.1365    95.784  414.94
  77   0.4366     88.650  0.1416    95.696  420.38
  78   0.4297     88.860  0.1437    95.480  425.80
  79   0.4110     89.120  0.1329    95.806  431.22
  80   0.3984     89.190  0.1393    95.686  436.78
  81   0.3893     89.390  0.1358    95.874  442.23
  82   0.4959     87.440  0.1340    95.770  447.67
  83   0.4121     89.340  0.1316    95.898  453.08
  84   0.4240     89.390  0.1321    95.874  458.50
  85   0.4046     89.370  0.1296    96.026  463.93
