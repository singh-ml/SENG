Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.1434     16.090  2.7664    14.016  7.04
   2   1.8558     24.360  1.8929    23.478  12.48
   3   1.5400     41.690  1.6308    34.374  17.90
   4   1.2308     55.760  1.3356    50.930  23.34
   5   1.3115     60.070  1.1139    60.368  28.91
   6   0.9166     68.080  0.9821    66.346  34.33
   7   1.2955     54.400  0.9823    67.692  39.78
   8   0.7630     73.860  0.8779    70.470  45.19
   9   0.7924     76.050  0.7386    75.804  50.64
  10   0.8870     72.260  0.8461    73.618  56.08
  11   0.7012     77.130  0.6513    78.726  61.67
  12   0.8847     75.580  0.6978    78.366  67.09
  13   0.6643     78.890  0.5869    80.914  72.54
  14   0.8834     72.660  0.6734    80.346  78.01
  15   0.8740     76.890  0.5686    81.980  83.42
  16   0.5957     80.500  0.5879    81.720  88.84
  17   0.6374     80.920  0.5318    83.132  94.30
  18   74.5907     56.000  0.4987    84.762  99.83
  19   0.6115     80.620  0.6257    81.482  105.27
  20   0.4916     83.490  0.4458    85.648  110.73
  21   0.5506     82.960  0.4168    86.428  116.15
  22   0.5219     83.040  0.4967    84.892  121.57
  23   0.4853     84.010  0.6777    81.666  127.02
  24   0.4382     85.560  0.3964    87.008  132.49
  25   0.4809     84.500  0.3699    88.046  138.03
  26   2.1914     78.910  0.3764    87.980  143.44
  27   0.4246     85.980  0.5020    84.974  148.85
  28   0.4657     85.040  0.3393    88.936  154.28
  29   0.6259     80.580  0.7148    81.866  159.70
  30   0.4826     84.540  0.5376    84.530  165.13
  31   0.4465     85.590  0.3524    88.568  170.54
  32   0.4627     85.630  0.3287    89.266  176.06
  33   0.4856     85.090  0.3230    89.446  181.49
  34   0.9935     76.960  0.4496    87.088  186.90
  35   0.4436     85.790  0.6438    84.430  192.32
  36   0.3963     87.370  0.3166    89.630  197.76
  37   0.4738     86.130  0.2797    90.872  203.23
  38   0.4220     86.610  0.2867    90.602  208.69
  39   27.1880     62.780  0.3016    90.366  214.29
  40   0.4252     87.170  0.3116    90.266  219.74
  41   0.4658     86.440  0.2625    91.538  225.16
  42   0.6890     77.700  0.3914    88.868  230.58
  43   0.3753     88.030  0.3274    89.544  236.05
  44   0.5263     84.800  0.2380    92.354  241.52
  45   0.4522     86.850  0.2436    92.056  246.95
  46   0.4089     87.640  0.2351    92.416  252.54
  47   0.4436     86.840  0.2733    91.582  257.96
  48   0.4455     87.120  0.2229    92.760  263.39
  49   0.3983     88.120  0.2144    93.080  268.83
  50   0.3971     87.750  0.2149    92.972  274.28
  51   0.4049     88.370  0.2295    92.768  279.72
  52   0.4612     87.250  0.1969    93.714  285.28
  53   0.4605     87.280  0.1949    93.734  290.72
  54   0.3560     89.440  0.1892    93.970  296.19
  55   0.4589     87.360  0.1836    94.138  301.60
  56   0.4344     87.530  0.1775    94.310  307.03
  57   0.3978     88.710  0.1859    94.080  312.46
  58   0.4303     88.090  0.1686    94.556  317.90
  59   0.4070     88.280  0.1712    94.590  323.37
  60   0.4033     88.090  0.1690    94.694  328.79
  61   0.3923     88.960  0.1577    94.966  334.21
  62   0.5197     85.890  0.1632    94.822  339.67
  63   0.3663     88.660  0.1569    94.962  345.13
  64   0.5142     86.460  0.1478    95.362  350.59
  65   0.4432     87.840  0.1459    95.480  356.05
  66   0.4496     87.800  0.1477    95.342  361.47
  67   0.4301     88.810  0.1400    95.634  367.04
  68   0.3916     89.450  0.1391    95.652  372.48
  69   0.4446     88.940  0.1351    95.730  377.95
  70   0.4219     88.310  0.1381    95.652  383.38
  71   0.3764     89.330  0.1426    95.444  388.84
  72   0.4167     89.470  0.1306    95.790  394.29
  73   0.4071     89.320  0.1262    96.016  399.86
  74   0.3849     89.810  0.1261    95.962  405.30
  75   0.4755     88.090  0.1244    96.062  410.73
  76   0.4444     88.760  0.1225    96.088  416.18
  77   0.3984     89.690  0.1380    95.822  421.64
  78   0.4042     89.700  0.1148    96.352  427.09
  79   0.5288     88.570  0.1194    96.072  432.54
  80   0.5032     87.340  0.1218    96.158  437.99
  81   0.4009     90.070  0.1172    96.294  443.42
  82   0.4416     89.230  0.1107    96.504  448.85
  83   0.4353     89.070  0.1079    96.670  454.29
  84   0.4369     88.690  0.1100    96.492  459.72
  85   0.4505     88.650  0.1102    96.554  465.15
