Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2290     13.890  2.7767    12.088  7.19
   2   1.7949     24.280  1.9437    21.456  12.57
   3   1.8724     30.950  1.7333    28.672  17.98
   4   1.4073     47.660  1.4515    43.180  23.39
   5   1.2195     58.580  1.1878    56.890  28.77
   6   3.4347     53.860  1.0356    64.088  34.20
   7   0.9152     68.070  1.0961    64.204  39.60
   8   0.9201     72.890  0.8419    71.124  45.15
   9   0.9140     71.400  0.9000    71.132  50.58
  10   0.7388     75.320  0.7816    74.006  55.99
  11   0.7735     76.750  0.6747    77.260  61.43
  12   0.6548     77.800  0.7403    76.558  66.85
  13   0.6283     79.690  0.5988    80.300  72.28
  14   1.0542     65.270  0.8752    74.678  77.74
  15   0.6154     80.470  0.6679    78.204  83.17
  16   0.5877     80.760  0.5314    82.632  88.60
  17   0.5779     81.650  0.5043    83.406  94.01
  18   0.5347     82.440  0.7008    80.160  99.41
  19   0.6375     80.710  0.4841    84.104  104.84
  20   0.5857     81.430  0.4544    85.164  110.25
  21   0.5682     81.310  0.4893    84.490  115.65
  22   0.6044     80.470  0.5030    84.262  121.18
  23   0.5566     82.570  0.4100    86.580  126.61
  24   0.6019     81.910  0.3951    87.136  132.01
  25   0.7953     75.690  0.9081    78.852  137.44
  26   0.5038     83.720  0.5649    81.908  142.87
  27   0.4767     85.220  0.4036    87.026  148.27
  28   0.4458     85.760  0.3576    88.498  153.81
  29   0.4497     85.930  0.3459    88.756  159.20
  30   0.7538     81.800  0.3977    87.572  164.64
  31   0.4679     85.530  0.5487    83.800  170.06
  32   0.4312     86.810  0.3557    88.740  175.50
  33   0.5643     84.140  0.3137    89.726  180.91
  34   1.0781     74.700  0.4109    88.008  186.34
  35   0.4662     84.440  0.9147    80.678  191.91
  36   0.4754     85.190  0.3169    89.724  197.32
  37   0.4934     85.820  0.2903    90.400  202.75
  38   1.0464     69.940  0.3326    89.906  208.16
  39   0.4388     86.740  0.5055    86.602  213.57
  40   0.3860     87.130  0.3925    88.180  219.00
  41   0.4157     86.930  0.2597    91.562  224.40
  42   0.4237     87.440  0.2625    91.362  229.90
  43   0.5038     85.580  0.2580    91.564  235.31
  44   0.5484     84.840  0.3464    89.812  240.75
  45   0.4278     86.530  0.5913    84.800  246.16
  46   0.4800     85.590  0.2614    91.506  251.59
  47   0.4716     86.530  0.2759    91.264  257.02
  48   0.4462     86.460  0.2247    92.734  262.43
  49   0.5055     86.260  0.2415    92.372  268.00
  50   0.4712     86.100  0.2215    92.570  273.43
  51   0.4210     88.090  0.2131    93.024  278.85
  52   0.5151     84.430  0.5901    85.632  284.32
  53   0.4278     87.740  0.2562    91.958  289.72
  54   0.4095     88.310  0.2217    93.052  295.15
  55   0.4027     88.460  0.1918    93.712  300.56
  56   0.4100     88.710  0.1921    93.704  305.97
  57   0.4241     87.950  0.1847    93.954  311.41
  58   0.4260     87.890  0.1802    94.052  316.82
  59   0.7328     87.110  0.1812    94.186  322.25
  60   0.4536     88.610  0.1786    94.210  327.69
  61   0.4312     88.220  0.1574    94.882  333.10
  62   0.4621     86.880  0.1721    94.532  338.63
  63   0.4532     88.050  0.1577    94.842  344.04
  64   0.4459     88.310  0.1531    95.020  349.44
  65   0.5167     87.040  0.1499    95.210  354.87
  66   0.4538     88.510  0.1537    95.148  360.29
  67   0.5077     87.010  0.1381    95.608  365.72
  68   0.4320     88.300  0.1347    95.668  371.13
  69   0.4016     88.860  0.1344    95.738  376.57
  70   0.5387     86.880  0.1304    95.828  382.16
  71   0.4043     88.990  0.1437    95.476  387.57
  72   0.4798     88.410  0.1180    96.094  393.01
  73   0.4092     89.180  0.1169    96.296  398.45
  74   0.4602     88.740  0.1195    96.234  403.86
  75   1.4257     83.870  0.1239    95.968  409.26
  76   0.5276     87.830  0.1278    95.916  414.70
  77   0.4379     89.260  0.1115    96.420  420.16
  78   0.4658     89.150  0.1089    96.492  425.57
  79   0.4327     89.010  0.1079    96.510  430.98
  80   0.4301     89.420  0.1024    96.708  436.40
  81   0.4165     89.540  0.1049    96.650  441.82
  82   0.5346     87.700  0.1106    96.610  447.23
  83   0.4709     88.470  0.1069    96.588  452.78
  84   0.4197     89.770  0.1014    96.756  458.18
  85   0.5018     88.580  0.1004    96.864  463.61
