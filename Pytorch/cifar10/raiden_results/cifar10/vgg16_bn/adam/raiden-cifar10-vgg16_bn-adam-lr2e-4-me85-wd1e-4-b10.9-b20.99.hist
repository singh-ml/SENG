Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7670     29.000  2.3102    17.798  6.99
   2   1.3883     47.620  1.6056    37.226  12.40
   3   1.1150     59.800  1.2745    53.536  17.79
   4   0.9660     65.170  1.0561    62.920  23.18
   5   0.8908     69.410  0.9026    69.042  28.57
   6   0.7755     73.460  0.8019    72.882  34.13
   7   0.6646     78.010  0.7309    75.422  39.52
   8   0.7858     74.610  0.6684    77.782  44.91
   9   0.7065     78.640  0.6193    79.382  50.31
  10   0.6553     78.180  0.5728    80.960  55.71
  11   0.5608     80.750  0.5425    81.966  61.11
  12   0.5762     81.800  0.5084    83.030  66.56
  13   0.5193     82.530  0.5422    82.540  72.14
  14   0.5662     81.670  0.4541    84.938  77.56
  15   0.5854     80.900  0.4476    85.392  82.97
  16   0.5304     82.810  0.4167    86.230  88.37
  17   0.5329     82.810  0.4020    86.676  93.77
  18   0.4948     84.520  0.3853    87.198  99.17
  19   0.4508     85.160  0.4568    85.906  104.59
  20   0.5121     83.870  0.3461    88.526  110.11
  21   0.5821     82.980  0.3344    88.832  115.54
  22   0.6174     80.590  0.3535    88.986  120.95
  23   0.4880     84.810  0.3988    87.568  126.37
  24   0.4567     85.810  0.2866    90.380  131.79
  25   0.4553     85.720  0.2875    90.360  137.22
  26   0.5387     83.890  0.2948    90.198  142.64
  27   2.9933     79.650  0.2887    90.380  148.19
  28   0.5104     85.220  0.3047    90.292  153.61
  29   0.4957     85.500  0.2495    91.698  159.01
  30   0.5335     85.380  0.2569    91.400  164.40
  31   0.4913     85.850  0.3237    90.124  169.83
  32   0.5257     84.980  0.3083    91.054  175.25
  33   0.4985     85.200  0.2641    91.442  180.67
  34   0.4372     86.800  0.2151    92.872  186.21
  35   0.4542     86.560  0.2120    93.030  191.65
  36   0.4260     87.190  0.2116    92.978  197.07
  37   0.5243     85.040  0.2116    92.924  202.49
  38   0.5134     85.750  0.2497    92.114  207.90
  39   0.4560     86.670  0.2165    93.008  213.31
  40   0.4361     86.840  0.1835    93.874  218.74
  41   0.5913     84.120  0.1832    93.890  224.28
  42   0.5285     86.110  0.1967    93.736  229.71
  43   0.4666     87.650  0.1894    94.024  235.15
  44   0.4478     87.820  0.1704    94.208  240.55
  45   0.4713     86.610  0.1610    94.580  245.98
  46   0.5252     86.290  0.1609    94.574  251.39
  47   0.4661     87.480  0.2485    93.292  256.78
  48   0.5020     85.250  0.2762    92.994  262.33
  49   0.4626     87.760  0.1614    94.666  267.74
  50   0.4721     87.970  0.1390    95.396  273.15
  51   0.4962     87.380  0.1445    95.176  278.54
  52   0.4586     87.160  0.1398    95.360  283.96
  53   0.4811     86.930  0.1315    95.648  289.38
  54   0.7000     84.140  0.1537    95.196  294.79
  55   0.4420     88.450  0.1734    94.762  300.33
  56   0.4254     88.600  0.1151    96.114  305.76
  57   0.4897     87.530  0.1200    96.042  311.19
  58   0.4777     87.610  0.1238    95.878  316.61
  59   0.4556     88.160  0.1178    95.930  322.01
  60   0.4653     87.570  0.1120    96.280  327.41
  61   0.6284     85.430  0.1780    94.944  332.96
  62   0.4682     88.590  0.1210    96.166  338.35
  63   0.5136     87.170  0.0947    96.862  343.75
  64   0.5039     86.680  0.1645    95.456  349.16
  65   0.4772     88.380  0.1005    96.564  354.59
  66   0.4474     88.810  0.0907    96.970  360.04
  67   0.5031     88.350  0.0987    96.662  365.45
  68   0.5359     87.960  0.0996    96.700  371.00
  69   0.5033     88.090  0.0949    96.854  376.40
  70   0.5474     86.710  0.0951    96.852  381.82
  71   0.4987     88.290  0.1408    95.926  387.26
  72   0.4778     88.320  0.0813    97.254  392.68
  73   0.5039     87.880  0.0820    97.338  398.10
  74   0.5284     87.420  0.0910    97.098  403.52
  75   0.4719     88.650  0.0879    97.080  409.09
  76   0.4762     88.530  0.0945    96.920  414.50
  77   0.5360     88.420  0.0950    97.014  419.90
  78   0.5540     87.390  0.0811    97.422  425.33
  79   0.5060     88.480  0.0808    97.338  430.75
  80   0.5285     88.150  0.0782    97.376  436.17
  81   0.4701     89.180  0.0818    97.460  441.56
  82   0.4992     88.610  0.0872    97.134  447.08
  83   0.4518     89.240  0.0748    97.544  452.48
  84   0.4855     88.460  0.0731    97.560  457.89
  85   0.4957     89.390  0.0712    97.788  463.31
