Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2760     11.910  2.9470    11.300  7.05
   2   2.2591     12.550  2.2792    11.954  12.48
   3   2.1278     17.200  2.2124    14.192  17.98
   4   1.9653     19.800  2.0976    16.656  23.38
   5   1.8695     22.360  1.9490    19.516  28.78
   6   1.8320     23.520  1.8972    20.958  34.20
   7   1.8674     23.980  1.8430    24.500  39.59
   8   1.6572     33.740  1.7558    29.218  45.01
   9   1.4982     39.030  1.6271    35.908  50.41
  10   1.4156     44.630  1.4891    41.676  55.95
  11   1.3206     50.690  1.3754    47.754  61.35
  12   1.2813     53.530  1.2973    51.976  66.75
  13   1.1225     58.050  1.1651    57.376  72.15
  14   1.0921     61.770  1.0832    60.984  77.56
  15   1.0726     65.420  0.9932    64.348  82.96
  16   0.8978     68.820  0.8931    68.970  88.36
  17   0.8214     72.300  0.8395    71.628  93.77
  18   0.7748     75.010  0.7532    74.962  99.18
  19   0.7226     76.690  0.7180    76.816  104.62
  20   0.6902     77.380  0.6663    78.548  110.02
  21   0.6368     79.280  0.6365    79.784  115.42
  22   0.5598     81.440  0.6012    80.738  120.83
  23   0.5580     81.960  0.5569    82.320  126.25
  24   0.5423     82.780  0.5084    83.832  131.82
  25   0.5774     81.730  0.4877    84.630  137.23
  26   0.5300     82.980  0.4887    84.504  142.64
  27   0.5993     81.910  0.4532    85.766  148.08
  28   0.4629     85.390  0.4194    86.816  153.48
  29   0.4875     84.690  0.4045    87.182  158.91
  30   0.4889     85.150  0.3887    87.804  164.35
  31   0.4729     85.010  0.3674    88.340  169.75
  32   0.4877     85.120  0.3573    88.750  175.27
  33   0.4503     85.930  0.3621    88.468  180.71
  34   0.4532     85.720  0.3515    89.208  186.12
  35   0.4344     86.490  0.3279    89.696  191.53
  36   0.4459     86.220  0.3130    89.986  196.93
  37   0.4413     86.430  0.3073    90.336  202.36
  38   0.4060     87.870  0.2893    90.860  207.91
  39   0.4086     87.670  0.2733    91.340  213.31
  40   0.4222     87.790  0.2699    91.328  218.74
  41   0.4129     87.450  0.2697    91.470  224.17
  42   0.4205     87.630  0.2586    91.856  229.56
  43   0.4403     87.220  0.2529    92.092  234.99
  44   0.4216     88.200  0.2430    92.498  240.41
  45   0.4636     87.390  0.2396    92.482  245.96
  46   0.3932     88.260  0.2374    92.516  251.38
  47   0.4218     88.070  0.2303    92.718  256.82
  48   0.4095     87.770  0.2271    92.916  262.24
  49   0.4044     88.810  0.2132    93.246  267.64
  50   0.4006     88.710  0.2075    93.624  273.06
  51   0.4567     87.450  0.2115    93.282  278.47
  52   0.4101     88.750  0.2085    93.518  283.92
  53   0.3928     88.620  0.1940    93.856  289.47
  54   0.3944     88.780  0.1906    93.950  294.89
  55   0.4675     87.810  0.1954    93.982  300.32
  56   0.4005     88.880  0.1937    93.952  305.75
  57   0.3888     88.760  0.1781    94.492  311.18
  58   0.4135     88.420  0.1797    94.474  316.59
  59   0.4123     88.480  0.1726    94.706  322.15
  60   0.4031     89.450  0.1755    94.586  327.56
  61   0.4329     88.500  0.1753    94.586  332.97
  62   0.4090     89.030  0.1773    94.488  338.39
  63   0.4305     89.180  0.1670    94.630  343.79
  64   0.3950     89.350  0.1587    94.998  349.21
  65   0.4396     88.760  0.1611    94.964  354.62
  66   0.4364     89.210  0.1578    95.122  360.20
  67   0.4366     89.020  0.1584    95.102  365.64
  68   0.4116     89.240  0.1619    95.076  371.07
  69   0.3835     89.530  0.1558    95.112  376.49
  70   0.3935     89.730  0.1453    95.506  381.92
  71   0.4142     89.260  0.1477    95.500  387.35
  72   0.4138     89.100  0.1516    95.384  392.78
  73   0.3830     89.340  0.1425    95.652  398.25
  74   0.3764     90.020  0.1437    95.672  403.65
  75   0.3834     89.500  0.1439    95.658  409.04
  76   0.3973     89.760  0.1362    95.826  414.46
  77   0.4005     89.170  0.1395    95.656  419.90
  78   0.4338     89.120  0.1394    95.758  425.33
  79   0.3987     89.790  0.1406    95.716  430.74
  80   0.4089     89.110  0.1408    95.650  436.18
  81   0.4089     89.360  0.1289    95.966  441.73
  82   0.4196     89.780  0.1304    95.978  447.16
  83   0.3865     90.070  0.1285    96.068  452.58
  84   0.3814     90.020  0.1297    96.150  458.00
  85   0.3798     89.950  0.1275    96.088  463.40
  86   0.4271     89.330  0.1204    96.222  468.81
  87   0.4444     88.850  0.1310    96.020  474.34
  88   0.4140     90.020  0.1284    95.992  479.77
  89   0.3900     89.850  0.1291    96.140  485.17
  90   0.4006     89.810  0.1264    96.168  490.60
