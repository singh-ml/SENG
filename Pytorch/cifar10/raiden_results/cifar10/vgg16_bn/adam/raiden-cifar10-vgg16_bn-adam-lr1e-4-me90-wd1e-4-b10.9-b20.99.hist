Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4052     47.430  1.9080    29.394  7.06
   2   1.1714     58.300  1.3458    51.148  12.47
   3   1.1168     60.120  1.1377    60.064  17.92
   4   0.9873     65.590  1.0152    64.452  23.47
   5   0.8781     69.110  0.9220    67.926  28.88
   6   0.8274     71.520  0.8354    71.132  34.30
   7   0.7646     73.830  0.7665    73.728  39.73
   8   0.7197     75.450  0.7091    75.780  45.18
   9   0.7251     74.440  0.6665    77.316  50.59
  10   0.7627     73.940  0.6270    78.734  56.04
  11   0.6803     76.200  0.5939    79.874  61.44
  12   0.6643     77.520  0.5579    80.894  66.84
  13   0.6158     78.460  0.5282    82.126  72.25
  14   0.5870     80.100  0.5056    82.912  77.69
  15   0.6252     78.920  0.4798    83.890  83.08
  16   0.6708     77.170  0.4526    84.504  88.53
  17   0.5567     80.890  0.4310    85.270  93.95
  18   0.5813     81.050  0.4120    86.006  99.38
  19   0.5711     80.370  0.3958    86.546  104.92
  20   0.5102     82.760  0.3804    87.114  110.34
  21   0.6002     81.310  0.3631    87.712  115.77
  22   0.5959     81.650  0.3503    88.238  121.20
  23   0.5212     83.270  0.3338    88.598  126.60
  24   0.5754     81.590  0.3190    89.062  132.03
  25   0.4969     84.130  0.3091    89.504  137.57
  26   0.4899     84.230  0.2920    90.158  143.02
  27   0.5645     81.910  0.2894    90.270  148.42
  28   0.5328     83.500  0.2788    90.680  153.85
  29   0.5538     83.350  0.2612    91.064  159.26
  30   0.5513     83.150  0.2516    91.456  164.70
  31   0.5365     84.380  0.2435    91.780  170.10
  32   0.5511     83.480  0.2326    92.002  175.71
  33   0.5627     83.770  0.2399    91.988  181.12
  34   0.5151     84.780  0.2196    92.598  186.53
  35   0.5467     84.430  0.2130    92.816  191.97
  36   0.5196     85.640  0.2001    93.306  197.37
  37   0.6391     82.220  0.2011    93.182  202.80
  38   0.5403     85.070  0.1944    93.398  208.22
  39   0.4868     85.940  0.1876    93.666  213.75
  40   0.5307     85.690  0.1792    93.928  219.16
  41   0.6798     83.120  0.1858    93.864  224.59
  42   0.4949     86.170  0.1627    94.430  230.02
  43   0.5630     85.590  0.1639    94.470  235.42
  44   0.5633     84.960  0.1645    94.404  240.82
  45   0.6292     84.230  0.1505    94.764  246.25
  46   0.6172     84.070  0.1530    94.890  251.79
  47   0.5790     85.390  0.1485    95.000  257.24
  48   0.5478     85.770  0.1398    95.254  262.64
  49   0.5860     85.340  0.1371    95.354  268.05
  50   0.6161     84.420  0.1288    95.522  273.47
  51   0.5169     86.450  0.1364    95.532  278.92
  52   0.5997     85.750  0.1272    95.692  284.31
  53   0.5632     86.160  0.1228    95.818  289.85
  54   0.5884     85.730  0.1205    95.946  295.26
  55   0.5783     86.370  0.1197    95.810  300.70
  56   0.5835     86.190  0.1173    96.130  306.13
  57   0.5653     86.820  0.1092    96.288  311.53
  58   0.5812     86.050  0.1082    96.388  316.93
  59   0.5888     85.930  0.1128    96.174  322.34
  60   0.5966     85.990  0.1003    96.602  327.89
  61   0.6028     85.770  0.1041    96.536  333.31
  62   0.5911     86.550  0.1039    96.464  338.73
  63   0.6084     86.260  0.0929    96.836  344.14
  64   0.6620     85.610  0.1032    96.568  349.56
  65   0.6074     86.790  0.0918    96.806  355.01
  66   0.6399     85.830  0.0905    96.908  360.43
  67   0.6150     86.180  0.0903    96.984  365.84
  68   0.6697     85.900  0.0868    97.080  371.27
  69   0.6121     85.330  0.0896    97.036  376.73
  70   0.5760     86.520  0.0868    97.124  382.18
  71   0.6422     86.430  0.0816    97.324  387.60
  72   0.7473     85.720  0.0853    97.168  393.00
  73   0.6169     86.850  0.0909    96.986  398.52
  74   0.5877     86.770  0.0787    97.444  403.95
  75   0.5935     86.930  0.0782    97.326  409.35
  76   0.5922     86.840  0.0792    97.374  414.78
  77   0.6533     86.720  0.0764    97.438  420.21
  78   0.5839     86.420  0.0776    97.374  425.63
  79   0.6295     86.740  0.0720    97.550  431.04
  80   0.5893     86.440  0.0728    97.656  436.46
  81   0.6134     86.880  0.0695    97.702  441.99
  82   0.6211     87.070  0.0713    97.592  447.40
  83   0.6586     86.710  0.0667    97.752  452.80
  84   0.5649     87.030  0.0773    97.498  458.21
  85   0.7150     86.520  0.0622    97.914  463.62
  86   0.6262     87.260  0.0714    97.690  469.05
  87   0.6481     86.190  0.0655    97.788  474.60
  88   0.6855     86.770  0.0651    97.820  480.04
  89   0.7305     87.150  0.0682    97.730  485.47
  90   0.9145     86.150  0.0632    97.848  490.92
