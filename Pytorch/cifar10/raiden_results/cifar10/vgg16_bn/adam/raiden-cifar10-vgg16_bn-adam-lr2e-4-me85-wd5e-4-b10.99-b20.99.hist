Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2470     13.520  2.4665    12.314  6.95
   2   1.9705     19.260  2.0865    16.962  12.36
   3   1.8693     25.260  1.8982    21.706  17.88
   4   1.6941     34.020  1.7755    28.276  23.27
   5   1.4700     40.660  1.5802    36.238  28.70
   6   1.3982     44.590  1.4465    42.102  34.14
   7   1.2773     51.870  1.3331    48.504  39.56
   8   1.1317     57.740  1.1742    56.280  44.99
   9   0.9750     65.510  1.0462    62.072  50.41
  10   0.9092     67.960  0.9532    66.302  55.94
  11   0.8648     69.580  0.8774    69.336  61.33
  12   0.7989     72.670  0.8092    72.128  66.76
  13   0.8926     71.170  0.7561    74.398  72.15
  14   0.7392     75.300  0.7002    76.532  77.55
  15   0.6992     76.930  0.6658    77.936  82.95
  16   0.6461     78.590  0.6311    79.076  88.38
  17   0.6182     79.650  0.5903    80.526  93.93
  18   0.6154     80.420  0.5465    82.070  99.33
  19   0.5940     81.090  0.5267    82.748  104.75
  20   0.5861     81.390  0.5001    83.650  110.16
  21   0.5635     82.410  0.4763    84.464  115.58
  22   0.5368     82.470  0.4599    85.156  120.98
  23   0.5353     83.480  0.4492    85.248  126.42
  24   0.5231     83.190  0.4350    85.930  131.95
  25   0.5073     83.760  0.3910    87.244  137.34
  26   0.5013     84.380  0.3755    87.740  142.74
  27   0.4800     85.010  0.3735    87.952  148.17
  28   0.5112     84.580  0.3569    88.420  153.56
  29   0.4553     86.450  0.3386    89.088  158.97
  30   0.4595     85.870  0.3204    89.450  164.53
  31   0.4671     85.750  0.3095    89.972  169.96
  32   0.4510     86.000  0.2979    90.266  175.39
  33   0.4462     86.290  0.2909    90.488  180.78
  34   0.4819     85.800  0.2809    90.810  186.19
  35   0.5048     85.910  0.2708    91.176  191.62
  36   0.4486     86.580  0.2621    91.570  197.04
  37   0.4495     86.990  0.2502    91.908  202.58
  38   0.4300     87.190  0.2410    92.136  208.01
  39   0.4900     85.630  0.2305    92.552  213.43
  40   0.4354     87.320  0.2351    92.434  218.84
  41   0.4478     86.920  0.2159    93.016  224.24
  42   0.4589     86.970  0.2154    93.068  229.64
  43   0.4219     87.570  0.2016    93.484  235.07
  44   0.4465     87.420  0.1933    93.714  240.62
  45   0.4169     87.930  0.1902    93.834  246.05
  46   0.4778     86.750  0.1892    93.790  251.44
  47   0.4819     87.400  0.1791    94.138  256.91
  48   0.4347     88.010  0.1736    94.390  262.30
  49   0.4642     87.470  0.1690    94.540  267.69
  50   0.4580     87.300  0.1681    94.524  273.11
  51   0.4463     87.810  0.1547    94.996  278.60
  52   0.4347     87.840  0.1488    95.084  284.02
  53   0.4349     87.990  0.1532    95.004  289.46
  54   0.4500     88.360  0.1533    95.000  294.89
  55   0.4472     88.030  0.1427    95.392  300.29
  56   0.4545     88.020  0.1482    95.244  305.70
  57   0.4635     87.910  0.1441    95.460  311.13
  58   0.4286     88.650  0.1404    95.542  316.64
  59   0.4365     88.480  0.1343    95.686  322.04
  60   0.4630     88.490  0.1331    95.712  327.44
  61   0.4393     88.520  0.1315    95.794  332.85
  62   0.4542     88.210  0.1263    95.928  338.24
  63   0.4368     88.530  0.1183    96.192  343.63
  64   0.4765     87.480  0.1163    96.176  349.05
  65   0.4502     88.800  0.1223    96.062  354.48
  66   0.4365     88.700  0.1150    96.268  360.06
  67   0.4360     88.500  0.1169    96.274  365.49
  68   0.4425     88.930  0.1112    96.462  370.87
  69   0.4640     88.500  0.1120    96.438  376.29
  70   0.4676     88.580  0.1071    96.540  381.71
  71   0.4882     88.200  0.1093    96.528  387.11
  72   0.4213     88.530  0.1043    96.678  392.54
  73   0.4547     89.110  0.1050    96.624  398.04
  74   0.4482     88.980  0.0980    96.966  403.44
  75   0.4721     89.220  0.1010    96.808  408.86
  76   0.4832     88.350  0.0980    96.896  414.29
  77   0.4548     89.050  0.0956    96.900  419.70
  78   0.4359     88.810  0.0953    97.012  425.11
  79   0.4377     89.170  0.0891    97.096  430.53
  80   0.4552     88.580  0.0914    97.010  436.07
  81   0.4705     89.090  0.0902    97.126  441.47
  82   0.4374     89.030  0.0886    97.196  446.88
  83   0.4714     88.760  0.0910    97.080  452.28
  84   0.4473     89.110  0.0837    97.302  457.69
  85   0.4387     88.950  0.0875    97.160  463.09
