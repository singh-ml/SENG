Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2364     14.600  3.4912    11.924  7.37
   2   2.1768     16.170  2.2619    13.096  12.81
   3   2.1584     15.180  2.2305    14.098  18.26
   4   2.1621     17.840  2.2038    14.600  23.71
   5   2.1227     17.440  2.1645    15.968  29.12
   6   1.8978     22.150  2.0411    19.736  34.53
   7   1.7923     27.410  1.9074    22.288  39.94
   8   1.7735     27.030  1.8522    24.330  45.48
   9   1.7050     29.310  1.8035    26.496  50.90
  10   1.6827     31.180  1.7650    28.088  56.34
  11   1.5824     37.240  1.6807    33.118  61.79
  12   1.4787     41.240  1.5690    37.712  67.22
  13   1.4286     47.240  1.4430    44.420  72.65
  14   1.2149     54.330  1.3166    50.736  78.07
  15   1.1126     59.250  1.1916    57.032  83.66
  16   1.3343     58.300  1.1097    60.884  89.07
  17   1.0530     62.570  1.0758    62.430  94.52
  18   1.0719     62.030  1.0599    62.972  99.95
  19   0.9530     67.810  0.9461    67.164  105.41
  20   0.8326     71.500  0.8845    69.756  110.86
  21   0.8459     71.340  0.8103    72.372  116.29
  22   0.9165     69.840  0.8633    71.384  121.84
  23   0.8979     70.670  0.8800    70.434  127.27
  24   0.7683     74.520  0.7847    73.774  132.70
  25   0.7179     76.390  0.6869    77.008  138.14
  26   0.6555     78.500  0.6327    79.052  143.56
  27   0.6681     79.100  0.6007    80.224  149.01
  28   0.6727     78.940  0.6236    80.326  154.46
  29   0.5704     81.610  0.5775    81.238  160.04
  30   0.6809     78.990  0.5373    82.830  165.45
  31   1.1361     67.460  0.8565    74.120  170.89
  32   0.7979     76.840  0.7962    74.288  176.34
  33   0.6105     80.040  0.6527    79.608  181.76
  34   0.5911     81.820  0.5626    82.088  187.21
  35   0.5413     83.400  0.4936    84.074  192.69
  36   0.5420     83.380  0.4698    84.896  198.24
  37   0.5624     82.770  0.4498    85.768  203.65
  38   1.7598     74.910  0.6218    81.212  209.10
  39   0.6189     80.110  0.6608    79.090  214.55
  40   0.5293     82.940  0.5273    83.308  219.97
  41   0.5203     83.060  0.4593    85.224  225.39
  42   0.5051     84.290  0.4209    86.606  230.82
  43   0.4794     84.700  0.4176    86.844  236.36
  44   0.6402     80.550  0.4118    87.056  241.83
  45   0.4709     85.060  0.4292    86.368  247.23
  46   0.4728     85.190  0.3886    87.644  252.65
  47   0.4557     85.660  0.3760    88.168  258.07
  48   0.4546     85.820  0.3594    88.654  263.50
  49   0.4878     85.370  0.3625    88.612  268.94
  50   0.4583     86.340  0.3585    88.728  274.51
  51   0.4477     86.460  0.3461    89.178  279.97
  52   0.4465     86.520  0.3500    88.900  285.46
  53   0.4691     86.110  0.3454    89.138  290.88
  54   0.4414     86.650  0.3259    89.848  296.31
  55   0.4744     86.170  0.3173    90.100  301.74
  56   0.4696     85.460  0.3067    90.402  307.19
  57   0.4424     86.750  0.3181    90.126  312.73
  58   0.4343     86.960  0.3140    90.074  318.14
  59   0.4289     86.800  0.2999    90.754  323.58
  60   0.4265     87.090  0.2981    90.656  329.03
  61   0.4346     86.950  0.3026    90.584  334.48
  62   0.4266     86.840  0.3009    90.738  339.92
  63   0.4374     87.040  0.2932    90.736  345.32
  64   0.4715     86.690  0.2795    91.382  350.87
  65   0.4180     87.740  0.2851    91.108  356.28
  66   0.4228     87.600  0.2728    91.546  361.71
  67   0.4362     87.330  0.2730    91.534  367.16
  68   0.4165     87.790  0.2742    91.568  372.59
  69   0.4440     87.080  0.2693    91.694  378.02
  70   0.4034     87.890  0.2598    91.984  383.46
  71   0.4141     87.710  0.2545    91.956  389.05
  72   0.4312     87.480  0.2572    91.926  394.47
  73   0.3973     88.360  0.2520    92.254  399.91
  74   0.4274     87.490  0.2542    92.216  405.34
  75   0.4287     87.460  0.2504    92.194  410.76
  76   0.3750     88.310  0.2465    92.292  416.20
  77   0.4449     87.640  0.2434    92.478  421.79
  78   0.4116     88.060  0.2519    92.126  427.24
  79   0.4211     88.240  0.2391    92.578  432.67
  80   0.4070     87.650  0.2398    92.632  438.11
  81   0.3980     87.720  0.2405    92.536  443.56
  82   0.3967     88.070  0.2329    92.762  449.01
  83   0.3933     88.750  0.2330    92.872  454.45
  84   0.4439     88.000  0.2304    92.724  460.02
  85   0.4032     88.000  0.2302    92.918  465.47
  86   0.4254     87.690  0.2281    92.916  470.91
  87   0.3939     88.420  0.2354    92.706  476.35
  88   0.4279     88.350  0.2244    93.130  481.77
  89   0.4271     87.920  0.2263    93.048  487.18
  90   0.3958     88.150  0.2285    92.974  492.64
