Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2826     16.310  2.8815    12.278  7.05
   2   2.0568     18.450  2.1570    15.938  12.46
   3   1.9043     21.550  2.0176    19.228  17.88
   4   1.8816     23.400  1.9011    22.398  23.30
   5   1.8032     27.980  1.8300    25.092  28.72
   6   1.7210     30.380  1.7668    28.538  34.11
   7   1.6072     38.170  1.6749    33.334  39.64
   8   1.5269     40.240  1.5779    38.664  45.07
   9   1.3908     48.000  1.4691    44.070  50.49
  10   1.2719     53.190  1.3645    49.860  55.88
  11   1.3656     52.270  1.2387    55.352  61.29
  12   1.0355     63.540  1.1199    60.272  66.70
  13   1.0246     63.250  1.0226    64.188  72.10
  14   0.9190     68.100  0.9588    66.566  77.63
  15   0.8160     71.960  0.8839    69.832  83.06
  16   0.7851     72.950  0.8120    71.830  88.51
  17   0.8472     70.910  0.7620    73.976  93.92
  18   0.8113     72.560  0.8327    72.464  99.33
  19   0.6841     77.080  0.7539    74.746  104.75
  20   0.6702     78.010  0.6647    77.894  110.15
  21   0.6473     79.150  0.6249    79.292  115.67
  22   0.6644     78.450  0.5797    80.978  121.06
  23   0.7466     80.010  0.5560    81.802  126.49
  24   0.6199     79.770  0.5768    81.192  131.91
  25   0.5653     81.890  0.5244    82.956  137.32
  26   0.5448     83.280  0.4944    83.868  142.73
  27   0.5930     81.870  0.4635    85.092  148.16
  28   0.5448     82.990  0.4533    85.498  153.56
  29   0.4730     85.040  0.4258    86.354  159.08
  30   0.4940     84.470  0.4017    86.994  164.48
  31   0.5435     83.310  0.4256    86.576  169.93
  32   0.5000     84.820  0.4405    85.960  175.36
  33   0.4563     85.380  0.3799    87.798  180.78
  34   0.4397     86.010  0.3465    88.882  186.19
  35   0.9702     78.530  0.4872    85.632  191.74
  36   0.6413     78.760  0.7743    76.996  197.14
  37   0.7578     80.110  0.5603    83.010  202.57
  38   1.9005     67.340  0.7184    79.410  207.98
  39   1.0873     62.920  1.4496    59.496  213.41
  40   0.7667     73.670  0.9731    67.972  218.82
  41   0.6406     78.960  0.7222    75.868  224.24
  42   0.5649     81.130  0.5955    80.216  229.78
  43   0.5215     82.890  0.5195    82.962  235.22
  44   0.5115     83.340  0.4649    84.736  240.64
  45   0.4829     84.210  0.4303    85.872  246.08
  46   0.4450     85.320  0.4043    86.888  251.51
  47   0.4485     85.380  0.3827    87.412  256.95
  48   0.4700     85.210  0.3748    87.852  262.38
  49   0.4498     85.800  0.3649    88.230  267.92
  50   0.4590     85.840  0.3617    88.104  273.32
  51   0.4595     85.740  0.3375    89.354  278.73
  52   0.4811     84.730  0.3230    89.412  284.14
  53   0.4118     87.260  0.3147    89.590  289.57
  54   0.4287     86.790  0.2813    90.900  294.99
  55   0.3938     87.500  0.2714    91.208  300.40
  56   0.4098     87.360  0.2628    91.448  305.97
  57   0.3929     87.830  0.2525    91.914  311.37
  58   0.3946     88.090  0.2355    92.398  316.79
  59   0.3973     88.160  0.2380    92.256  322.24
  60   0.4133     88.070  0.2392    92.286  327.67
  61   0.4269     87.830  0.2261    92.752  333.10
  62   0.3906     87.690  0.2221    92.774  338.52
  63   0.3966     88.540  0.2243    92.700  344.06
  64   0.5388     87.520  0.2203    92.930  349.46
  65   0.6214     84.030  0.2764    91.978  354.88
  66   0.5031     85.050  0.3919    87.852  360.29
  67   0.4255     87.130  0.3089    90.002  365.73
  68   0.3808     88.680  0.2357    92.398  371.17
  69   0.4195     87.910  0.2043    93.426  376.60
  70   0.3973     88.660  0.1968    93.698  382.19
  71   0.3971     89.030  0.1819    94.104  387.60
  72   0.4479     87.790  0.1702    94.472  393.01
  73   0.3863     88.690  0.1739    94.472  398.46
  74   0.3892     88.860  0.1678    94.626  403.88
  75   0.4068     88.830  0.1657    94.662  409.27
  76   0.3992     88.800  0.1720    94.568  414.67
  77   0.4256     88.170  0.1663    94.704  420.24
  78   0.4072     88.450  0.1526    95.112  425.68
  79   0.4036     88.870  0.1486    95.226  431.09
  80   0.5345     88.240  0.1581    95.074  436.48
  81   0.4867     87.370  0.1725    94.746  441.89
  82   0.4322     87.600  0.1907    93.940  447.33
  83   0.3882     89.360  0.1657    94.688  452.88
  84   0.4045     89.130  0.1401    95.522  458.27
  85   0.4198     88.910  0.1365    95.718  463.68
  86   0.4133     89.280  0.1379    95.754  469.10
  87   0.4368     89.100  0.1314    95.812  474.50
  88   0.4048     89.790  0.1297    95.810  479.93
  89   0.4337     88.700  0.1241    96.096  485.34
  90   0.4217     89.490  0.1202    96.106  490.76
