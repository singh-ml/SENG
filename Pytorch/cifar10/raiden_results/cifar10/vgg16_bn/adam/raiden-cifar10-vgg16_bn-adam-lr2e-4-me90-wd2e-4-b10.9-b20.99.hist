Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7347     29.660  2.2565    18.200  7.22
   2   1.3485     49.550  1.5657    39.228  12.65
   3   1.0475     62.960  1.2535    54.844  18.09
   4   0.9121     67.260  1.0212    64.498  23.53
   5   0.9151     68.140  0.8837    69.994  29.08
   6   0.8243     72.180  0.7835    73.838  34.51
   7   0.7504     75.140  0.7165    76.448  39.92
   8   0.6964     76.880  0.6500    78.342  45.35
   9   0.7409     76.010  0.6188    79.612  50.77
  10   0.6117     79.620  0.5677    81.386  56.20
  11   0.6729     78.410  0.5441    82.024  61.63
  12   0.5852     81.210  0.5216    83.234  67.05
  13   0.6819     78.850  0.4710    84.508  72.61
  14   0.5904     82.110  0.4809    84.248  78.07
  15   0.6548     80.430  0.4852    84.368  83.51
  16   0.6089     79.550  0.4170    86.344  88.94
  17   0.5680     81.520  0.3916    87.094  94.39
  18   0.5174     83.420  0.3871    87.222  99.80
  19   0.5429     83.300  0.3638    87.954  105.24
  20   0.5434     82.930  0.3682    88.056  110.79
  21   0.5087     84.370  0.3303    88.968  116.21
  22   0.5239     83.850  0.3623    88.400  121.63
  23   0.4969     84.460  0.2969    90.086  127.05
  24   0.6231     82.270  0.3076    89.942  132.51
  25   0.4929     84.950  0.2859    90.452  137.95
  26   0.4545     86.070  0.2719    90.816  143.40
  27   0.5231     84.650  0.2879    90.634  148.97
  28   0.5390     83.780  0.2531    91.710  154.38
  29   0.4722     84.940  0.2545    91.590  159.82
  30   0.5025     85.120  0.2685    91.404  165.27
  31   0.4229     87.320  0.2842    90.918  170.73
  32   0.4543     86.490  0.2133    92.908  176.15
  33   0.5059     85.760  0.2141    92.860  181.61
  34   0.8798     83.550  0.2217    92.708  187.13
  35   0.4396     87.170  0.2127    92.972  192.57
  36   0.6879     84.420  0.2228    92.870  197.99
  37   0.4586     85.770  0.2568    92.044  203.44
  38   0.4996     86.070  0.1870    93.718  208.86
  39   0.4352     87.340  0.1815    94.040  214.28
  40   0.4382     86.720  0.1870    93.830  219.73
  41   0.4769     86.770  0.1778    94.102  225.28
  42   0.4789     86.800  0.1751    94.132  230.72
  43   0.5162     86.510  0.1629    94.492  236.14
  44   0.5010     86.380  0.1778    94.350  241.56
  45   0.4818     86.110  0.1662    94.562  247.00
  46   0.4450     87.820  0.1496    95.116  252.43
  47   0.5001     86.510  0.1422    95.210  257.87
  48   0.4579     87.300  0.1514    94.942  263.38
  49   0.5062     86.360  0.1440    95.346  268.82
  50   0.5301     86.450  0.1371    95.480  274.25
  51   0.4650     87.450  0.1331    95.656  279.67
  52   0.4701     87.560  0.1327    95.638  285.09
  53   0.5013     88.070  0.1297    95.706  290.53
  54   0.4761     87.650  0.1326    95.620  295.96
  55   0.5093     87.590  0.1258    95.990  301.51
  56   0.4391     87.790  0.1235    95.974  306.96
  57   0.5095     87.350  0.1160    96.118  312.39
  58   0.4621     88.030  0.1157    96.320  317.81
  59   0.4515     88.270  0.1142    96.280  323.24
  60   0.4538     88.200  0.1057    96.544  328.67
  61   0.4378     88.760  0.1104    96.446  334.08
  62   0.5744     87.370  0.1037    96.626  339.64
  63   0.4652     87.790  0.1069    96.566  345.09
  64   0.5236     87.750  0.1011    96.738  350.54
  65   0.4962     87.920  0.1006    96.752  355.98
  66   0.4698     88.640  0.1015    96.696  361.42
  67   0.4882     88.190  0.1000    96.946  366.83
  68   0.4642     88.750  0.1029    96.622  372.28
  69   0.5218     88.090  0.0936    96.868  377.85
  70   0.4600     88.280  0.0902    97.154  383.31
  71   0.5623     87.350  0.0893    97.094  388.78
  72   0.4768     88.600  0.0888    97.144  394.22
  73   0.4651     88.390  0.0875    97.178  399.66
  74   0.4634     88.810  0.0857    97.102  405.09
  75   0.5061     88.130  0.0858    97.172  410.55
  76   0.5107     88.160  0.0815    97.412  416.11
  77   0.4797     88.380  0.0846    97.218  421.57
  78   0.6174     87.040  0.0802    97.376  426.98
  79   0.5548     87.640  0.0855    97.300  432.42
  80   0.4924     88.500  0.0810    97.376  437.89
  81   0.4369     89.230  0.0790    97.404  443.31
  82   0.5673     87.370  0.0751    97.556  448.89
  83   0.5122     88.060  0.0802    97.382  454.33
  84   0.5338     87.160  0.0709    97.644  459.74
  85   0.5188     88.090  0.0726    97.600  465.16
  86   0.4845     88.900  0.0718    97.680  470.62
  87   0.4846     88.540  0.0734    97.580  476.06
  88   0.5056     88.070  0.0720    97.650  481.51
  89   0.5068     88.900  0.0691    97.868  487.11
  90   0.4533     89.410  0.0729    97.658  492.55
