Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8215     28.180  2.2899    17.500  7.17
   2   1.5277     44.040  1.5437    39.800  12.61
   3   1.1777     58.490  1.2202    56.624  18.02
   4   0.9524     66.410  0.9994    65.460  23.44
   5   0.8226     71.180  0.8654    70.794  28.85
   6   0.7926     74.090  0.7694    74.348  34.27
   7   0.7790     75.080  0.7006    76.934  39.70
   8   0.7683     76.230  0.6440    78.720  45.26
   9   0.8765     74.600  0.6066    80.192  50.69
  10   0.7615     76.160  0.5590    81.552  56.12
  11   0.5933     80.310  0.5421    82.338  61.56
  12   0.6807     79.080  0.4944    83.588  66.98
  13   0.5800     81.310  0.5216    83.366  72.44
  14   0.6247     79.880  0.4727    84.650  77.86
  15   0.5307     82.830  0.4295    85.872  83.41
  16   0.7626     78.330  0.4782    84.722  88.87
  17   0.6343     80.270  0.4321    85.974  94.30
  18   0.5182     83.800  0.3820    87.510  99.71
  19   1.0325     78.750  0.3971    87.214  105.12
  20   0.5309     83.590  0.3803    87.596  110.53
  21   0.6601     82.190  0.3559    88.424  115.97
  22   0.5381     83.160  0.3745    87.792  121.54
  23   0.5271     84.130  0.3167    89.524  126.95
  24   0.5436     82.910  0.3094    89.812  132.37
  25   0.5881     82.890  0.3055    89.908  137.82
  26   0.4837     86.040  0.2975    90.156  143.26
  27   0.5001     84.480  0.2918    90.540  148.69
  28   0.6148     81.820  0.2692    90.966  154.14
  29   0.4859     84.920  0.2714    91.218  159.70
  30   0.4701     85.790  0.2533    91.772  165.13
  31   0.5860     82.660  0.2435    92.032  170.56
  32   0.5802     84.560  0.2374    92.122  175.97
  33   0.4785     86.010  0.2282    92.398  181.41
  34   0.5249     85.350  0.2228    92.774  186.85
  35   0.4632     86.380  0.2150    93.010  192.28
  36   0.5101     85.730  0.2120    93.056  197.87
  37   0.5018     85.290  0.2024    93.536  203.29
  38   0.4506     86.700  0.2015    93.480  208.72
  39   0.4601     86.280  0.1946    93.682  214.12
  40   0.4474     86.920  0.1836    94.084  219.54
  41   0.7073     83.070  0.1799    94.178  224.96
  42   0.4468     87.620  0.1764    94.174  230.52
  43   0.4947     86.010  0.1717    94.302  235.94
  44   0.4894     86.280  0.1673    94.528  241.38
  45   0.5075     86.140  0.1669    94.490  246.83
  46   0.4965     86.870  0.1587    94.836  252.25
  47   0.4606     87.090  0.1531    94.990  257.74
  48   0.4718     87.180  0.1548    95.014  263.17
  49   0.4628     87.270  0.1517    95.158  268.62
  50   0.5437     86.500  0.1436    95.304  274.18
  51   0.4576     87.230  0.1434    95.270  279.61
  52   0.4880     86.740  0.1438    95.342  285.05
  53   0.4923     87.490  0.1364    95.608  290.49
  54   0.4850     87.160  0.1352    95.646  295.91
  55   0.4947     86.840  0.1295    95.790  301.33
  56   0.4957     87.580  0.1298    95.758  306.88
  57   0.4637     87.510  0.1269    95.872  312.33
  58   0.4264     88.270  0.1266    95.988  317.75
  59   0.4553     88.400  0.1250    95.904  323.19
  60   0.4817     87.820  0.1166    96.154  328.67
  61   0.4828     87.680  0.1195    96.020  334.11
  62   0.5070     87.550  0.1160    96.190  339.56
  63   0.4445     88.290  0.1172    96.256  345.00
  64   0.4660     87.860  0.1101    96.356  350.53
  65   0.4547     88.600  0.1110    96.352  355.96
  66   0.4536     88.350  0.1133    96.296  361.40
  67   0.5247     87.930  0.1028    96.712  366.82
  68   0.4862     88.100  0.1074    96.566  372.24
  69   0.4546     88.390  0.1016    96.726  377.68
  70   0.5488     86.790  0.1012    96.752  383.11
  71   0.5688     87.830  0.1003    96.738  388.51
  72   0.4628     89.330  0.1028    96.726  394.10
  73   0.4089     89.100  0.0959    96.830  399.55
  74   0.4415     88.450  0.0968    96.788  404.99
  75   0.4676     88.610  0.0993    96.830  410.41
  76   0.4397     88.580  0.0967    96.824  415.83
  77   0.4756     87.920  0.0944    97.006  421.30
  78   0.4351     88.950  0.0896    97.038  426.88
  79   0.4878     88.430  0.0965    96.918  432.31
  80   0.4598     88.540  0.0934    97.042  437.76
  81   0.5419     87.150  0.0882    97.120  443.20
  82   0.4831     88.980  0.0915    97.146  448.62
  83   0.4539     88.800  0.0880    97.178  454.07
  84   0.5027     88.430  0.0876    97.134  459.51
  85   0.4626     88.990  0.0877    97.192  465.09
  86   0.4546     88.350  0.0879    97.218  470.51
  87   0.4981     88.190  0.0823    97.412  475.97
  88   0.5850     86.790  0.0854    97.280  481.38
  89   0.5135     87.960  0.0808    97.364  486.83
  90   0.4892     88.990  0.0847    97.302  492.25
