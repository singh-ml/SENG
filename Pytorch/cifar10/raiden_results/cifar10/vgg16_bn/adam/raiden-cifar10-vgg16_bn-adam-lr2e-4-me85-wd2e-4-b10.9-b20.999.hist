Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2935     11.270  2.4970    11.970  7.12
   2   1.5685     37.840  1.8285    26.348  12.66
   3   1.2716     52.410  1.4651    44.100  18.06
   4   1.2123     59.320  1.2004    57.024  23.50
   5   1.2067     59.760  1.0117    64.812  28.92
   6   0.8524     70.880  0.8695    70.580  34.34
   7   0.7714     75.160  0.7763    74.226  39.78
   8   0.6954     77.000  0.7031    76.854  45.20
   9   0.6777     78.140  0.6435    79.040  50.79
  10   0.6895     77.240  0.5973    80.578  56.23
  11   0.6333     79.190  0.5622    81.722  61.64
  12   0.6261     79.190  0.5182    83.336  67.06
  13   0.5590     81.860  0.4961    84.270  72.47
  14   0.5615     81.430  0.4708    84.840  77.91
  15   0.5258     82.570  0.4522    85.478  83.36
  16   0.5174     82.780  0.4205    86.468  88.87
  17   0.5276     82.560  0.4016    86.942  94.29
  18   0.5355     82.710  0.3882    87.690  99.70
  19   0.4913     83.880  0.3673    88.260  105.13
  20   0.4723     84.240  0.3571    88.652  110.58
  21   0.5132     83.660  0.3346    89.296  115.99
  22   0.4736     84.660  0.3344    89.156  121.39
  23   0.4866     84.410  0.3170    89.934  126.91
  24   0.4719     83.780  0.3010    90.272  132.33
  25   0.4632     85.240  0.2928    90.652  137.74
  26   0.4436     85.290  0.2769    91.124  143.15
  27   0.4536     85.500  0.2810    91.094  148.57
  28   0.4631     85.600  0.2665    91.396  153.96
  29   0.4886     84.580  0.2485    91.976  159.37
  30   0.4946     84.940  0.2386    92.372  164.96
  31   0.4966     85.480  0.2412    92.228  170.37
  32   0.4689     85.680  0.2285    92.584  175.79
  33   0.4179     87.290  0.2180    93.006  181.23
  34   0.4451     86.390  0.2098    93.280  186.67
  35   0.4793     85.830  0.2077    93.402  192.13
  36   0.4136     87.700  0.1974    93.650  197.67
  37   0.4259     87.860  0.1892    93.934  203.10
  38   0.4925     86.440  0.1798    94.280  208.52
  39   0.4834     86.540  0.1783    94.190  213.95
  40   0.7236     80.080  0.2579    92.342  219.37
  41   0.4354     87.520  0.2437    92.164  224.77
  42   0.4577     87.030  0.1731    94.380  230.20
  43   0.4525     87.270  0.1608    94.898  235.64
  44   0.4598     87.380  0.1523    95.012  241.23
  45   0.4534     87.120  0.1466    95.240  246.65
  46   0.4639     87.380  0.1438    95.344  252.08
  47   0.4525     88.110  0.1405    95.436  257.55
  48   0.5102     86.980  0.1393    95.570  262.97
  49   0.4583     87.630  0.1375    95.500  268.42
  50   0.4976     87.350  0.1325    95.624  274.00
  51   0.4777     88.020  0.1295    95.836  279.45
  52   0.4277     88.730  0.1331    95.796  284.89
  53   0.4517     88.220  0.1300    95.820  290.34
  54   0.4358     88.400  0.1185    96.168  295.80
  55   0.5159     88.510  0.1141    96.350  301.21
  56   0.5179     87.110  0.1765    94.824  306.70
  57   0.4476     88.520  0.1145    96.338  312.16
  58   0.5046     87.590  0.1090    96.520  317.58
  59   0.4891     88.340  0.1022    96.772  323.01
  60   0.5102     87.430  0.1112    96.436  328.48
  61   0.4497     89.070  0.1009    96.728  333.93
  62   0.4766     88.010  0.0992    96.852  339.38
  63   0.4391     88.570  0.1008    96.696  344.82
  64   0.4935     87.920  0.0997    96.870  350.38
  65   0.4148     89.170  0.0996    96.750  355.80
  66   0.5185     87.630  0.0914    96.998  361.23
  67   0.7955     87.620  0.1004    96.832  366.64
  68   0.4930     88.540  0.1250    96.008  372.06
  69   0.4555     88.400  0.0996    96.866  377.47
  70   0.5493     86.930  0.0854    97.260  383.04
  71   0.4829     88.870  0.0845    97.286  388.49
  72   0.5365     87.680  0.0801    97.394  393.93
  73   0.4721     89.200  0.0843    97.336  399.37
  74   0.4782     88.920  0.0827    97.392  404.81
  75   0.5034     88.480  0.0832    97.354  410.25
  76   0.4639     89.600  0.0819    97.346  415.67
  77   0.4470     89.100  0.0765    97.528  421.11
  78   0.4632     89.080  0.0822    97.354  426.68
  79   0.5850     86.660  0.0793    97.362  432.12
  80   0.5528     87.620  0.0811    97.342  437.55
  81   0.4715     89.610  0.0789    97.404  442.97
  82   0.5337     87.820  0.0736    97.584  448.39
  83   0.4708     89.010  0.0742    97.632  453.84
  84   0.5123     88.880  0.0719    97.656  459.35
  85   0.4939     88.480  0.0752    97.666  464.78
