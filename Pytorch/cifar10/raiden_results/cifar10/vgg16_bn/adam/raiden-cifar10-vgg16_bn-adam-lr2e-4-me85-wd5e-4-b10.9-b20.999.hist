Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8440     25.880  2.2673    18.290  7.17
   2   1.3395     49.440  1.5867    38.428  12.58
   3   1.1959     56.540  1.2600    54.346  17.99
   4   1.0121     64.540  1.0339    64.038  23.42
   5   0.8938     69.870  0.8973    69.452  28.81
   6   0.8708     70.120  0.7927    73.394  34.23
   7   0.7740     74.010  0.7220    76.074  39.63
   8   0.7451     75.210  0.6580    78.298  45.08
   9   0.7052     76.410  0.6150    79.790  50.63
  10   0.6696     76.780  0.5781    81.020  56.03
  11   0.5762     81.020  0.5396    82.442  61.45
  12   0.5366     81.670  0.5206    83.146  66.90
  13   0.6239     79.960  0.4853    84.186  72.32
  14   0.5814     81.920  0.4594    85.298  77.75
  15   0.5649     82.420  0.4423    85.676  83.31
  16   0.5159     82.630  0.4212    86.376  88.71
  17   0.5270     83.530  0.4064    86.884  94.14
  18   0.5155     82.980  0.3886    87.408  99.57
  19   0.5161     83.100  0.3774    87.944  105.00
  20   0.4837     84.110  0.3533    88.646  110.43
  21   0.4744     84.850  0.3454    88.952  115.86
  22   0.5225     83.290  0.3279    89.444  121.47
  23   0.4986     85.010  0.3209    89.626  126.89
  24   0.4872     85.270  0.3095    90.010  132.31
  25   0.4716     85.400  0.2897    90.486  137.74
  26   0.4475     86.450  0.2895    90.628  143.16
  27   0.4668     85.020  0.2728    91.148  148.58
  28   0.5049     85.390  0.2604    91.634  154.02
  29   0.5166     85.390  0.2731    91.234  159.55
  30   0.4812     85.750  0.2507    92.042  165.00
  31   0.4670     86.350  0.2450    92.120  170.44
  32   0.4940     85.200  0.2331    92.516  175.88
  33   0.4977     85.770  0.2345    92.518  181.32
  34   0.5345     84.960  0.2258    92.632  186.74
  35   0.4834     86.130  0.2154    92.986  192.26
  36   0.4515     86.670  0.2073    93.278  197.68
  37   0.5016     86.650  0.1995    93.684  203.09
  38   0.4511     87.080  0.1987    93.518  208.54
  39   0.4244     87.760  0.1896    93.964  213.94
  40   0.4227     87.630  0.1886    93.916  219.37
  41   0.4681     86.570  0.1846    94.112  224.80
  42   0.4399     87.670  0.1735    94.424  230.36
  43   0.4633     87.040  0.1727    94.362  235.79
  44   0.4819     86.220  0.1722    94.478  241.20
  45   0.4897     86.050  0.1645    94.688  246.61
  46   0.4727     87.180  0.1605    94.786  252.04
  47   0.5135     86.400  0.1591    94.922  257.45
  48   0.4487     87.850  0.1506    95.094  262.86
  49   0.4894     87.440  0.1477    95.212  268.42
  50   0.5149     86.920  0.1450    95.388  273.83
  51   0.4520     87.500  0.1437    95.346  279.27
  52   0.4332     88.150  0.1341    95.606  284.67
  53   0.4378     88.690  0.1370    95.664  290.10
  54   0.5408     87.710  0.1295    95.830  295.54
  55   0.4724     88.210  0.1320    95.728  301.11
  56   0.5208     86.150  0.1319    95.714  306.56
  57   0.5131     86.520  0.1214    96.074  311.98
  58   0.5730     87.010  0.1212    95.986  317.40
  59   0.4828     87.480  0.1174    96.312  322.82
  60   0.4828     88.090  0.1149    96.296  328.26
  61   0.4622     88.600  0.1189    96.264  333.70
  62   0.4585     88.370  0.1094    96.494  339.14
  63   0.4846     88.230  0.1151    96.354  344.70
  64   0.4515     88.110  0.1126    96.484  350.19
  65   0.4657     88.140  0.1139    96.272  355.64
  66   0.4760     88.860  0.1041    96.602  361.08
  67   0.5009     87.670  0.1051    96.648  366.52
  68   0.4830     88.170  0.1033    96.762  371.96
  69   0.5196     87.270  0.1017    96.746  377.51
  70   0.4579     88.250  0.1031    96.716  382.96
  71   0.4736     88.380  0.1001    96.844  388.38
  72   0.4603     88.770  0.0964    96.946  393.80
  73   0.4435     88.280  0.0978    96.886  399.25
  74   0.4802     88.610  0.0984    96.876  404.72
  75   0.4896     87.690  0.0951    96.960  410.19
  76   0.5027     87.340  0.0968    96.914  415.79
  77   0.4341     88.640  0.0982    96.886  421.24
  78   0.4883     88.250  0.0897    97.178  426.72
  79   0.4423     89.840  0.0888    97.222  432.16
  80   0.5442     87.950  0.0895    97.232  437.62
  81   0.4599     89.440  0.0946    97.032  443.07
  82   0.4955     88.610  0.0846    97.330  448.54
  83   0.5388     87.950  0.0883    97.234  454.08
  84   0.5085     88.900  0.0845    97.296  459.54
  85   0.4599     88.630  0.0839    97.288  464.99
