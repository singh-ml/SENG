Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8456     26.530  2.2465    17.910  7.10
   2   1.3813     46.850  1.6370    35.086  12.61
   3   1.2141     57.360  1.3205    52.384  18.03
   4   1.0145     64.050  1.0859    62.302  23.45
   5   0.8822     70.060  0.9273    68.464  28.88
   6   0.8587     71.040  0.8124    72.664  34.31
   7   0.8388     71.660  0.7258    75.866  39.73
   8   0.6996     76.600  0.6685    78.102  45.15
   9   0.6483     78.760  0.6193    79.694  50.70
  10   0.6303     79.370  0.5684    81.492  56.12
  11   0.6395     79.060  0.5367    82.630  61.53
  12   0.5933     79.650  0.5173    83.344  66.94
  13   0.5824     80.420  0.4726    84.782  72.40
  14   0.5402     82.090  0.4550    85.292  77.82
  15   0.5594     81.900  0.4334    86.154  83.23
  16   0.5383     82.350  0.4109    86.690  88.78
  17   0.6242     80.670  0.3974    87.066  94.21
  18   0.5002     83.210  0.3789    87.816  99.63
  19   0.4795     84.550  0.3597    88.500  105.09
  20   0.4971     84.070  0.3386    88.946  110.52
  21   0.5234     83.040  0.3277    89.454  115.94
  22   0.4763     84.680  0.3116    89.822  121.37
  23   0.4749     85.000  0.3051    90.078  126.91
  24   0.4678     85.720  0.2895    90.532  132.34
  25   0.4355     86.340  0.2790    91.010  137.77
  26   0.4323     85.950  0.2688    91.180  143.23
  27   0.5035     83.780  0.2624    91.410  148.68
  28   0.5110     85.220  0.2688    91.360  154.10
  29   0.4489     86.070  0.2764    91.302  159.54
  30   0.4182     86.810  0.2330    92.478  165.08
  31   0.5036     85.130  0.2123    93.064  170.52
  32   0.4463     86.610  0.2100    93.138  175.97
  33   0.5454     84.830  0.2035    93.454  181.41
  34   0.4765     86.840  0.2031    93.404  186.84
  35   0.5467     84.860  0.1949    93.698  192.27
  36   0.5289     86.150  0.1993    93.572  197.72
  37   0.5094     85.900  0.2041    93.506  203.25
  38   0.4523     86.720  0.1813    93.960  208.71
  39   0.5122     86.630  0.1939    93.848  214.13
  40   0.6267     82.860  0.1611    94.716  219.59
  41   0.4689     86.940  0.1871    93.876  225.03
  42   0.4599     87.400  0.1572    94.826  230.43
  43   0.4886     86.960  0.1482    95.156  235.96
  44   0.4746     87.030  0.1466    95.180  241.38
  45   0.4679     87.280  0.1380    95.458  246.80
  46   0.4918     87.140  0.1406    95.536  252.23
  47   0.4314     88.090  0.1332    95.756  257.65
  48   0.4699     88.210  0.1299    95.704  263.09
  49   0.6217     84.230  0.1402    95.574  268.52
  50   0.4594     85.510  0.3024    91.074  274.08
  51   0.4193     88.500  0.1497    95.118  279.52
  52   0.4972     87.460  0.1189    96.092  284.94
  53   0.4565     88.080  0.1134    96.238  290.40
  54   0.5187     87.910  0.1002    96.696  295.86
  55   0.4427     88.060  0.1050    96.578  301.31
  56   0.4306     88.390  0.1045    96.588  306.74
  57   0.5170     87.950  0.1044    96.632  312.32
  58   0.5093     87.490  0.1016    96.570  317.78
  59   0.4661     88.180  0.0986    96.748  323.22
  60   0.5398     87.560  0.0924    96.948  328.65
  61   0.5314     86.930  0.1004    96.708  334.10
  62   0.4869     88.140  0.0954    96.924  339.52
  63   0.4701     88.400  0.0908    97.048  344.93
  64   0.4649     88.760  0.0953    96.986  350.49
  65   0.4943     87.820  0.0895    97.128  355.91
  66   0.4644     88.700  0.0882    97.146  361.35
  67   0.5411     87.610  0.0855    97.196  366.79
  68   0.4645     88.300  0.0842    97.246  372.21
  69   0.5205     87.250  0.0815    97.438  377.66
  70   0.5216     87.820  0.0835    97.310  383.15
  71   0.5024     88.660  0.0842    97.248  388.60
  72   0.4892     88.910  0.0763    97.558  394.02
  73   0.5206     83.680  0.6659    83.218  399.45
  74   0.4786     87.120  0.2668    91.796  404.87
  75   0.4376     88.480  0.1379    95.400  410.31
  76   0.4829     88.280  0.0987    96.644  415.80
  77   0.4782     88.610  0.0806    97.370  421.23
  78   0.4793     88.750  0.0741    97.572  426.83
  79   0.5157     88.150  0.0657    97.866  432.28
  80   0.4866     88.940  0.0685    97.840  437.70
  81   0.5606     88.930  0.0596    98.094  443.12
  82   0.5275     88.910  0.0640    97.918  448.54
  83   0.5032     88.750  0.0592    98.114  453.96
  84   0.4940     88.880  0.0623    97.992  459.41
  85   0.5042     89.040  0.0590    97.990  464.93
  86   0.5239     88.770  0.0591    97.976  470.38
  87   0.4591     89.500  0.0600    98.056  475.80
  88   0.5032     89.190  0.0622    98.032  481.23
  89   0.5048     88.740  0.0629    97.974  486.65
  90   0.5096     89.210  0.0584    98.136  492.08
