Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7912     28.900  2.1383    20.100  7.08
   2   1.5067     41.060  1.6723    33.348  12.48
   3   1.3082     49.920  1.4705    43.254  17.86
   4   1.1886     56.250  1.3159    50.994  23.25
   5   1.0274     62.570  1.1499    58.760  28.63
   6   0.9502     65.840  1.0279    63.598  34.16
   7   0.8473     70.260  0.9356    67.058  39.55
   8   0.8045     71.520  0.8473    70.438  44.96
   9   0.7504     73.640  0.7853    73.038  50.35
  10   0.7099     75.240  0.7282    74.898  55.77
  11   0.6755     76.550  0.6807    76.758  61.16
  12   0.6232     78.770  0.6366    78.430  66.57
  13   0.6654     77.570  0.5956    80.064  71.99
  14   0.5969     79.550  0.5618    81.034  77.49
  15   0.5807     79.980  0.5253    82.336  82.90
  16   0.5502     81.590  0.5108    82.824  88.29
  17   0.5518     80.930  0.4855    83.732  93.67
  18   0.5322     82.220  0.4644    84.348  99.07
  19   0.5185     82.440  0.4379    85.380  104.47
  20   0.5128     83.010  0.4142    86.128  110.00
  21   0.5447     81.650  0.4025    86.554  115.40
  22   0.5176     82.670  0.3826    87.182  120.79
  23   0.5215     83.340  0.3632    87.662  126.18
  24   0.5265     83.150  0.3593    88.150  131.58
  25   0.5188     83.640  0.3425    88.628  137.01
  26   0.5087     83.610  0.3273    89.184  142.42
  27   0.5113     84.190  0.3155    89.428  147.81
  28   0.5287     83.190  0.2993    89.786  153.35
  29   0.5152     84.230  0.2968    89.904  158.76
  30   0.4918     84.780  0.2861    90.462  164.13
  31   0.4971     84.230  0.2805    90.616  169.51
  32   0.5032     84.870  0.2522    91.650  174.89
  33   0.4890     84.650  0.2569    91.536  180.30
  34   0.5011     85.010  0.2453    91.730  185.69
  35   0.5079     85.310  0.2307    92.406  191.23
  36   0.5378     84.060  0.2271    92.314  196.62
  37   0.4959     85.470  0.2219    92.684  202.01
  38   0.5350     84.310  0.2132    92.816  207.40
  39   0.5089     84.590  0.2062    93.084  212.80
  40   0.5111     85.630  0.1986    93.416  218.21
  41   0.5256     85.140  0.1956    93.564  223.63
  42   0.5341     85.290  0.1915    93.638  229.22
  43   0.5132     85.180  0.1820    93.890  234.61
  44   0.5111     85.650  0.1792    93.998  240.02
  45   0.5043     86.000  0.1755    94.034  245.44
  46   0.5340     86.090  0.1639    94.460  250.86
  47   0.5296     85.790  0.1583    94.628  256.30
  48   0.5668     84.680  0.1556    94.730  261.84
  49   0.5123     85.740  0.1515    94.920  267.26
  50   0.5228     85.590  0.1537    95.010  272.66
  51   0.5081     85.450  0.1479    95.134  278.07
  52   0.5668     85.510  0.1359    95.470  283.45
  53   0.5637     85.440  0.1346    95.542  288.85
  54   0.5601     86.020  0.1520    94.980  294.29
  55   0.4958     86.490  0.1383    95.378  299.71
  56   0.5266     86.040  0.1328    95.488  305.24
  57   0.5278     86.390  0.1322    95.516  310.64
  58   0.5192     86.490  0.1208    96.014  316.05
  59   0.5572     85.970  0.1184    96.036  321.45
  60   0.5432     85.670  0.1185    96.106  326.82
  61   0.5512     86.050  0.1217    95.958  332.21
  62   0.5307     86.750  0.1132    96.192  337.75
  63   0.4915     86.580  0.1115    96.354  343.17
  64   0.5140     86.740  0.1058    96.508  348.57
  65   0.5385     86.600  0.1099    96.436  353.95
  66   0.5242     86.620  0.1087    96.386  359.34
  67   0.5506     85.850  0.1046    96.528  364.74
  68   0.5267     86.920  0.1005    96.664  370.16
  69   0.5310     86.850  0.0972    96.762  375.72
  70   0.5037     87.150  0.0931    97.036  381.13
  71   0.5304     86.580  0.0899    96.964  386.52
  72   0.5375     86.860  0.0963    96.808  391.92
  73   0.5483     86.540  0.0926    96.910  397.35
  74   0.5796     86.440  0.0984    96.742  402.76
  75   0.5717     87.200  0.0922    96.876  408.18
  76   0.5456     86.840  0.0878    96.986  413.70
  77   0.5333     87.320  0.0837    97.174  419.11
  78   0.5227     87.260  0.0879    97.064  424.53
  79   0.4927     87.220  0.0901    97.046  429.94
  80   0.5219     86.980  0.0898    97.026  435.35
  81   0.5447     87.160  0.0752    97.454  440.79
  82   0.5266     87.500  0.0800    97.402  446.22
  83   0.5097     87.620  0.0767    97.512  451.74
  84   0.5309     87.340  0.0794    97.382  457.17
  85   0.5292     87.120  0.0827    97.198  462.57
