Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7661     28.810  2.1333    20.890  7.05
   2   1.5059     42.010  1.6683    34.438  12.47
   3   1.3275     50.010  1.4697    43.768  17.85
   4   1.2160     55.000  1.3180    51.052  23.40
   5   1.0921     60.530  1.1878    56.570  28.79
   6   0.9779     65.200  1.0775    61.930  34.18
   7   0.9008     68.170  0.9632    66.016  39.60
   8   0.8531     70.420  0.8778    69.466  44.99
   9   0.7697     72.860  0.8035    72.456  50.40
  10   0.7247     74.700  0.7377    74.674  55.81
  11   0.6963     76.290  0.6964    76.152  61.33
  12   0.6857     76.210  0.6498    78.122  66.73
  13   0.6981     75.530  0.6132    79.206  72.11
  14   0.6155     79.280  0.5716    80.688  77.53
  15   0.5914     80.050  0.5443    81.798  82.93
  16   0.5677     80.870  0.5105    82.864  88.35
  17   0.5823     80.620  0.4870    83.618  93.74
  18   0.5424     81.650  0.4671    84.400  99.28
  19   0.5607     80.930  0.4393    85.466  104.66
  20   0.5304     82.200  0.4141    86.330  110.08
  21   0.5193     82.510  0.4041    86.550  115.46
  22   0.5004     83.070  0.3831    87.248  120.85
  23   0.4948     83.080  0.3648    87.788  126.27
  24   0.4937     83.600  0.3477    88.408  131.69
  25   0.5216     82.810  0.3357    88.904  137.25
  26   0.5007     83.380  0.3249    89.206  142.64
  27   0.4992     83.470  0.3148    89.404  148.05
  28   0.4997     83.730  0.3018    89.928  153.43
  29   0.5099     82.960  0.2868    90.430  158.84
  30   0.4807     84.540  0.2767    90.804  164.24
  31   0.5022     83.700  0.2701    91.012  169.64
  32   0.4726     85.220  0.2528    91.650  175.20
  33   0.4876     84.820  0.2482    91.666  180.60
  34   0.4829     84.310  0.2413    91.866  185.97
  35   0.4628     85.310  0.2331    92.282  191.35
  36   0.4865     85.510  0.2193    92.692  196.76
  37   0.4664     85.500  0.2088    93.030  202.16
  38   0.5043     85.280  0.1996    93.382  207.59
  39   0.4862     85.200  0.1942    93.544  213.11
  40   0.4824     85.750  0.1849    93.794  218.53
  41   0.5163     85.020  0.1809    93.978  223.91
  42   0.4696     85.600  0.1810    94.042  229.31
  43   0.4563     85.940  0.1779    94.032  234.72
  44   0.4966     85.590  0.1579    94.712  240.12
  45   0.5153     85.440  0.1606    94.696  245.54
  46   0.5045     86.110  0.1509    94.920  250.92
  47   0.5108     85.660  0.1523    94.946  256.31
  48   0.5147     86.050  0.1431    95.256  261.71
  49   0.4720     86.530  0.1413    95.336  267.11
  50   0.5132     85.880  0.1380    95.384  272.53
  51   0.5234     86.040  0.1307    95.632  277.95
  52   0.5291     85.690  0.1381    95.516  283.38
  53   0.5391     85.430  0.1369    95.430  288.88
  54   0.4848     86.620  0.1255    95.824  294.31
  55   0.5328     86.170  0.1176    96.038  299.73
  56   0.5018     86.700  0.1098    96.352  305.12
  57   0.5206     86.610  0.1089    96.360  310.54
  58   0.5714     86.240  0.1028    96.496  315.96
  59   0.5483     86.260  0.1104    96.200  321.37
  60   0.5165     86.720  0.1041    96.538  326.78
  61   0.5240     86.550  0.1001    96.706  332.22
  62   0.5334     86.500  0.1058    96.466  337.63
  63   0.5671     86.140  0.0999    96.674  343.02
  64   0.5538     86.600  0.0935    96.898  348.43
  65   0.5551     86.770  0.0864    97.118  353.85
  66   0.5550     86.610  0.0905    97.026  359.38
  67   0.5579     86.350  0.0891    97.040  364.79
  68   0.5874     86.420  0.0901    97.024  370.21
  69   0.5668     86.610  0.0860    97.170  375.62
  70   0.5533     86.210  0.0856    97.058  381.04
  71   0.5451     86.740  0.0829    97.188  386.46
  72   0.5315     87.190  0.0800    97.348  391.89
  73   0.5321     86.440  0.0830    97.188  397.44
  74   0.5555     87.140  0.0771    97.536  402.87
  75   0.5341     86.850  0.0762    97.470  408.26
  76   0.5886     87.010  0.0695    97.626  413.66
  77   0.5842     87.080  0.0689    97.654  419.05
  78   0.5724     87.090  0.0715    97.592  424.46
  79   0.5314     87.250  0.0722    97.634  429.85
  80   0.5534     87.180  0.0671    97.820  435.37
  81   0.5212     87.490  0.0687    97.756  440.78
  82   0.5675     86.960  0.0674    97.792  446.19
  83   0.5756     86.900  0.0695    97.752  451.59
  84   0.6215     86.870  0.0689    97.770  457.00
  85   0.5616     87.300  0.0649    97.802  462.41
