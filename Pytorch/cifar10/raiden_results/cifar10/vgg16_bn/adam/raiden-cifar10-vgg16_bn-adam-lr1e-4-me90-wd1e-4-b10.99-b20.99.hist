Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7718     29.890  2.2087    18.674  7.34
   2   1.5339     41.140  1.7154    32.110  12.75
   3   1.3700     48.400  1.4933    42.710  18.18
   4   1.1913     55.700  1.3237    50.576  23.61
   5   1.1037     59.760  1.1988    56.324  29.01
   6   0.9807     64.900  1.0827    61.224  34.41
   7   0.9291     66.770  0.9754    65.210  39.94
   8   0.8557     69.160  0.8922    68.564  45.35
   9   0.8088     71.200  0.8299    70.862  50.77
  10   0.7641     73.590  0.7709    72.922  56.16
  11   0.7399     74.910  0.7144    75.356  61.58
  12   0.6906     76.270  0.6785    76.734  66.99
  13   0.6512     77.980  0.6320    78.278  72.41
  14   0.6209     78.830  0.5963    79.866  77.95
  15   0.6237     79.360  0.5716    80.556  83.38
  16   0.5946     80.240  0.5409    81.670  88.77
  17   0.5937     80.340  0.5115    82.782  94.18
  18   0.5748     81.160  0.4819    83.768  99.60
  19   0.5459     82.080  0.4596    84.300  105.04
  20   0.5655     81.980  0.4406    84.902  110.47
  21   0.5363     82.630  0.4207    85.672  115.99
  22   0.5506     82.290  0.3943    86.584  121.40
  23   0.5295     82.720  0.3788    87.076  126.82
  24   0.5290     83.340  0.3628    87.636  132.25
  25   0.5365     83.350  0.3461    88.306  137.65
  26   0.5557     83.270  0.3294    88.920  143.06
  27   0.5683     83.170  0.3231    88.986  148.46
  28   0.5668     83.580  0.3131    89.316  153.85
  29   0.5592     83.780  0.3018    89.850  159.41
  30   0.5040     84.730  0.2931    90.012  164.82
  31   0.5196     84.290  0.2835    90.374  170.24
  32   0.5828     83.030  0.2736    90.676  175.64
  33   0.5210     84.500  0.2571    91.244  181.05
  34   0.5802     84.300  0.2566    91.310  186.46
  35   0.5341     85.250  0.2385    92.050  191.87
  36   0.5738     84.500  0.2327    91.992  197.42
  37   0.5460     84.860  0.2238    92.418  202.86
  38   0.6357     83.910  0.2180    92.668  208.27
  39   0.5482     85.040  0.2129    92.824  213.66
  40   0.5858     84.980  0.2014    93.222  219.09
  41   0.5451     85.680  0.1863    93.716  224.54
  42   0.5079     85.680  0.1902    93.592  229.96
  43   0.5505     85.300  0.1810    93.846  235.51
  44   0.5475     85.750  0.1743    94.058  240.92
  45   0.5437     85.690  0.1767    93.982  246.37
  46   0.5287     86.070  0.1658    94.416  251.81
  47   0.5480     85.850  0.1593    94.624  257.24
  48   0.5937     85.640  0.1558    94.860  262.66
  49   0.5784     85.860  0.1529    94.860  268.09
  50   0.5203     86.100  0.1523    95.082  273.63
  51   0.5484     85.900  0.1426    95.222  279.04
  52   0.5480     85.960  0.1363    95.312  284.46
  53   0.5802     85.580  0.1331    95.548  289.89
  54   0.5607     85.960  0.1245    95.818  295.33
  55   0.5337     86.220  0.1237    95.752  300.74
  56   0.5306     86.390  0.1220    95.878  306.16
  57   0.5721     86.020  0.1169    95.960  311.71
  58   0.5673     85.800  0.1215    95.948  317.16
  59   0.5741     86.190  0.1130    96.118  322.60
  60   0.6144     85.460  0.1144    96.222  328.02
  61   0.5388     86.600  0.1185    96.074  333.48
  62   0.5398     86.440  0.1077    96.312  338.91
  63   0.5440     86.420  0.1068    96.400  344.36
  64   0.5704     86.770  0.1066    96.486  349.97
  65   0.6279     85.680  0.0983    96.726  355.39
  66   0.5761     86.630  0.1009    96.600  360.80
  67   0.5953     86.230  0.0971    96.720  366.27
  68   0.5788     86.620  0.0989    96.702  371.70
  69   0.5533     86.580  0.1021    96.658  377.14
  70   0.5334     86.630  0.0914    96.990  382.59
  71   0.5955     86.520  0.0797    97.236  388.17
  72   0.5678     86.820  0.0867    97.008  393.62
  73   0.5980     86.390  0.0851    97.108  399.06
  74   0.5698     86.210  0.0833    97.108  404.49
  75   0.5641     86.680  0.0787    97.392  409.96
  76   0.6346     86.790  0.0734    97.516  415.39
  77   0.6233     86.440  0.0818    97.256  420.98
  78   0.5696     87.190  0.0751    97.442  426.41
  79   0.5740     86.420  0.0752    97.576  431.83
  80   0.6023     86.930  0.0758    97.464  437.27
  81   0.5911     87.130  0.0769    97.424  442.70
  82   0.5652     86.920  0.0733    97.516  448.16
  83   0.5711     86.630  0.0734    97.528  453.59
  84   0.6141     86.510  0.0719    97.590  459.15
  85   0.5599     86.870  0.0706    97.676  464.58
  86   0.5614     87.190  0.0671    97.772  470.04
  87   0.5747     87.200  0.0703    97.666  475.46
  88   0.5872     87.130  0.0638    97.848  480.88
  89   0.6170     87.180  0.0608    97.946  486.32
  90   0.5786     87.360  0.0623    97.802  491.76
