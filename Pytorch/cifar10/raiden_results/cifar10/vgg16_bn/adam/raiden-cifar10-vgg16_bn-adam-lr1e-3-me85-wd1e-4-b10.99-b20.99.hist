Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2787     11.960  3.9288    10.922  6.98
   2   2.2349     13.200  2.2921    11.590  12.43
   3   2.1828     15.100  2.2579    13.028  17.88
   4   2.1476     16.770  2.2047    14.764  23.31
   5   1.9662     19.980  2.1160    16.618  28.75
   6   1.8864     20.180  1.9723    19.524  34.17
   7   1.8488     23.890  1.8911    22.306  39.60
   8   1.7107     31.670  1.7898    27.134  45.18
   9   1.5483     40.150  1.6834    34.268  50.64
  10   1.4416     44.410  1.5088    41.956  56.08
  11   1.3943     47.760  1.3821    48.004  61.54
  12   1.2423     55.050  1.2689    53.486  66.99
  13   1.1234     60.260  1.1637    58.208  72.45
  14   1.0470     63.170  1.1127    61.614  77.89
  15   1.1383     58.960  1.2298    57.864  83.34
  16   1.0513     62.450  1.1252    61.762  88.78
  17   1.2186     60.720  1.1305    62.046  94.22
  18   1.4035     60.700  1.1568    62.076  99.68
  19   1.0499     62.840  1.1074    62.958  105.11
  20   0.9916     66.010  1.0139    65.376  110.55
  21   1.0050     64.920  0.9811    66.708  115.99
  22   0.8833     69.720  0.9741    67.194  121.44
  23   1.3505     67.970  0.8781    69.714  126.88
  24   1.2659     65.040  0.9649    68.706  132.44
  25   1.0463     64.900  1.0709    65.442  137.91
  26   0.8853     69.260  0.9601    67.674  143.38
  27   0.8262     72.130  0.8577    70.780  148.82
  28   0.6942     75.620  0.7394    74.266  154.27
  29   0.8585     74.080  0.7507    74.708  159.70
  30   1.0303     73.330  0.7781    74.948  165.13
  31   0.8383     71.700  0.9387    70.356  170.69
  32   0.7444     74.460  0.7701    74.208  176.13
  33   0.7549     74.800  0.7151    76.278  181.55
  34   0.6225     79.200  0.6530    78.128  186.99
  35   0.5982     80.170  0.5912    80.076  192.43
  36   0.5710     80.840  0.5546    81.442  197.86
  37   0.5963     80.420  0.5553    81.608  203.30
  38   0.5761     81.160  0.5146    83.302  208.88
  39   0.5998     80.210  0.5592    82.368  214.33
  40   0.6750     78.720  0.5320    82.944  219.81
  41   0.6339     78.800  0.6754    78.676  225.24
  42   0.5296     82.580  0.5527    82.224  230.71
  43   0.5218     83.060  0.4807    84.272  236.15
  44   0.4945     84.070  0.4591    85.260  241.60
  45   0.5730     82.750  0.4312    86.154  247.17
  46   0.4759     84.230  0.4403    85.764  252.60
  47   0.4674     85.100  0.4034    87.018  258.06
  48   0.4564     85.400  0.3834    87.740  263.50
  49   0.4411     85.760  0.3593    88.362  268.92
  50   0.4766     85.050  0.3551    88.530  274.39
  51   0.4436     86.260  0.3583    88.548  279.84
  52   0.4395     86.320  0.3345    89.206  285.42
  53   0.4600     85.980  0.3095    90.116  290.84
  54   0.4888     85.550  0.3088    90.238  296.29
  55   0.4481     86.470  0.3180    89.814  301.74
  56   0.4344     87.210  0.2854    90.910  307.19
  57   0.4139     86.910  0.2744    91.338  312.63
  58   0.4304     87.320  0.2588    91.816  318.06
  59   0.4309     87.160  0.2485    91.962  323.64
  60   0.4270     87.810  0.2457    92.172  329.07
  61   0.3898     87.770  0.2269    92.794  334.51
  62   0.4582     87.210  0.2205    92.890  339.93
  63   0.4305     87.540  0.2251    92.730  345.38
  64   0.4028     87.830  0.2163    93.098  350.81
  65   0.4285     87.620  0.2024    93.524  356.39
  66   0.4807     87.580  0.1994    93.676  361.83
  67   0.4227     88.070  0.2231    93.066  367.28
  68   0.4182     87.900  0.2029    93.506  372.70
  69   0.4723     88.430  0.1977    93.924  378.14
  70   0.4296     87.820  0.2188    93.112  383.58
  71   0.4040     88.170  0.2002    93.662  389.01
  72   0.4339     88.080  0.1829    94.310  394.59
  73   0.4246     87.840  0.1777    94.418  400.02
  74   0.4266     88.820  0.1770    94.556  405.46
  75   0.4157     88.700  0.1725    94.674  410.92
  76   0.4333     88.330  0.1766    94.504  416.35
  77   0.4723     87.910  0.1747    94.696  421.81
  78   0.4164     88.930  0.1635    94.866  427.25
  79   0.4131     88.900  0.1565    95.130  432.83
  80   0.4029     88.860  0.1541    95.152  438.29
  81   0.4191     89.010  0.1567    95.092  443.73
  82   0.4823     88.040  0.1446    95.534  449.14
  83   0.4150     89.210  0.1468    95.448  454.56
  84   0.3955     89.020  0.1467    95.412  460.02
  85   0.4349     88.440  0.1392    95.586  465.46
