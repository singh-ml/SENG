Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4790     45.730  1.9258    28.450  7.14
   2   1.1363     58.430  1.3655    50.004  12.54
   3   1.0438     62.420  1.1587    58.728  18.12
   4   0.9501     66.400  1.0227    63.956  23.55
   5   0.9007     67.730  0.9218    67.908  28.94
   6   0.8296     71.000  0.8385    71.036  34.37
   7   0.7599     73.800  0.7676    73.772  39.80
   8   0.7585     74.890  0.7102    75.728  45.25
   9   0.7022     75.770  0.6618    77.370  50.81
  10   0.6557     77.660  0.6277    78.644  56.23
  11   0.6240     78.530  0.5875    80.094  61.66
  12   0.6393     78.950  0.5597    81.066  67.06
  13   0.6159     79.140  0.5285    82.090  72.49
  14   0.6365     79.270  0.5013    82.954  77.89
  15   0.5942     79.930  0.4760    83.948  83.31
  16   0.5416     81.760  0.4588    84.580  88.85
  17   0.5695     80.820  0.4337    85.358  94.27
  18   0.5413     81.880  0.4174    85.826  99.71
  19   0.5051     82.970  0.3975    86.354  105.14
  20   0.5600     81.680  0.3848    86.938  110.58
  21   0.5635     82.470  0.3688    87.484  115.98
  22   0.5374     82.320  0.3465    88.202  121.38
  23   0.6087     81.290  0.3386    88.402  126.86
  24   0.6137     80.550  0.3215    88.974  132.29
  25   0.4852     84.330  0.3088    89.530  137.74
  26   0.5620     83.160  0.3001    89.796  143.15
  27   0.5946     82.090  0.2871    90.340  148.57
  28   0.5617     83.600  0.2705    90.844  154.01
  29   0.5513     83.610  0.2649    91.002  159.44
  30   0.4979     84.480  0.2580    91.174  164.87
  31   0.5448     84.320  0.2379    91.848  170.40
  32   0.5113     84.630  0.2410    91.752  175.82
  33   0.6460     81.690  0.2356    92.092  181.23
  34   0.5605     84.040  0.2262    92.394  186.64
  35   0.5059     84.700  0.2135    92.736  192.07
  36   0.5415     84.710  0.2043    93.062  197.50
  37   0.5102     84.600  0.2033    93.206  203.05
  38   0.5836     84.060  0.2041    93.136  208.45
  39   0.5094     85.070  0.1871    93.688  213.86
  40   0.6030     83.690  0.1812    93.868  219.31
  41   0.5490     85.020  0.1761    93.952  224.75
  42   0.5187     85.380  0.1719    94.128  230.17
  43   0.5476     84.730  0.1697    94.180  235.57
  44   0.5678     85.260  0.1609    94.508  241.14
  45   0.5456     85.830  0.1529    94.838  246.57
  46   0.5391     85.740  0.1564    94.608  251.98
  47   0.5473     85.700  0.1445    95.158  257.39
  48   0.5755     85.110  0.1476    94.896  262.81
  49   0.5756     85.300  0.1404    95.230  268.24
  50   0.5693     85.540  0.1353    95.442  273.67
  51   0.5432     86.470  0.1288    95.628  279.24
  52   0.6139     85.240  0.1321    95.598  284.66
  53   0.5213     86.510  0.1264    95.754  290.09
  54   0.5510     85.950  0.1227    95.870  295.50
  55   0.6039     85.150  0.1204    95.900  300.95
  56   0.5824     85.830  0.1169    96.114  306.38
  57   0.5757     86.200  0.1112    96.182  311.81
  58   0.5542     86.370  0.1129    96.164  317.35
  59   0.5702     85.970  0.1077    96.386  322.76
  60   0.5784     85.920  0.1055    96.508  328.16
  61   0.5926     86.330  0.1028    96.532  333.60
  62   0.6416     84.990  0.1030    96.550  339.00
  63   0.5395     86.740  0.0990    96.648  344.42
  64   0.6201     85.670  0.0940    96.800  349.86
  65   0.5808     85.730  0.0968    96.772  355.28
  66   0.6452     85.350  0.0948    96.804  360.82
  67   0.6215     85.970  0.0932    96.924  366.22
  68   0.5625     87.040  0.0907    96.982  371.66
  69   0.6153     85.940  0.0886    97.022  377.09
  70   0.5719     86.250  0.0943    96.928  382.54
  71   0.6029     85.560  0.0858    97.118  387.99
  72   0.5808     87.050  0.0891    97.008  393.44
  73   0.5072     86.860  0.0883    97.090  398.97
  74   0.5687     86.630  0.0836    97.206  404.38
  75   0.5904     87.090  0.0806    97.364  409.80
  76   0.5329     87.600  0.0805    97.278  415.22
  77   0.5360     87.070  0.0802    97.364  420.66
  78   0.7182     85.570  0.0740    97.474  426.11
  79   0.5650     87.260  0.0765    97.344  431.65
  80   0.7185     85.710  0.0759    97.456  437.05
  81   0.6470     85.850  0.0719    97.560  442.47
  82   0.5607     87.510  0.0764    97.440  447.88
  83   0.6336     86.680  0.0708    97.638  453.29
  84   0.6314     86.900  0.0706    97.620  458.70
  85   0.6596     86.710  0.0716    97.698  464.13
  86   0.6033     87.020  0.0695    97.676  469.54
  87   0.6329     87.050  0.0672    97.780  475.00
  88   0.5848     87.180  0.0693    97.674  480.45
  89   0.5633     87.320  0.0690    97.770  485.87
  90   0.5749     86.830  0.0653    97.806  491.27
