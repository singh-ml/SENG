Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.1659     16.920  3.7064    12.692  7.19
   2   2.0028     19.260  2.0273    17.822  12.64
   3   1.9092     22.090  1.8871    21.820  18.08
   4   1.9703     29.400  1.7747    27.546  23.54
   5   1.6285     34.850  1.6182    35.700  28.99
   6   1.5655     41.390  1.4690    41.834  34.45
   7   1.4256     52.270  1.4399    45.700  40.02
   8   1.1914     57.430  1.2594    55.390  45.47
   9   1.7136     49.030  1.3478    56.252  50.89
  10   1.0267     62.780  1.3426    53.644  56.32
  11   1.1517     61.320  1.0738    63.316  61.75
  12   0.9936     64.880  1.1867    62.930  67.19
  13   0.9693     65.880  1.3715    58.636  72.62
  14   1.5976     60.130  1.3087    61.722  78.20
  15   1.6513     57.360  1.6261    56.516  83.62
  16   1.0283     63.840  1.3855    58.380  89.04
  17   0.9384     69.570  0.9472    67.084  94.49
  18   0.8266     71.320  0.8511    70.746  99.91
  19   0.9196     70.260  0.7982    72.860  105.35
  20   3.5709     60.140  0.8070    74.202  110.83
  21   0.9762     68.550  0.9723    71.730  116.31
  22   1.0103     67.200  0.8071    73.674  121.76
  23   0.7162     77.570  0.9193    71.784  127.19
  24   0.6748     77.990  0.6799    77.512  132.62
  25   0.6308     79.730  0.6269    79.370  138.05
  26   0.7770     76.580  0.7369    77.206  143.50
  27   0.7263     77.760  0.7209    77.884  149.07
  28   0.5661     81.380  0.5778    81.272  154.49
  29   0.5972     81.280  0.5333    82.354  159.94
  30   0.7490     75.570  0.6448    81.070  165.40
  31   0.6037     80.820  0.7017    79.602  170.86
  32   0.5606     82.150  0.4983    83.750  176.30
  33   0.5230     83.280  0.4685    84.962  181.75
  34   0.6046     80.510  0.5146    83.846  187.33
  35   0.5093     83.960  0.4541    85.512  192.75
  36   0.5623     82.240  0.4214    86.580  198.19
  37   0.7501     77.470  0.4263    86.478  203.62
  38   0.4848     85.270  0.4080    86.996  209.06
  39   0.5589     83.760  0.3817    87.830  214.50
  40   0.5078     84.770  0.3656    88.276  219.97
  41   0.4697     86.020  0.4022    87.606  225.55
  42   0.5375     83.570  0.3497    88.960  231.02
  43   0.5053     85.150  0.3308    89.548  236.45
  44   0.5330     84.500  0.3251    89.612  241.87
  45   0.6354     83.120  0.3197    89.838  247.33
  46   0.4243     87.100  0.3361    89.628  252.77
  47   0.4287     87.200  0.2892    90.942  258.22
  48   0.4720     86.140  0.2943    90.842  263.82
  49   0.4497     87.000  0.2769    91.242  269.28
  50   0.4076     87.530  0.3180    90.256  274.73
  51   0.3944     88.150  0.2597    91.902  280.19
  52   0.4638     86.730  0.2543    91.996  285.65
  53   0.4597     86.840  0.2476    92.088  291.10
  54   0.4512     87.360  0.2488    92.186  296.55
  55   0.4525     87.510  0.2555    92.040  302.15
  56   0.4304     87.450  0.2362    92.534  307.61
  57   0.4325     87.920  0.2301    92.724  313.07
  58   0.4683     86.670  0.2152    93.236  318.53
  59   0.4499     87.600  0.2210    92.896  323.98
  60   0.4193     88.140  0.2593    92.088  329.43
  61   0.4699     87.980  0.2075    93.450  334.89
  62   0.5039     86.690  0.1950    93.866  340.46
  63   0.4479     87.270  0.2082    93.778  345.89
  64   0.4297     87.480  0.2254    93.042  351.33
  65   0.4928     87.620  0.1861    94.160  356.77
  66   0.4607     87.620  0.1875    94.142  362.25
  67   0.4449     87.880  0.1837    94.152  367.70
  68   0.5882     85.210  0.1909    94.154  373.31
  69   0.4304     88.210  0.1721    94.482  378.76
  70   0.5099     87.280  0.1782    94.560  384.22
  71   0.4234     88.860  0.2019    93.820  389.67
  72   0.4845     87.020  0.1641    94.832  395.13
  73   0.4961     87.790  0.1698    94.774  400.56
  74   0.5350     86.960  0.1615    95.016  406.01
  75   0.5146     87.210  0.1600    94.974  411.59
  76   0.4484     88.420  0.1554    95.158  417.03
  77   0.3902     89.060  0.1586    95.202  422.49
  78   0.4855     88.170  0.1595    95.026  427.95
  79   0.4287     88.980  0.1566    95.118  433.39
  80   0.4478     88.300  0.1475    95.506  438.86
  81   0.4030     89.390  0.1521    95.306  444.32
  82   0.4419     88.960  0.1512    95.362  449.89
  83   0.4457     89.190  0.1445    95.570  455.33
  84   0.5465     87.440  0.1410    95.622  460.76
  85   0.4412     88.350  0.1463    95.550  466.19
