Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2393     13.270  2.4733    12.328  7.02
   2   1.9435     20.610  2.0992    16.848  12.45
   3   1.8041     27.000  1.8943    21.516  17.86
   4   1.6769     34.980  1.7695    28.604  23.30
   5   1.5463     39.140  1.6178    35.024  28.71
   6   1.4179     45.310  1.4760    41.200  34.11
   7   1.2661     51.830  1.3438    47.830  39.55
   8   1.1466     56.910  1.2286    53.982  45.07
   9   1.0450     62.810  1.0959    60.060  50.47
  10   0.9596     66.440  0.9785    65.490  55.90
  11   0.9027     68.960  0.9066    68.272  61.32
  12   0.8210     71.600  0.8385    70.890  66.76
  13   0.7645     74.030  0.7780    73.306  72.21
  14   0.7194     75.800  0.7096    76.246  77.63
  15   0.6681     77.560  0.6643    77.818  83.19
  16   0.6682     77.960  0.6312    78.944  88.62
  17   0.6274     79.260  0.5862    80.578  94.07
  18   0.6623     79.270  0.5540    81.758  99.51
  19   0.6048     80.330  0.5426    82.120  104.93
  20   0.5763     81.650  0.5195    83.048  110.37
  21   0.5714     81.980  0.4866    83.990  115.94
  22   0.5389     82.560  0.4548    85.242  121.40
  23   0.5437     82.480  0.4447    85.588  126.84
  24   0.5812     81.950  0.4636    84.938  132.26
  25   0.5667     83.180  0.4405    85.764  137.68
  26   0.5526     82.870  0.4986    86.074  143.10
  27   0.5752     83.280  0.4674    85.168  148.55
  28   0.5486     82.960  0.4537    85.650  154.00
  29   0.5683     81.630  0.5081    84.382  159.58
  30   0.5261     83.310  0.4389    85.574  165.00
  31   0.4852     84.590  0.3822    87.504  170.44
  32   0.4653     84.840  0.3577    88.202  175.89
  33   0.5472     84.130  0.3422    88.678  181.30
  34   1.0840     82.490  0.3608    88.748  186.73
  35   0.5145     83.050  0.4102    86.834  192.29
  36   0.5922     83.580  0.3944    87.602  197.75
  37   0.4895     83.870  0.3890    87.888  203.21
  38   0.5272     84.220  0.3895    87.294  208.64
  39   0.6174     83.740  0.4047    87.632  214.05
  40   0.5280     84.160  0.4073    87.790  219.50
  41   0.5401     82.910  0.4959    85.824  224.95
  42   0.5191     83.550  0.4244    86.764  230.51
  43   0.4479     85.500  0.3487    88.450  235.94
  44   0.4713     85.230  0.3103    89.718  241.35
  45   0.4637     85.920  0.2780    90.792  246.77
  46   0.4749     86.100  0.2668    91.258  252.22
  47   0.4790     86.340  0.2681    91.050  257.64
  48   0.4453     86.300  0.2526    91.800  263.07
  49   0.4707     86.540  0.2314    92.336  268.62
  50   0.4379     87.120  0.2266    92.560  274.03
  51   0.4528     86.490  0.2453    92.144  279.46
  52   0.5061     86.500  0.2258    92.582  284.89
  53   0.5057     86.260  0.2188    92.882  290.31
  54   0.4633     86.380  0.2154    92.860  295.76
  55   0.4810     86.380  0.1997    93.480  301.17
  56   0.4412     87.260  0.1828    93.946  306.74
  57   0.4891     86.500  0.1833    93.886  312.16
  58   0.4704     87.370  0.1771    94.236  317.61
  59   0.4681     86.950  0.1744    94.196  323.06
  60   0.5014     87.190  0.1682    94.460  328.47
  61   0.4333     87.760  0.1562    94.856  333.90
  62   0.4419     87.920  0.1483    95.010  339.34
  63   0.4697     87.680  0.1406    95.286  344.91
  64   0.4445     87.690  0.1382    95.478  350.35
  65   0.4565     87.350  0.1437    95.426  355.77
  66   0.4537     87.960  0.1357    95.548  361.22
  67   0.4673     87.160  0.1326    95.678  366.65
  68   0.4579     87.790  0.1280    95.846  372.09
  69   0.4345     88.310  0.1247    95.772  377.55
  70   0.4424     88.040  0.1152    96.158  383.11
  71   0.4714     88.110  0.1110    96.434  388.55
  72   0.4962     87.850  0.1172    96.248  393.96
  73   0.4583     87.850  0.1187    96.172  399.38
  74   0.4425     88.360  0.1152    96.190  404.84
  75   0.4715     88.010  0.1112    96.368  410.27
  76   0.4488     88.850  0.1087    96.480  415.72
  77   0.4787     88.170  0.0975    96.826  421.28
  78   0.4807     88.040  0.0990    96.826  426.71
  79   0.4792     88.590  0.0929    96.976  432.15
  80   0.4717     88.360  0.0909    96.938  437.58
  81   0.4581     88.900  0.0918    97.004  442.99
  82   0.4509     88.610  0.0903    97.108  448.41
  83   0.4818     88.350  0.0888    97.084  453.85
  84   0.4908     88.410  0.0863    97.110  459.41
  85   0.5181     87.860  0.0830    97.262  464.82
