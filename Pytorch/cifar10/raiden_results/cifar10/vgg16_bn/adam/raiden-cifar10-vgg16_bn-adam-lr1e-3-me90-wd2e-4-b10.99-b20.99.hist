Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2522     15.270  3.9561    11.254  7.10
   2   2.1629     16.270  2.2759    13.382  12.54
   3   2.1510     15.950  2.2120    14.824  18.09
   4   2.0290     19.200  2.1626    16.286  23.52
   5   1.9174     21.600  2.0035    19.358  28.95
   6   1.8538     24.330  1.8705    23.230  34.37
   7   1.6920     35.600  1.7508    29.316  39.80
   8   1.4762     43.180  1.6133    37.568  45.24
   9   1.5592     44.770  1.4664    44.694  50.68
  10   1.3051     51.720  1.3316    50.938  56.23
  11   1.1363     58.410  1.2290    55.624  61.69
  12   1.0409     64.500  1.1116    60.448  67.11
  13   1.2355     59.720  1.0937    62.982  72.54
  14   1.3034     62.850  1.0925    63.070  77.96
  15   1.0559     64.710  1.1484    62.644  83.41
  16   1.1933     61.850  1.1053    64.804  88.87
  17   1.3280     61.120  1.1555    61.838  94.44
  18   0.9742     66.340  1.1798    61.246  99.90
  19   0.9750     66.850  1.0012    66.540  105.35
  20   0.8925     69.630  0.9131    68.800  110.77
  21   0.7381     75.160  0.7887    72.982  116.21
  22   1.1168     70.700  0.7698    74.338  121.64
  23   0.7391     75.140  0.8003    73.098  127.09
  24   0.7490     75.200  0.7109    76.250  132.61
  25   0.6463     78.410  0.6769    77.768  138.07
  26   0.5972     79.540  0.6359    79.060  143.49
  27   0.6114     79.590  0.6151    79.702  148.94
  28   0.6501     79.470  0.5590    81.540  154.39
  29   0.5490     81.900  0.5308    82.596  159.81
  30   0.5216     82.810  0.5045    83.554  165.37
  31   0.5450     82.550  0.4824    84.118  170.79
  32   0.4872     83.940  0.4650    84.838  176.23
  33   0.5076     83.590  0.4220    86.238  181.67
  34   0.4737     84.860  0.4092    86.662  187.12
  35   0.4533     85.710  0.3944    87.408  192.57
  36   0.4770     85.260  0.3941    87.214  197.99
  37   0.4745     85.040  0.3958    87.288  203.54
  38   0.4488     86.170  0.3773    87.962  208.96
  39   0.4596     86.250  0.3599    88.534  214.42
  40   0.4536     85.850  0.3351    89.298  219.84
  41   0.4250     86.960  0.3208    89.738  225.28
  42   0.4210     87.320  0.3135    90.088  230.69
  43   0.4250     86.950  0.3065    90.252  236.15
  44   0.4232     86.800  0.2898    90.686  241.73
  45   0.3973     87.460  0.2782    91.032  247.15
  46   0.4216     87.120  0.2728    91.318  252.58
  47   0.4136     87.530  0.2578    91.714  258.04
  48   0.4275     87.750  0.2543    92.028  263.46
  49   0.4185     88.100  0.2574    91.968  268.89
  50   0.4222     87.530  0.2480    92.098  274.35
  51   0.4688     86.200  0.2395    92.340  279.91
  52   0.4121     88.030  0.2439    92.326  285.33
  53   0.4078     87.900  0.2332    92.608  290.79
  54   0.3967     87.970  0.2293    92.764  296.21
  55   0.4461     87.390  0.2281    92.880  301.64
  56   0.4112     88.150  0.2153    93.128  307.10
  57   0.4202     88.500  0.2060    93.562  312.54
  58   0.4053     88.290  0.2000    93.760  317.97
  59   0.3919     88.520  0.2039    93.554  323.39
  60   0.4247     87.970  0.2025    93.600  328.84
  61   0.4019     88.500  0.1982    93.920  334.29
  62   0.3791     88.840  0.2164    93.358  339.72
  63   0.4227     88.220  0.2057    93.664  345.16
  64   0.4035     88.540  0.2004    93.734  350.59
  65   0.4028     88.890  0.1891    94.108  356.17
  66   0.3887     89.060  0.1824    94.284  361.61
  67   0.4060     88.840  0.1809    94.408  367.03
  68   0.3913     89.290  0.1806    94.492  372.45
  69   0.3738     89.640  0.1726    94.630  377.89
  70   0.4387     87.710  0.1741    94.458  383.37
  71   0.3768     89.110  0.1767    94.638  388.82
  72   0.4351     88.320  0.1761    94.568  394.40
  73   0.3801     89.100  0.1714    94.716  399.86
  74   0.4047     88.880  0.1665    94.890  405.30
  75   0.4083     89.020  0.1625    94.978  410.76
  76   0.3891     89.170  0.1617    94.914  416.18
  77   0.3783     89.490  0.1603    95.014  421.62
  78   0.4149     88.890  0.1616    94.882  427.18
  79   0.4397     88.400  0.1528    95.358  432.64
  80   0.3977     88.990  0.1609    95.034  438.07
  81   0.4304     88.850  0.1479    95.404  443.53
  82   0.3708     89.340  0.1593    95.040  448.98
  83   0.3940     89.080  0.1569    95.166  454.44
  84   0.3853     89.470  0.1593    95.102  459.88
  85   0.4165     88.640  0.1491    95.230  465.44
  86   0.4187     88.910  0.1463    95.470  470.87
  87   0.3719     89.560  0.1459    95.382  476.29
  88   0.3816     89.570  0.1513    95.366  481.73
  89   0.3969     89.530  0.1484    95.348  487.16
  90   0.3994     90.210  0.1384    95.654  492.59
