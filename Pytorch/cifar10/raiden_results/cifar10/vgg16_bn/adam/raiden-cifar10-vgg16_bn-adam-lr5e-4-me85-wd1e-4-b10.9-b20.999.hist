Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.1299     17.290  2.8481    11.640  7.05
   2   1.8810     23.550  1.9751    19.620  12.46
   3   1.6274     34.880  1.7469    28.222  17.85
   4   1.4910     43.060  1.5408    37.840  23.37
   5   1.2862     49.910  1.3551    46.870  28.78
   6   1.1500     57.380  1.1877    53.750  34.19
   7   1.0115     64.850  1.0571    60.196  39.59
   8   0.9640     68.520  0.9150    68.546  45.00
   9   0.8636     72.320  0.8078    73.134  50.41
  10   0.9061     72.200  0.7283    76.382  55.82
  11   0.8289     75.190  0.6873    78.072  61.35
  12   0.7627     76.380  0.6426    79.800  66.76
  13   0.6784     79.480  0.5810    81.674  72.16
  14   0.7177     78.390  0.6618    80.096  77.58
  15   0.5909     81.840  0.5575    82.702  83.00
  16   0.6068     81.980  0.4979    84.680  88.41
  17   0.5639     82.330  0.4640    85.412  93.95
  18   0.7083     77.870  0.4633    85.526  99.35
  19   0.6966     78.730  0.6977    79.568  104.75
  20   6.6935     55.200  0.6269    81.546  110.18
  21   1.5518     69.360  0.6150    80.902  115.58
  22   0.4461     85.640  0.4703    85.344  120.99
  23   0.5440     82.700  0.3979    87.516  126.38
  24   0.4186     86.720  0.3674    88.378  131.95
  25   0.4673     85.150  0.3446    89.144  137.36
  26   0.4271     86.290  0.3335    89.592  142.76
  27   0.4214     86.760  0.3171    89.976  148.17
  28   0.4405     86.390  0.3096    90.126  153.58
  29   0.4146     86.500  0.2950    90.780  158.99
  30   0.4015     87.360  0.2848    91.028  164.41
  31   0.8027     73.630  0.8327    76.204  169.82
  32   0.4704     84.730  0.5181    83.236  175.26
  33   0.4642     84.980  0.6159    81.382  180.70
  34   0.4332     86.280  0.3626    88.366  186.13
  35   0.3967     87.170  0.3110    90.166  191.53
  36   0.3825     87.570  0.2794    91.014  196.95
  37   0.3889     88.050  0.2616    91.440  202.49
  38   0.3774     88.100  0.2493    92.074  207.91
  39   0.4023     87.440  0.2339    92.482  213.34
  40   0.3866     87.950  0.2305    92.540  218.76
  41   0.3665     88.320  0.2225    92.936  224.20
  42   0.4104     87.420  0.2142    93.202  229.62
  43   0.4099     88.050  0.2095    93.258  235.04
  44   0.4143     88.010  0.1994    93.618  240.52
  45   0.3930     88.120  0.2036    93.568  245.97
  46   0.4066     88.250  0.2003    93.696  251.40
  47   0.4250     87.400  0.1916    93.938  256.84
  48   0.4459     87.250  0.1870    94.050  262.26
  49   0.4376     87.510  0.1855    94.098  267.69
  50   0.4226     88.710  0.1818    94.328  273.13
  51   0.4389     87.630  0.1704    94.592  278.70
  52   0.4422     87.940  0.1659    94.598  284.10
  53   143.3347     33.860  0.1653    94.828  289.52
  54   1.3582     59.390  1.9062    50.574  294.95
  55   0.7558     72.730  1.0054    65.792  300.35
  56   0.6169     79.120  0.6754    77.298  305.78
  57   0.4873     84.540  0.4845    83.956  311.20
  58   0.4690     85.570  0.3736    88.024  316.73
  59   0.4550     86.660  0.3007    90.292  322.16
  60   0.4250     87.740  0.2507    91.726  327.61
  61   0.4484     86.790  0.2251    92.620  333.04
  62   0.4100     88.830  0.2004    93.342  338.46
  63   0.4489     87.610  0.1794    94.188  343.90
  64   0.4249     88.100  0.1679    94.556  349.33
  65   0.3780     89.350  0.1617    94.812  354.88
  66   0.4129     88.670  0.1495    95.166  360.33
  67   0.4269     88.830  0.1429    95.496  365.74
  68   0.4259     88.730  0.1370    95.562  371.18
  69   0.4131     88.960  0.1339    95.634  376.61
  70   0.4367     88.920  0.1376    95.664  382.04
  71   0.4716     87.940  0.1255    95.946  387.56
  72   0.4397     89.260  0.1282    95.932  392.96
  73   0.4327     88.910  0.1205    96.216  398.39
  74   0.4460     88.900  0.1220    96.086  403.82
  75   33.4932     34.090  0.8251    83.940  409.25
  76   0.5962     80.570  1.0065    67.286  414.67
  77   0.4592     85.500  0.4554    85.344  420.11
  78   0.4303     86.800  0.3092    89.938  425.55
  79   0.4004     88.100  0.2231    92.722  431.09
  80   0.4136     88.290  0.1895    93.802  436.55
  81   0.4363     88.530  0.1601    94.704  441.94
  82   0.3751     89.290  0.1581    95.434  447.36
  83   0.4348     88.920  0.1305    95.842  452.79
  84   0.4212     89.060  0.1216    96.120  458.22
  85   0.4380     89.310  0.1123    96.390  463.78
