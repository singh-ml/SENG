Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2753     12.060  2.9107    11.510  7.06
   2   2.2408     12.750  2.2647    12.484  12.48
   3   2.1071     16.330  2.1892    14.642  17.91
   4   1.9869     18.130  2.0720    17.032  23.34
   5   1.8691     20.950  1.9197    20.290  28.90
   6   1.8223     25.030  1.8700    22.014  34.31
   7   1.8072     28.460  1.8223    25.604  39.74
   8   1.7305     32.890  1.7336    30.450  45.18
   9   1.4943     39.870  1.6179    35.532  50.60
  10   1.4533     45.420  1.4854    41.990  56.02
  11   1.2363     51.980  1.3616    48.640  61.43
  12   1.1610     57.020  1.2467    54.054  66.96
  13   1.1131     61.240  1.1268    58.942  72.41
  14   0.9827     65.480  1.0420    63.132  77.82
  15   0.9369     67.230  0.9584    66.628  83.23
  16   0.8536     71.840  0.8614    70.446  88.66
  17   0.7672     73.310  0.7934    73.178  94.11
  18   0.7690     74.490  0.7601    74.736  99.55
  19   0.9327     72.600  0.7635    75.102  105.12
  20   0.6830     77.460  0.7161    76.768  110.53
  21   0.6568     79.760  0.6384    79.274  115.98
  22   0.5713     81.490  0.5820    81.324  121.43
  23   0.5872     81.230  0.5412    82.638  126.86
  24   0.5957     82.260  0.5228    83.240  132.31
  25   0.9688     68.740  0.7829    76.686  137.74
  26   0.7112     76.250  0.8548    73.802  143.31
  27   1.0018     65.660  1.1573    62.620  148.73
  28   0.7411     75.060  0.8537    71.096  154.16
  29   0.6933     77.420  0.6920    77.134  159.61
  30   0.6008     80.470  0.6127    79.862  165.04
  31   0.5411     81.920  0.5499    81.832  170.46
  32   0.5103     83.220  0.5045    83.580  176.05
  33   0.4918     83.790  0.4668    84.708  181.52
  34   0.5023     83.600  0.4502    85.444  186.95
  35   0.5328     84.020  0.4241    86.358  192.42
  36   0.5213     83.480  0.4448    85.690  197.83
  37   0.4760     84.120  0.4216    86.410  203.26
  38   0.9507     81.080  0.4534    85.868  208.67
  39   0.6016     80.340  0.6134    80.748  214.10
  40   0.5796     81.950  0.5867    82.136  219.70
  41   0.5570     81.960  0.5772    82.768  225.13
  42   0.4693     84.920  0.4535    85.472  230.55
  43   0.4624     85.110  0.3869    87.568  235.97
  44   0.4330     86.170  0.3455    88.782  241.40
  45   0.4411     85.980  0.3264    89.354  246.85
  46   0.4604     86.200  0.3194    89.566  252.34
  47   0.4289     86.390  0.3077    89.994  257.79
  48   0.4359     86.710  0.2952    90.532  263.25
  49   0.4372     86.460  0.2864    90.730  268.66
  50   0.4260     87.320  0.2769    91.022  274.07
  51   0.8367     84.480  0.2875    90.966  279.49
  52   0.5452     82.970  0.4473    86.610  284.91
  53   0.8496     75.510  0.8174    78.696  290.48
  54   0.6988     76.610  0.9142    73.754  295.89
  55   0.5113     83.030  0.5768    81.218  301.32
  56   0.4776     85.110  0.4537    85.162  306.77
  57   0.4384     85.690  0.3754    87.630  312.19
  58   0.4420     85.670  0.3304    88.972  317.63
  59   0.4372     86.570  0.3066    89.846  323.08
  60   0.4077     87.270  0.2803    90.756  328.55
  61   0.4053     87.300  0.2657    91.296  333.96
  62   0.4211     87.520  0.2462    91.978  339.42
  63   0.4083     87.720  0.2389    92.264  344.85
  64   0.4469     87.400  0.2374    92.234  350.27
  65   0.3957     88.310  0.2313    92.522  355.70
  66   0.4086     87.990  0.2158    92.828  361.13
  67   0.4055     88.530  0.2145    93.028  366.66
  68   0.3958     88.640  0.2014    93.426  372.09
  69   0.4182     87.780  0.2082    93.166  377.53
  70   0.4366     88.140  0.2127    93.238  382.97
  71   0.6218     84.900  0.2313    92.596  388.39
  72   0.4655     86.480  0.2752    91.006  393.84
  73   0.3924     88.630  0.2231    92.822  399.28
  74   0.4814     86.100  0.3415    90.342  404.88
  75   0.4289     87.740  0.2631    91.400  410.31
  76   0.4067     88.500  0.2100    93.090  415.74
  77   0.4312     88.360  0.1793    94.218  421.18
  78   0.4049     88.120  0.1677    94.478  426.61
  79   0.3983     89.150  0.1708    94.540  432.04
  80   0.3964     89.230  0.1522    95.098  437.48
  81   0.4056     89.120  0.1600    94.912  443.06
  82   0.4278     88.500  0.1541    94.956  448.49
  83   0.4152     88.930  0.1509    95.126  453.95
  84   0.4152     88.450  0.1561    94.974  459.38
  85   0.4218     89.440  0.1497    95.270  464.81
