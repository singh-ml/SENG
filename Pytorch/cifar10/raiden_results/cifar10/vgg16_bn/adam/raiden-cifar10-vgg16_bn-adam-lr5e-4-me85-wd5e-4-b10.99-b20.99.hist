Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2166     13.930  2.8748    11.902  7.12
   2   2.1376     16.170  2.2076    14.404  12.53
   3   2.1436     17.130  2.0976    16.894  17.95
   4   1.9359     20.730  1.9392    20.286  23.36
   5   1.7227     31.040  1.8437    25.356  28.90
   6   1.5527     39.130  1.6807    33.074  34.29
   7   1.3562     48.580  1.4885    43.234  39.69
   8   1.2788     54.310  1.3277    50.918  45.07
   9   1.2751     57.250  1.1979    56.500  50.50
  10   1.1264     61.580  1.0694    62.064  55.89
  11   0.9393     66.950  0.9637    66.122  61.28
  12   0.8920     69.270  0.8843    69.470  66.81
  13   0.7999     72.990  0.8236    72.082  72.22
  14   0.8185     72.540  0.7506    74.510  77.63
  15   0.7859     74.220  0.7201    75.970  83.03
  16   0.6524     78.320  0.6805    77.568  88.42
  17   0.6852     78.010  0.6352    79.314  93.82
  18   0.5892     80.330  0.5825    81.108  99.23
  19   0.5809     81.490  0.5585    81.958  104.65
  20   0.5901     80.720  0.5298    82.884  110.07
  21   0.5337     83.050  0.4913    84.152  115.48
  22   0.5381     82.370  0.4611    85.154  120.88
  23   0.4757     84.550  0.4405    86.010  126.29
  24   0.5562     82.310  0.4216    86.616  131.72
  25   0.4826     84.950  0.4096    86.908  137.25
  26   0.4540     85.660  0.3978    87.310  142.67
  27   0.4515     85.470  0.3691    88.140  148.08
  28   0.5386     84.310  0.3584    88.508  153.51
  29   0.4388     86.580  0.3433    89.066  158.90
  30   0.4549     86.790  0.3357    89.308  164.32
  31   0.4205     87.020  0.3129    89.954  169.71
  32   0.4322     86.300  0.3117    89.996  175.25
  33   0.4315     87.020  0.3069    90.204  180.64
  34   0.4276     87.280  0.2975    90.470  186.07
  35   0.3992     88.120  0.2813    91.026  191.50
  36   0.3963     88.070  0.2732    91.254  196.91
  37   0.4264     87.040  0.2634    91.602  202.33
  38   0.4393     87.410  0.2585    91.772  207.74
  39   0.4258     87.450  0.2452    92.142  213.32
  40   0.4282     87.720  0.2443    92.266  218.72
  41   0.3831     88.560  0.2359    92.390  224.14
  42   0.4074     88.070  0.2310    92.650  229.57
  43   0.3982     88.170  0.2229    92.980  234.96
  44   0.4115     88.610  0.2134    93.212  240.37
  45   0.3938     88.590  0.2254    92.848  245.81
  46   0.4096     88.370  0.2117    93.394  251.22
  47   0.4215     88.390  0.2064    93.536  256.75
  48   0.4202     88.090  0.1990    93.806  262.18
  49   0.4314     88.160  0.1924    93.884  267.60
  50   0.3954     89.160  0.1923    93.834  272.99
  51   0.4082     88.790  0.1860    94.176  278.39
  52   0.3886     89.280  0.1896    94.062  283.80
  53   0.3997     88.780  0.1772    94.520  289.23
  54   0.3809     89.430  0.1711    94.542  294.79
  55   0.4158     88.840  0.1681    94.670  300.18
  56   0.3952     89.300  0.1702    94.612  305.60
  57   0.4392     88.610  0.1684    94.856  311.02
  58   0.3865     89.380  0.1635    94.864  316.45
  59   0.4258     88.630  0.1627    94.932  321.86
  60   0.3732     89.470  0.1579    94.964  327.28
  61   0.4036     89.290  0.1628    94.990  332.77
  62   0.3880     89.130  0.1527    95.206  338.17
  63   0.3918     89.400  0.1570    94.994  343.59
  64   0.4155     88.910  0.1512    95.312  349.01
  65   0.3990     89.700  0.1506    95.448  354.43
  66   0.3884     89.830  0.1487    95.342  359.85
  67   0.3918     89.150  0.1491    95.344  365.38
  68   0.3737     89.730  0.1453    95.518  370.79
  69   0.4124     89.040  0.1440    95.572  376.20
  70   0.4087     89.470  0.1440    95.570  381.60
  71   0.4268     89.000  0.1353    95.846  387.04
  72   0.4767     88.480  0.1415    95.590  392.43
  73   0.4227     88.690  0.1359    95.728  397.86
  74   0.4134     89.130  0.1402    95.812  403.41
  75   0.4212     89.360  0.1326    95.786  408.82
  76   0.4112     89.520  0.1321    95.830  414.25
  77   0.4157     89.700  0.1303    95.938  419.66
  78   0.4254     89.310  0.1282    96.044  425.06
  79   0.4205     88.950  0.1298    95.988  430.46
  80   0.4321     88.850  0.1259    95.992  435.87
  81   0.4030     89.740  0.1272    96.100  441.42
  82   0.4110     89.990  0.1221    96.208  446.84
  83   0.4228     88.850  0.1253    96.000  452.27
  84   0.3965     89.420  0.1253    96.256  457.71
  85   0.4081     89.650  0.1212    96.192  463.14
