Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3917     48.140  1.8974    30.626  7.07
   2   1.1779     56.910  1.3366    51.396  12.45
   3   1.0177     64.130  1.1443    59.526  17.85
   4   0.9778     65.560  1.0170    64.522  23.23
   5   0.8373     70.750  0.9180    68.274  28.79
   6   0.8591     70.290  0.8329    71.418  34.18
   7   0.8397     70.790  0.7609    74.272  39.57
   8   0.7353     74.890  0.7054    75.978  44.97
   9   0.6589     77.670  0.6590    77.608  50.39
  10   0.7523     74.280  0.6136    79.244  55.80
  11   0.6114     78.880  0.5888    80.142  61.34
  12   0.6255     78.760  0.5575    81.362  66.72
  13   0.5860     79.960  0.5306    82.238  72.11
  14   0.5984     80.030  0.4978    83.196  77.50
  15   0.5708     80.980  0.4773    83.884  82.88
  16   0.5675     81.570  0.4539    84.758  88.27
  17   0.5476     81.830  0.4358    85.358  93.69
  18   0.5576     81.470  0.4171    85.914  99.23
  19   0.6041     80.680  0.3965    86.848  104.64
  20   0.5796     81.390  0.3835    87.046  110.01
  21   0.5972     80.290  0.3663    87.554  115.41
  22   0.5358     82.730  0.3471    88.170  120.80
  23   0.5101     83.650  0.3484    88.308  126.16
  24   0.5605     82.700  0.3219    89.064  131.56
  25   0.5632     82.800  0.3083    89.662  137.08
  26   0.5259     83.740  0.3099    89.554  142.48
  27   0.5183     84.280  0.2907    90.250  147.86
  28   0.5566     82.840  0.2812    90.354  153.23
  29   0.5084     84.240  0.2760    90.828  158.64
  30   0.5146     84.820  0.2616    91.156  164.04
  31   0.5243     84.510  0.2563    91.462  169.42
  32   0.5106     84.440  0.2460    91.708  174.80
  33   0.5876     83.860  0.2412    91.918  180.35
  34   0.5679     83.540  0.2263    92.398  185.76
  35   0.5366     84.660  0.2261    92.538  191.19
  36   0.5846     83.480  0.2154    92.778  196.61
  37   0.5894     83.840  0.2108    92.898  202.03
  38   0.5372     85.370  0.2005    93.230  207.44
  39   0.5925     84.440  0.1937    93.530  212.97
  40   0.5913     82.880  0.1964    93.550  218.35
  41   0.5249     85.730  0.1831    93.770  223.77
  42   0.5614     84.440  0.1795    93.920  229.19
  43   0.5644     85.430  0.1688    94.330  234.59
  44   0.5093     85.640  0.1760    94.090  239.98
  45   0.5327     85.260  0.1625    94.646  245.39
  46   0.6390     83.090  0.1642    94.360  250.80
  47   0.5522     85.250  0.1603    94.666  256.33
  48   0.4917     86.310  0.1536    94.886  261.71
  49   0.5712     85.050  0.1527    94.960  267.11
  50   0.5189     86.310  0.1479    95.116  272.51
  51   0.5369     85.500  0.1479    95.016  277.91
  52   0.5682     85.400  0.1437    95.272  283.29
  53   0.5128     86.380  0.1410    95.252  288.69
  54   0.4873     87.180  0.1362    95.458  294.10
  55   0.5748     85.330  0.1250    95.740  299.65
  56   0.4969     86.850  0.1257    95.778  305.06
  57   0.5184     86.810  0.1215    95.936  310.43
  58   0.5656     85.970  0.1251    95.966  315.84
  59   0.6193     85.000  0.1219    95.852  321.23
  60   0.5470     86.710  0.1191    96.042  326.64
  61   0.5722     86.290  0.1158    96.120  332.20
  62   0.5598     85.610  0.1137    96.250  337.59
  63   0.5079     87.010  0.1136    96.230  343.00
  64   0.6326     85.010  0.1084    96.340  348.39
  65   0.5547     86.330  0.1080    96.372  353.82
  66   0.5849     86.480  0.1063    96.504  359.22
  67   0.6095     85.460  0.1030    96.622  364.62
  68   0.5649     87.120  0.1059    96.418  370.03
  69   0.5571     86.070  0.1031    96.566  375.55
  70   0.5960     86.150  0.0986    96.744  380.96
  71   0.5711     86.380  0.0968    96.736  386.36
  72   0.5685     86.470  0.0936    96.842  391.78
  73   0.5863     86.060  0.0938    96.860  397.20
  74   0.5681     86.320  0.0969    96.730  402.60
  75   0.5113     87.490  0.0944    96.812  408.10
  76   0.5840     86.650  0.0896    96.986  413.48
  77   0.5339     87.430  0.0903    96.936  418.86
  78   0.5788     86.640  0.0873    97.038  424.25
  79   0.5954     85.880  0.0894    96.988  429.66
  80   0.5570     86.480  0.0871    97.028  435.06
  81   0.5867     86.160  0.0859    97.028  440.45
  82   0.5374     87.340  0.0862    97.112  445.83
  83   0.7202     84.360  0.0822    97.196  451.37
  84   0.5883     86.780  0.0820    97.196  456.77
  85   0.5556     86.870  0.0809    97.294  462.16
