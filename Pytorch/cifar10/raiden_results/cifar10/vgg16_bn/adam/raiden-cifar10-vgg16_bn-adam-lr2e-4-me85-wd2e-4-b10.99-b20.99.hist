Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2492     12.420  2.5324    11.858  7.09
   2   2.1091     17.420  2.2013    14.772  12.51
   3   1.9010     23.700  2.0422    19.292  17.93
   4   1.7665     29.470  1.8654    25.040  23.35
   5   1.6027     37.100  1.7313    30.634  28.89
   6   1.4686     42.820  1.5555    39.208  34.31
   7   1.3047     50.930  1.4228    45.798  39.75
   8   1.2485     53.890  1.3013    51.690  45.19
   9   1.1700     57.940  1.1909    56.428  50.62
  10   1.0598     63.590  1.0925    61.104  56.03
  11   0.9374     66.880  1.0128    64.472  61.46
  12   0.8651     69.860  0.9041    68.376  66.98
  13   0.8401     71.570  0.8452    70.754  72.42
  14   0.7686     73.570  0.7781    73.120  77.84
  15   0.7305     75.240  0.7181    75.412  83.27
  16   0.7202     75.860  0.6992    76.194  88.67
  17   0.6651     77.380  0.6505    77.954  94.10
  18   0.6492     78.340  0.6108    79.362  99.51
  19   0.6396     78.860  0.5857    80.450  104.94
  20   0.5808     81.010  0.5525    81.644  110.48
  21   0.6092     80.010  0.5265    82.366  115.93
  22   0.5616     81.570  0.5084    82.862  121.39
  23   0.5633     81.430  0.4753    84.362  126.81
  24   0.5840     81.170  0.4662    84.502  132.23
  25   0.5583     82.930  0.4517    84.704  137.67
  26   0.5141     83.700  0.4444    85.396  143.12
  27   0.5181     83.710  0.4136    86.212  148.67
  28   0.5596     82.630  0.3908    87.056  154.12
  29   0.5138     84.130  0.3820    87.550  159.53
  30   0.5364     84.000  0.3692    88.024  164.95
  31   0.5054     84.200  0.3548    88.520  170.38
  32   0.4860     84.470  0.3528    88.676  175.83
  33   0.4874     85.320  0.3338    89.052  181.23
  34   0.4555     85.740  0.3190    89.548  186.79
  35   0.4905     85.710  0.2992    90.180  192.22
  36   0.4799     85.310  0.2922    90.540  197.67
  37   0.4843     84.980  0.2887    90.446  203.08
  38   0.4509     86.380  0.2768    90.852  208.53
  39   0.4462     86.010  0.2681    91.032  213.94
  40   0.5158     85.840  0.2551    91.578  219.36
  41   0.4798     86.600  0.2468    91.884  224.76
  42   0.4548     86.520  0.2453    91.978  230.31
  43   0.4631     86.510  0.2472    91.978  235.73
  44   0.4584     87.150  0.2200    92.768  241.17
  45   0.4807     86.310  0.2101    93.030  246.62
  46   0.4709     86.610  0.2148    93.010  252.09
  47   0.4477     86.970  0.2014    93.350  257.51
  48   0.4637     87.000  0.1917    93.840  263.07
  49   0.4551     86.660  0.1925    93.702  268.48
  50   0.4489     87.800  0.1900    93.808  273.90
  51   0.4448     87.120  0.1841    94.050  279.34
  52   0.4614     86.890  0.1786    94.192  284.79
  53   0.4866     87.250  0.1736    94.286  290.22
  54   0.4526     87.540  0.1661    94.504  295.66
  55   0.4636     87.700  0.1609    94.680  301.20
  56   0.4465     88.300  0.1459    95.246  306.61
  57   0.4381     87.850  0.1493    95.112  312.03
  58   0.4648     88.390  0.1386    95.376  317.46
  59   0.4530     88.200  0.1428    95.436  322.90
  60   0.4563     87.900  0.1354    95.602  328.32
  61   0.4723     87.360  0.1327    95.608  333.79
  62   0.4850     87.950  0.1359    95.596  339.34
  63   0.4788     87.570  0.1246    95.922  344.76
  64   0.4446     88.310  0.1275    95.840  350.19
  65   0.4399     88.580  0.1202    96.042  355.64
  66   0.5241     87.530  0.1274    95.830  361.04
  67   0.4429     88.550  0.1192    96.036  366.45
  68   0.4451     88.190  0.1141    96.366  371.87
  69   0.4692     88.300  0.1078    96.484  377.44
  70   0.4980     88.070  0.1059    96.596  382.88
  71   0.5336     87.360  0.1099    96.450  388.31
  72   0.4837     87.960  0.1039    96.560  393.76
  73   0.5117     88.220  0.1011    96.722  399.18
  74   0.4862     88.540  0.1091    96.568  404.63
  75   0.4766     88.280  0.1020    96.700  410.07
  76   0.4651     88.850  0.0943    97.036  415.63
  77   0.4829     88.130  0.0977    96.844  421.04
  78   0.4469     89.000  0.0875    97.152  426.48
  79   0.4887     88.290  0.0930    96.954  431.89
  80   0.4688     88.670  0.0948    96.998  437.30
  81   0.4604     88.300  0.0920    97.042  442.71
  82   0.5304     88.080  0.0884    97.098  448.17
  83   0.4772     88.440  0.0881    97.210  453.59
  84   0.4938     89.060  0.0845    97.274  459.03
  85   0.4556     88.680  0.0836    97.274  464.49
