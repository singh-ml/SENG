Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2818     12.440  3.5891    11.350  8.29
   2   2.2917     11.860  2.3236    11.708  13.76
   3   2.2533     14.050  2.3393    11.574  19.28
   4   2.1909     14.690  2.2453    13.684  24.69
   5   1.9441     20.540  2.0926    17.308  30.10
   6   1.8610     21.820  1.9315    20.320  35.49
   7   1.7246     29.640  1.8471    24.926  40.92
   8   1.6065     34.590  1.6957    31.464  46.32
   9   1.4843     40.450  1.5795    36.710  51.74
  10   1.4341     43.940  1.4729    42.052  57.28
  11   1.3254     49.640  1.3637    47.938  62.69
  12   1.2212     57.270  1.2063    56.174  68.10
  13   1.0297     64.160  1.0956    61.398  73.52
  14   1.0863     62.530  1.0712    63.484  78.93
  15   1.0024     65.430  1.0371    64.778  84.35
  16   1.0313     64.810  1.0020    65.798  89.79
  17   1.3215     67.160  1.0021    66.002  95.22
  18   1.4141     61.530  1.2119    63.116  100.65
  19   1.0412     64.020  1.1750    62.120  106.06
  20   0.9545     66.610  1.0930    65.714  111.46
  21   0.8031     72.860  0.8779    69.540  116.88
  22   0.8371     71.560  0.8679    72.060  122.30
  23   0.8758     70.980  0.8403    72.012  127.82
  24   1.4616     68.790  0.9323    68.972  133.26
  25   1.2069     64.360  1.1141    66.616  138.66
  26   1.0609     63.640  1.2173    63.316  144.06
  27   0.9991     66.060  1.0787    64.570  149.49
  28   1.4748     69.090  1.1457    67.438  154.90
  29   1.3102     64.100  1.2225    65.282  160.35
  30   0.8485     70.620  0.9696    67.692  165.90
  31   0.7381     75.350  0.7914    73.334  171.30
  32   0.7182     76.140  0.7104    76.068  176.72
  33   0.6608     78.370  0.6561    78.070  182.12
  34   0.6633     79.340  0.6233    79.258  187.52
  35   0.6202     79.580  0.7024    80.150  192.92
  36   0.7594     76.190  0.6969    78.284  198.32
  37   0.6712     77.760  0.7205    77.448  203.89
  38   0.7844     73.690  0.7180    77.440  209.30
  39   0.7307     76.630  0.7772    75.462  214.72
  40   0.6097     79.690  0.6474    78.690  220.12
  41   0.5779     80.820  0.5713    80.990  225.55
  42   0.5284     82.750  0.5200    82.834  230.97
  43   0.5020     83.550  0.4840    83.914  236.38
  44   0.5049     83.520  0.4567    84.848  241.92
  45   0.4950     84.180  0.4195    86.178  247.32
  46   0.5822     83.560  0.4329    86.084  252.75
  47   0.5321     83.520  0.4428    85.750  258.18
  48   0.4565     85.500  0.3913    87.202  263.60
  49   0.4722     85.170  0.3735    87.906  269.03
  50   0.4693     85.560  0.3513    88.518  274.58
  51   0.4288     86.300  0.3448    88.854  279.97
  52   0.4330     86.520  0.3250    89.616  285.38
  53   0.4447     86.220  0.3070    90.020  290.80
  54   0.4293     86.790  0.2939    90.554  296.23
  55   0.4507     86.130  0.2857    90.812  301.63
  56   0.4255     86.970  0.2770    90.946  307.07
  57   0.4187     86.980  0.2630    91.590  312.64
  58   0.4064     87.900  0.2590    91.770  318.06
  59   0.3915     88.150  0.2506    91.922  323.49
  60   0.4295     87.540  0.2376    92.508  328.90
  61   0.3948     87.830  0.2326    92.638  334.33
  62   0.4100     87.400  0.2241    92.880  339.75
  63   0.4140     87.930  0.2096    93.220  345.15
  64   0.4311     87.630  0.2214    93.034  350.70
  65   0.3871     88.150  0.2137    93.252  356.09
  66   0.4082     88.020  0.2000    93.752  361.50
  67   0.3941     88.330  0.1948    93.726  366.93
  68   0.4099     88.110  0.1876    93.918  372.35
  69   0.4184     88.250  0.1799    94.202  377.77
  70   0.3908     88.510  0.1779    94.290  383.18
  71   0.4370     87.940  0.1729    94.430  388.73
  72   0.4164     88.320  0.1767    94.472  394.16
  73   0.4139     88.520  0.1676    94.680  399.60
  74   0.4323     88.070  0.1660    94.694  405.01
  75   0.4319     88.560  0.1891    94.276  410.42
  76   0.4208     88.120  0.2220    93.220  415.84
  77   0.4093     88.810  0.1886    93.942  421.27
  78   0.4044     89.310  0.1568    94.994  426.83
  79   0.4432     88.540  0.1561    95.114  432.26
  80   0.4250     88.870  0.1530    94.998  437.66
  81   0.4383     88.620  0.1533    95.332  443.06
  82   0.4188     89.220  0.1521    95.156  448.47
  83   0.4010     88.730  0.1436    95.472  453.87
  84   0.4420     88.570  0.1441    95.394  459.28
  85   0.4444     88.850  0.1356    95.806  464.80
  86   0.4257     88.760  0.1382    95.518  470.21
  87   0.4113     89.380  0.1330    95.884  475.64
  88   0.4161     88.850  0.1271    95.950  481.03
  89   0.4491     88.400  0.1302    95.852  486.46
  90   0.4164     89.150  0.1272    95.994  491.87
