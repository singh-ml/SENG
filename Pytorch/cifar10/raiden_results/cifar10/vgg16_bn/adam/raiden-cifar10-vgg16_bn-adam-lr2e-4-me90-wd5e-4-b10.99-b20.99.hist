Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.1381     17.390  2.4531    13.906  7.08
   2   1.9334     22.420  2.0897    18.132  12.54
   3   1.7976     25.530  1.8854    23.886  18.02
   4   1.7513     30.890  1.7543    30.030  23.47
   5   1.4098     46.450  1.5610    39.738  28.94
   6   1.3134     49.780  1.3965    47.188  34.44
   7   1.1381     57.780  1.2466    53.868  40.01
   8   1.0552     61.900  1.1278    59.128  45.47
   9   0.9301     67.070  1.0124    64.002  50.92
  10   0.8495     70.330  0.9267    67.536  56.39
  11   0.8390     71.770  0.8527    70.108  61.85
  12   0.7482     74.390  0.7794    73.058  67.34
  13   0.8177     73.010  0.7323    74.896  72.93
  14   0.6817     77.130  0.6821    76.852  78.41
  15   0.6681     78.300  0.6347    78.676  83.85
  16   0.6496     79.210  0.6032    80.034  89.32
  17   0.6059     80.330  0.5605    81.384  94.78
  18   0.5773     80.870  0.5284    82.404  100.25
  19   0.5784     81.310  0.5066    83.338  105.71
  20   0.5410     82.530  0.4907    83.622  111.18
  21   0.5431     82.860  0.4605    84.768  116.77
  22   0.5333     83.480  0.4346    85.698  122.25
  23   0.5427     83.370  0.4177    86.422  127.73
  24   0.5373     82.590  0.4023    86.738  133.19
  25   0.5414     83.180  0.3911    87.222  138.65
  26   0.5473     83.510  0.3828    87.580  144.13
  27   0.4560     86.420  0.3600    88.182  149.63
  28   0.4836     85.200  0.3368    88.968  155.22
  29   0.4663     85.210  0.3278    89.424  160.72
  30   0.4777     85.650  0.3180    89.672  166.20
  31   0.4971     85.100  0.3145    89.764  171.65
  32   0.4829     85.840  0.3046    90.172  177.13
  33   0.4794     85.890  0.2876    90.710  182.59
  34   0.4741     86.250  0.2710    91.044  188.07
  35   0.4349     86.680  0.2638    91.344  193.66
  36   0.4560     86.910  0.2569    91.610  199.12
  37   0.4579     85.860  0.2491    91.988  204.61
  38   0.4688     86.380  0.2429    92.282  210.07
  39   0.4629     86.640  0.2336    92.316  215.54
  40   0.4549     87.130  0.2239    92.658  221.02
  41   0.4475     87.540  0.2068    93.326  226.61
  42   0.4204     87.850  0.2040    93.152  232.08
  43   0.4438     87.580  0.2110    93.198  237.54
  44   0.4141     88.100  0.1999    93.628  243.00
  45   0.4280     87.530  0.1918    93.792  248.49
  46   0.4254     87.780  0.1855    94.108  253.98
  47   0.4313     87.740  0.1797    94.154  259.47
  48   0.4465     87.950  0.1789    94.102  265.09
  49   0.4237     88.150  0.1796    94.184  270.54
  50   0.4420     87.970  0.1660    94.582  276.02
  51   0.4603     87.850  0.1660    94.704  281.49
  52   0.4346     88.510  0.1576    94.918  286.98
  53   0.4371     87.820  0.1573    94.868  292.44
  54   0.4134     88.200  0.1504    94.992  297.92
  55   0.4267     88.330  0.1527    95.046  303.54
  56   0.4566     87.940  0.1451    95.344  309.00
  57   0.4656     88.540  0.1399    95.456  314.47
  58   0.4413     87.990  0.1421    95.546  319.94
  59   0.4328     88.780  0.1319    95.842  325.42
  60   0.4439     88.170  0.1329    95.662  330.89
  61   0.4213     88.780  0.1231    95.920  336.36
  62   0.4739     88.480  0.1219    96.000  341.97
  63   0.4446     88.070  0.1260    95.938  347.45
  64   0.4395     88.560  0.1220    96.120  352.93
  65   0.4626     88.540  0.1146    96.280  358.41
  66   0.3872     89.160  0.1204    96.208  363.88
  67   0.4208     88.710  0.1164    96.228  369.36
  68   0.4490     88.770  0.1091    96.478  374.87
  69   0.4664     88.800  0.1089    96.458  380.45
  70   0.4612     87.860  0.1174    96.250  385.94
  71   0.4370     89.370  0.1116    96.386  391.42
  72   0.4820     88.720  0.1053    96.668  396.89
  73   0.4408     88.900  0.1090    96.548  402.34
  74   0.4324     89.250  0.1023    96.796  407.81
  75   0.4461     89.100  0.1035    96.758  413.43
  76   0.4754     88.710  0.0999    96.862  418.91
  77   0.4374     88.780  0.1011    96.824  424.39
  78   0.4519     88.880  0.0979    96.958  429.87
  79   0.4381     89.370  0.0943    97.010  435.33
  80   0.4493     89.000  0.0882    97.178  440.82
  81   0.4453     89.530  0.0902    97.088  446.29
  82   0.4386     89.410  0.0929    97.016  451.89
  83   0.4319     89.230  0.0879    97.170  457.39
  84   0.4817     88.690  0.0889    97.110  462.87
  85   0.4793     89.100  0.0885    97.170  468.35
  86   0.4213     89.310  0.0844    97.318  473.84
  87   0.4586     89.230  0.0889    97.266  479.31
  88   0.4163     89.730  0.0828    97.380  484.78
  89   0.4435     89.420  0.0845    97.356  490.36
  90   0.4818     88.980  0.0844    97.290  495.85
