Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2835     11.800  2.9237    11.086  7.12
   2   2.2701     11.690  2.2823    11.508  12.53
   3   2.2641     12.310  2.2673    12.060  17.92
   4   1.8918     21.200  2.0580    17.870  23.35
   5   1.8538     24.600  1.8813    22.434  28.75
   6   1.7215     30.910  1.7721    27.524  34.20
   7   1.5599     37.980  1.6562    34.044  39.64
   8   1.5515     41.590  1.5349    40.278  45.06
   9   1.3597     47.920  1.4350    44.882  50.49
  10   1.2521     53.550  1.3180    49.974  55.89
  11   1.1266     59.920  1.1943    56.816  61.31
  12   1.0548     64.240  1.0849    61.512  66.73
  13   0.9969     65.530  0.9764    65.936  72.25
  14   0.9053     68.960  0.9085    68.964  77.70
  15   0.8963     70.930  0.8668    70.768  83.10
  16   0.8342     72.620  0.8106    72.490  88.54
  17   0.9861     70.850  0.7949    73.330  93.95
  18   0.9251     70.350  0.9667    69.908  99.37
  19   0.7338     75.710  0.8109    72.472  104.78
  20   0.6766     77.090  0.7100    76.078  110.33
  21   0.6846     77.090  0.6892    77.700  115.74
  22   0.7098     76.640  0.7172    77.010  121.17
  23   0.8916     74.380  0.8128    75.886  126.60
  24   0.7104     76.380  0.7379    76.008  132.01
  25   0.9834     74.930  0.7288    77.196  137.43
  26   2.0193     70.590  0.7972    74.644  142.83
  27   0.8036     74.640  0.8482    74.676  148.33
  28   0.7698     74.000  0.8174    75.240  153.75
  29   0.7423     75.390  0.7623    75.428  159.14
  30   0.6378     78.830  0.6997    77.768  164.54
  31   0.6141     79.610  0.6532    78.716  169.96
  32   0.6062     79.730  0.5963    80.714  175.40
  33   0.5777     81.010  0.5773    81.126  180.83
  34   0.5999     81.110  0.5541    81.892  186.26
  35   0.5409     81.600  0.5311    82.690  191.77
  36   0.5095     82.580  0.4896    83.798  197.18
  37   0.6156     81.370  0.5398    83.120  202.62
  38   0.5169     83.060  0.4968    83.710  208.03
  39   0.4996     83.950  0.4415    85.562  213.46
  40   0.5503     82.190  0.4188    86.408  218.89
  41   0.6290     84.730  0.4071    86.626  224.41
  42   0.5646     83.620  0.4284    86.764  229.83
  43   0.5402     83.200  0.4472    86.012  235.25
  44   0.4732     85.130  0.4176    86.530  240.70
  45   0.4703     85.220  0.3683    88.074  246.11
  46   0.4308     86.450  0.3412    88.896  251.51
  47   0.4634     85.960  0.3204    89.394  256.91
  48   0.4388     86.080  0.3068    89.952  262.45
  49   0.4871     86.030  0.3054    90.152  267.84
  50   0.5454     85.360  0.3194    90.062  273.26
  51   0.4301     86.790  0.3116    89.912  278.66
  52   0.4560     86.490  0.2819    90.858  284.08
  53   0.4208     87.200  0.2689    91.176  289.50
  54   0.4416     86.930  0.2608    91.488  294.89
  55   0.4268     87.720  0.2594    91.736  300.43
  56   0.4034     87.790  0.2450    92.072  305.83
  57   0.4345     87.120  0.2280    92.534  311.26
  58   0.4544     87.480  0.2219    92.812  316.68
  59   0.3925     88.470  0.2068    93.402  322.09
  60   0.4011     88.040  0.2056    93.336  327.50
  61   0.4185     88.190  0.1993    93.668  332.89
  62   0.4627     87.050  0.1932    93.694  338.42
  63   0.4561     87.450  0.1924    93.810  343.82
  64   0.3968     88.150  0.1839    94.020  349.23
  65   0.3888     88.240  0.1795    94.142  354.62
  66   0.3922     89.020  0.1719    94.346  360.02
  67   0.3994     88.570  0.1681    94.566  365.43
  68   0.4265     88.170  0.1613    94.850  370.86
  69   0.4486     88.420  0.1722    94.542  376.38
  70   0.4225     88.250  0.1669    94.688  381.78
  71   0.3972     89.040  0.1551    95.008  387.18
  72   0.4039     89.440  0.1505    95.132  392.61
  73   0.4123     88.880  0.1421    95.528  398.00
  74   0.4106     88.980  0.1355    95.536  403.40
  75   0.4342     89.040  0.1314    95.694  408.83
  76   0.4154     89.300  0.1349    95.666  414.25
  77   0.4567     88.530  0.1323    95.650  419.67
  78   0.4246     89.240  0.1296    95.948  425.07
  79   0.4074     89.460  0.1231    96.072  430.48
  80   0.4497     88.470  0.1155    96.236  435.89
  81   0.4218     89.560  0.1215    96.164  441.32
  82   0.4194     89.440  0.1187    96.202  446.87
  83   0.3980     89.670  0.1096    96.454  452.26
  84   0.4104     89.110  0.1114    96.484  457.66
  85   0.4395     89.080  0.1092    96.570  463.08
