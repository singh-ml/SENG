Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.3056     10.000  123.5302    10.408  7.11
   2   2.3036     10.000  3.9323     9.952  12.53
   3   2.3025     10.070  2.6475     9.870  18.00
   4   4.0640     10.000  2.6983     9.940  23.42
   5   2.3030     10.000  2.6370    10.100  28.86
   6   2.3442     11.040  2.6168    10.292  34.30
   7   2.3536     10.290  2.7531    10.618  39.72
   8   6.1317     10.900  3.7811    10.382  45.14
   9   2.6200     10.800  3.6978    10.304  50.56
  10   2.4185     10.500  2.7511    10.368  56.14
  11   5.2960     12.670  2.2861    12.856  61.59
  12   15.5302     14.190  1.9750    20.634  67.01
  13   1.7330     28.630  1.7967    28.090  72.46
  14   1.8175     38.490  1.6277    36.280  77.91
  15   1.4410     48.130  1.4770    43.762  83.36
  16   1.2198     55.920  1.3115    51.874  88.80
  17   1.1079     61.830  1.1562    59.164  94.38
  18   1.1309     63.460  1.0380    64.282  99.81
  19   0.9549     68.240  0.9762    66.838  105.25
  20   1.0256     66.460  0.9255    69.288  110.70
  21   1.0129     68.160  0.8897    71.196  116.14
  22   0.8808     72.070  0.8548    72.730  121.58
  23   0.9105     71.220  0.8320    73.454  127.02
  24   0.8243     73.350  0.8119    74.662  132.54
  25   0.8313     73.030  0.8030    74.736  137.98
  26   0.8310     73.730  0.7921    75.176  143.38
  27   1.9866     52.150  0.7704    75.866  148.82
  28   0.8596     72.710  0.7685    75.914  154.26
  29   0.8877     72.540  0.7417    76.884  159.70
  30   0.7331     76.120  0.7256    77.272  165.13
  31   0.7735     75.460  0.7130    77.480  170.68
  32   0.6912     78.120  0.6947    78.066  176.10
  33   0.7767     75.950  0.6927    78.002  181.53
  34   0.8265     75.370  0.6956    78.110  186.98
  35   0.6975     77.800  0.6912    78.110  192.40
  36   0.8687     74.310  0.6972    77.866  197.85
  37   0.7467     76.660  0.6907    78.418  203.29
  38   0.7008     77.910  0.6790    78.498  208.85
  39   0.7274     77.990  0.6745    78.790  214.28
  40   0.6825     78.490  0.6734    78.714  219.72
  41   0.8944     73.940  0.6626    79.090  225.17
  42   0.6646     79.000  0.6651    79.150  230.58
  43   0.7513     77.290  0.6581    79.530  236.03
  44   0.8397     75.820  0.6593    79.588  241.60
  45   1.9945     49.530  0.6605    79.318  247.04
  46   0.6765     78.950  0.6601    79.308  252.50
  47   0.8696     75.390  0.6703    79.012  257.94
  48   0.7409     78.260  0.6635    79.312  263.37
  49   0.6221     80.510  0.6465    79.788  268.83
  50   0.9821     72.330  0.6603    79.204  274.26
  51   0.6836     79.440  0.6464    79.692  279.81
  52   0.7221     77.450  0.6381    80.062  285.23
  53   0.7441     77.040  0.6607    79.626  290.65
  54   0.6766     79.090  0.6504    79.816  296.09
  55   1.0418     69.950  0.6503    79.638  301.52
  56   0.7179     78.180  0.6404    79.830  306.94
  57   0.8523     74.180  0.6315    79.976  312.36
  58   0.6456     79.740  0.6370    80.228  317.90
  59   0.7396     76.730  0.6393    79.892  323.33
  60   0.6655     79.460  0.6361    80.284  328.74
  61   0.6497     79.270  0.6341    80.458  334.16
  62   0.6868     78.760  0.6290    80.328  339.58
  63   0.6362     80.150  0.6335    80.350  345.02
  64   0.6602     79.050  0.6232    80.516  350.44
  65   0.6263     79.980  0.6282    80.462  355.85
  66   0.6593     79.540  0.6259    80.492  361.28
  67   0.6352     80.520  0.6171    80.688  366.69
  68   0.6869     79.450  0.6176    80.654  372.12
  69   0.6287     80.650  0.6081    81.064  377.56
  70   0.5916     81.070  0.6184    80.494  382.98
  71   0.7074     78.850  0.6146    80.806  388.40
  72   0.6682     79.210  0.6180    80.812  393.82
  73   0.6763     79.660  0.6167    80.688  399.38
  74   0.6340     80.040  0.6153    80.796  404.83
  75   0.6533     79.960  0.6192    80.820  410.24
  76   0.6419     80.240  0.6108    81.062  415.67
  77   0.6875     79.220  0.5968    81.404  421.09
  78   0.6281     80.480  0.6058    81.130  426.52
  79   1.0058     72.720  0.6050    81.290  432.05
  80   0.6926     79.080  0.6005    81.332  437.46
  81   0.7070     77.570  0.5998    81.224  442.89
  82   0.6444     80.270  0.6143    81.030  448.34
  83   0.6793     78.790  0.6114    80.868  453.79
  84   0.6910     79.310  0.6105    81.066  459.23
  85   0.6338     80.420  0.5984    81.330  464.67
  86   0.6421     79.560  0.6140    80.980  470.24
  87   0.6523     79.750  0.6039    81.092  475.66
  88   0.6336     80.180  0.6096    81.088  481.09
  89   0.6851     79.120  0.5977    81.368  486.51
  90   0.6793     79.020  0.5955    81.582  491.93
