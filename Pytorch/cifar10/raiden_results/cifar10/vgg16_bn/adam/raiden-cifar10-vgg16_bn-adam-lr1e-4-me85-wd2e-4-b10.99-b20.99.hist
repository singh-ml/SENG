Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.9283     21.140  2.2910    14.916  7.04
   2   1.7726     27.190  1.8942    22.446  12.58
   3   1.5816     36.640  1.7152    30.772  18.01
   4   1.4471     44.860  1.5437    39.316  23.45
   5   1.3010     51.570  1.4084    46.480  28.90
   6   1.1803     56.080  1.2790    52.442  34.32
   7   1.0526     61.520  1.1403    58.160  39.70
   8   0.9642     66.130  1.0257    62.720  45.13
   9   0.9130     68.550  0.9386    66.624  50.68
  10   0.8408     70.940  0.8638    69.652  56.08
  11   0.7788     73.010  0.7861    72.666  61.47
  12   0.7740     73.550  0.7326    74.836  66.87
  13   0.6820     76.790  0.6758    76.756  72.29
  14   0.6819     76.790  0.6363    78.220  77.69
  15   0.6272     78.780  0.5942    79.838  83.11
  16   0.6937     77.140  0.5756    80.496  88.67
  17   0.6232     79.350  0.5545    80.992  94.10
  18   0.5985     80.500  0.5260    82.364  99.54
  19   0.5961     80.040  0.4932    83.434  104.96
  20   0.5769     80.930  0.4771    83.866  110.37
  21   0.5790     81.120  0.4538    84.744  115.80
  22   0.5739     81.680  0.4382    85.240  121.22
  23   0.5634     81.940  0.4173    85.894  126.77
  24   0.5541     82.220  0.4073    86.180  132.17
  25   0.5205     83.190  0.3855    86.884  137.61
  26   0.5965     82.110  0.3720    87.398  143.05
  27   0.5452     83.080  0.3520    88.098  148.47
  28   0.5700     83.410  0.3444    88.356  153.90
  29   0.5278     84.080  0.3288    88.936  159.30
  30   0.5774     83.560  0.3199    89.090  164.85
  31   0.5245     83.870  0.3012    89.910  170.27
  32   0.5195     84.020  0.2895    90.090  175.69
  33   0.5241     84.260  0.2842    90.564  181.12
  34   0.5178     84.370  0.2705    90.964  186.54
  35   0.5206     84.710  0.2594    91.192  191.94
  36   0.5345     83.720  0.2577    91.234  197.34
  37   0.5093     84.830  0.2501    91.446  202.93
  38   0.5178     85.070  0.2307    92.220  208.36
  39   0.5237     84.020  0.2302    92.136  213.79
  40   0.5409     84.630  0.2234    92.454  219.20
  41   0.5166     85.070  0.2114    92.768  224.62
  42   0.5312     84.910  0.2023    93.212  230.07
  43   0.5194     84.620  0.1935    93.500  235.62
  44   0.5154     84.860  0.1950    93.560  241.06
  45   0.5463     85.060  0.1873    93.598  246.48
  46   0.5615     85.190  0.1893    93.726  251.88
  47   0.5281     85.270  0.1853    93.846  257.32
  48   0.5079     85.650  0.1716    94.274  262.73
  49   0.5074     86.040  0.1630    94.462  268.15
  50   0.5349     85.220  0.1566    94.778  273.71
  51   0.5148     85.890  0.1548    94.752  279.15
  52   0.5599     85.550  0.1479    94.996  284.58
  53   0.5147     86.160  0.1375    95.332  289.99
  54   0.5390     86.010  0.1379    95.326  295.38
  55   0.5228     86.110  0.1453    95.136  300.83
  56   0.5648     85.560  0.1390    95.386  306.24
  57   0.5136     86.710  0.1304    95.620  311.78
  58   0.5320     86.180  0.1387    95.388  317.19
  59   0.5175     86.060  0.1249    95.744  322.58
  60   0.5530     86.020  0.1158    96.064  328.00
  61   0.5598     85.830  0.1100    96.224  333.44
  62   0.5442     86.350  0.1160    96.120  338.85
  63   0.5569     86.000  0.1075    96.346  344.29
  64   0.5182     86.070  0.1083    96.264  349.81
  65   0.5247     86.640  0.1074    96.412  355.23
  66   0.5353     86.430  0.1009    96.614  360.65
  67   0.5253     86.510  0.0998    96.624  366.08
  68   0.6085     86.220  0.0998    96.594  371.49
  69   0.5214     86.520  0.0965    96.718  376.94
  70   0.5341     86.060  0.0974    96.688  382.33
  71   0.5786     86.120  0.0923    96.830  387.89
  72   0.5552     86.720  0.0875    96.938  393.31
  73   0.5655     86.710  0.0880    97.048  398.75
  74   0.5831     87.070  0.0870    97.060  404.18
  75   0.5312     87.010  0.0827    97.174  409.61
  76   0.5361     87.020  0.0908    96.942  415.07
  77   0.5702     86.590  0.0770    97.414  420.67
  78   0.5737     86.810  0.0821    97.264  426.08
  79   0.5758     86.660  0.0866    97.104  431.51
  80   0.5162     87.150  0.0888    97.014  436.94
  81   0.5353     86.820  0.0805    97.256  442.36
  82   0.5770     86.860  0.0778    97.392  447.77
  83   0.5564     86.840  0.0766    97.428  453.18
  84   0.5649     86.910  0.0767    97.464  458.73
  85   0.5827     86.820  0.0739    97.614  464.11
