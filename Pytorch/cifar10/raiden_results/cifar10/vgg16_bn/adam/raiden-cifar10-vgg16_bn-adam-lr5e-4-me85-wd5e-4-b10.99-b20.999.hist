Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2537     12.180  2.8908    11.226  7.05
   2   2.1918     15.290  2.2487    12.744  12.60
   3   2.1562     16.110  2.1919    14.684  18.04
   4   1.9630     20.600  2.0957    17.534  23.45
   5   1.8206     25.280  1.9190    21.596  28.86
   6   1.7803     28.870  1.8348    25.102  34.31
   7   1.7225     31.260  1.7643    28.922  39.73
   8   1.6316     35.170  1.7149    31.938  45.15
   9   1.4463     42.680  1.5759    38.350  50.72
  10   1.3219     48.740  1.4246    44.582  56.13
  11   1.2557     54.030  1.2917    51.034  61.54
  12   1.0859     61.220  1.1603    58.020  66.94
  13   1.0848     64.060  1.0336    63.654  72.36
  14   0.9229     65.840  0.9444    67.090  77.79
  15   0.8379     71.280  0.8744    69.724  83.28
  16   0.8366     71.780  0.8204    72.038  88.69
  17   0.7291     75.400  0.7533    74.486  94.09
  18   0.7006     76.430  0.6981    76.800  99.54
  19   0.6897     77.220  0.6664    77.996  104.99
  20   0.6828     78.590  0.6123    80.268  110.41
  21   0.6679     78.790  0.5864    81.138  115.86
  22   0.6647     80.100  0.5571    81.950  121.29
  23   0.5560     82.360  0.5294    83.102  126.82
  24   0.5653     82.460  0.4984    84.182  132.24
  25   0.5342     83.450  0.4642    85.388  137.65
  26   0.5600     82.510  0.4599    85.442  143.08
  27   0.5483     83.140  0.4384    86.094  148.49
  28   0.4883     85.200  0.4214    86.870  153.93
  29   0.5082     84.600  0.4104    87.250  159.48
  30   0.4711     85.570  0.3881    87.810  164.89
  31   0.4823     85.100  0.3682    88.476  170.32
  32   0.4711     85.440  0.3416    89.152  175.76
  33   0.5305     83.850  0.3273    89.518  181.21
  34   0.4542     86.540  0.3354    89.396  186.64
  35   0.4808     86.010  0.3177    90.146  192.10
  36   0.4254     87.000  0.3073    90.332  197.55
  37   0.4474     86.630  0.2886    90.876  203.09
  38   0.4806     85.890  0.2893    90.918  208.54
  39   0.4205     87.570  0.2875    91.136  213.97
  40   0.4100     87.310  0.2713    91.406  219.40
  41   0.4205     87.360  0.2658    91.734  224.85
  42   0.4228     87.690  0.2570    91.856  230.29
  43   0.4244     87.900  0.2409    92.432  235.70
  44   0.4305     87.250  0.2396    92.530  241.32
  45   0.3831     88.520  0.2345    92.578  246.76
  46   0.4008     88.560  0.2189    93.084  252.23
  47   0.4178     87.880  0.2216    93.128  257.65
  48   0.3989     87.860  0.2188    93.204  263.10
  49   0.4056     88.190  0.2099    93.456  268.52
  50   0.3717     88.920  0.2051    93.590  274.10
  51   0.4554     87.450  0.1979    93.838  279.54
  52   0.4595     87.990  0.2018    93.696  284.96
  53   0.4212     88.250  0.2009    93.808  290.38
  54   0.4013     88.920  0.1875    94.220  295.82
  55   0.3919     88.450  0.1895    94.080  301.26
  56   0.3878     89.390  0.1888    94.128  306.70
  57   0.3997     88.860  0.1804    94.398  312.27
  58   0.4277     88.010  0.1761    94.486  317.70
  59   0.4300     88.500  0.1838    94.306  323.12
  60   0.3905     89.090  0.1702    94.656  328.54
  61   0.4321     87.750  0.1665    94.860  333.96
  62   0.4114     89.010  0.1700    94.806  339.41
  63   0.3990     89.170  0.1677    94.852  344.83
  64   0.4220     88.750  0.1636    94.958  350.26
  65   0.4615     88.360  0.1586    95.094  355.86
  66   0.4346     88.670  0.1540    95.194  361.27
  67   0.4136     89.210  0.1564    95.152  366.69
  68   0.3955     89.370  0.1563    95.100  372.13
  69   0.3955     89.300  0.1539    95.284  377.56
  70   0.3933     89.560  0.1503    95.320  383.00
  71   0.3808     89.270  0.1412    95.656  388.56
  72   0.4046     88.930  0.1441    95.602  394.02
  73   0.4377     88.870  0.1424    95.598  399.44
  74   0.3911     89.470  0.1346    95.860  404.87
  75   0.4039     89.170  0.1359    95.878  410.33
  76   0.4213     89.230  0.1311    96.012  415.79
  77   0.4194     89.290  0.1348    95.874  421.36
  78   0.4391     89.640  0.1319    95.890  426.78
  79   0.4419     89.070  0.1304    96.000  432.23
  80   0.4257     89.420  0.1311    96.016  437.63
  81   0.4461     88.540  0.1244    96.290  443.07
  82   0.4389     89.750  0.1321    96.080  448.49
  83   0.3847     89.860  0.1324    95.938  453.93
  84   0.4071     89.770  0.1330    95.974  459.36
  85   0.4033     89.900  0.1226    96.200  464.93
