Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.1795     14.990  2.5208    12.726  7.09
   2   2.0241     18.440  2.1170    16.232  12.55
   3   1.8594     24.560  1.9462    20.796  17.94
   4   1.7794     28.250  1.8485    25.180  23.34
   5   1.6549     34.450  1.7399    30.108  28.73
   6   1.4785     41.820  1.5868    37.466  34.14
   7   1.3479     48.210  1.4544    43.474  39.57
   8   1.2641     52.750  1.3204    50.618  44.97
   9   1.0947     60.240  1.1811    56.514  50.47
  10   0.9881     65.040  1.0627    61.656  55.89
  11   0.9422     67.830  0.9625    66.292  61.28
  12   0.8620     70.290  0.8835    69.326  66.67
  13   0.7867     73.260  0.8051    72.502  72.07
  14   0.7506     74.720  0.7449    74.998  77.48
  15   0.6991     75.840  0.7083    76.654  82.89
  16   0.6638     77.470  0.6564    78.432  88.45
  17   0.6386     78.800  0.6092    79.944  93.84
  18   0.6021     79.860  0.5707    81.358  99.23
  19   0.6224     79.670  0.5408    82.730  104.65
  20   0.5687     81.450  0.5212    83.210  110.03
  21   0.5384     82.140  0.4863    84.298  115.44
  22   0.5462     81.740  0.4655    84.878  120.83
  23   0.5073     83.410  0.4443    85.528  126.41
  24   0.5097     82.810  0.4145    86.584  131.81
  25   0.4952     84.400  0.3943    87.240  137.22
  26   0.5339     82.560  0.3940    87.290  142.62
  27   0.4696     84.740  0.3878    87.544  148.01
  28   0.5030     84.960  0.3552    88.748  153.41
  29   0.4646     85.200  0.3359    89.256  158.95
  30   0.4692     86.130  0.3147    89.784  164.37
  31   0.4861     84.350  0.3098    89.982  169.79
  32   0.4541     85.680  0.3202    89.666  175.23
  33   0.5015     85.690  0.2927    90.580  180.65
  34   2.4849     82.160  0.3033    90.474  186.05
  35   0.4863     84.930  0.4290    87.318  191.43
  36   0.4298     86.500  0.3180    89.750  196.84
  37   0.4419     86.630  0.2664    91.546  202.28
  38   0.4103     87.430  0.2455    91.882  207.68
  39   0.4200     88.030  0.2294    92.528  213.10
  40   0.4056     87.100  0.2185    92.832  218.53
  41   0.4260     87.350  0.2213    92.844  223.94
  42   0.4386     87.660  0.2156    92.988  229.32
  43   0.4321     87.670  0.2145    93.046  234.86
  44   0.4324     87.750  0.2105    93.256  240.26
  45   0.4810     86.620  0.2194    92.962  245.64
  46   0.5969     83.390  0.2935    91.158  251.08
  47   0.4372     86.470  0.3095    90.078  256.48
  48   0.4395     87.190  0.2336    92.658  261.89
  49   0.4587     87.530  0.1918    93.716  267.34
  50   0.4651     86.850  0.1739    94.304  272.87
  51   0.4189     88.400  0.1620    94.688  278.29
  52   0.4025     88.680  0.1582    94.866  283.70
  53   0.4439     87.920  0.1534    95.018  289.14
  54   0.4533     87.830  0.1453    95.404  294.54
  55   0.4425     88.320  0.1465    95.238  299.95
  56   0.4238     88.450  0.1438    95.422  305.36
  57   0.4477     88.330  0.1390    95.556  310.77
  58   0.4697     88.070  0.1422    95.406  316.29
  59   0.4618     88.460  0.1276    95.894  321.72
  60   0.4536     88.690  0.1286    95.848  327.12
  61   0.4599     88.000  0.1318    95.768  332.54
  62   0.4555     88.220  0.1234    96.024  337.93
  63   0.4206     89.020  0.1272    95.964  343.33
  64   0.4450     88.160  0.1202    96.008  348.85
  65   0.4488     88.790  0.1147    96.270  354.23
  66   0.4449     88.990  0.1115    96.262  359.64
  67   0.5004     87.920  0.1128    96.354  365.04
  68   0.4828     88.010  0.1114    96.352  370.44
  69   0.4377     89.210  0.1071    96.476  375.86
  70   0.4302     88.740  0.1073    96.458  381.26
  71   0.4408     88.910  0.1041    96.626  386.67
  72   0.4945     88.650  0.0969    96.918  392.25
  73   0.4477     88.930  0.0962    96.878  397.65
  74   0.4586     89.000  0.0947    96.968  403.06
  75   0.4919     88.800  0.0949    96.924  408.48
  76   0.4744     88.400  0.0977    96.890  413.91
  77   0.4740     88.800  0.0914    97.050  419.35
  78   0.4652     89.030  0.0952    96.932  424.75
  79   0.4264     89.560  0.0921    97.026  430.32
  80   0.4710     88.910  0.0837    97.350  435.73
  81   0.4833     88.840  0.0832    97.340  441.13
  82   0.4667     88.840  0.0924    97.030  446.53
  83   0.4519     89.240  0.0854    97.254  451.91
  84   0.4456     89.520  0.0883    97.236  457.32
  85   0.5027     88.510  0.0780    97.394  462.86
