Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2280     14.680  2.4845    12.506  7.09
   2   2.1119     15.540  2.1821    15.178  12.48
   3   1.8805     24.570  1.9706    20.798  17.91
   4   1.7278     29.510  1.8371    25.666  23.35
   5   1.5491     38.630  1.6750    32.212  28.88
   6   1.4260     45.130  1.5140    40.534  34.27
   7   1.3016     50.990  1.3528    48.902  39.66
   8   1.0929     60.200  1.1877    56.264  45.06
   9   1.0273     62.700  1.0441    62.474  50.46
  10   0.8937     68.480  0.9336    67.100  55.86
  11   0.8072     71.760  0.8535    70.504  61.28
  12   0.7845     73.530  0.7811    73.428  66.84
  13   0.7486     74.470  0.7201    75.670  72.25
  14   0.7353     76.190  0.6637    78.078  77.67
  15   0.6674     78.320  0.6279    79.158  83.09
  16   0.6659     79.190  0.5859    80.764  88.49
  17   0.6088     80.600  0.5541    81.984  93.90
  18   0.5804     81.400  0.5258    83.050  99.32
  19   0.5486     82.810  0.4970    83.958  104.83
  20   0.5959     81.400  0.4727    84.842  110.24
  21   0.5156     83.610  0.4503    85.486  115.62
  22   0.4904     84.210  0.4156    86.416  121.04
  23   0.5123     83.880  0.4208    86.582  126.48
  24   0.5431     84.130  0.3890    87.356  131.88
  25   0.4580     85.560  0.3656    88.100  137.27
  26   0.5166     84.170  0.3568    88.466  142.80
  27   0.4802     84.810  0.3521    88.682  148.21
  28   0.4512     85.830  0.3410    89.104  153.64
  29   0.4612     85.890  0.3099    89.894  159.04
  30   0.4873     85.640  0.2987    90.300  164.44
  31   0.4653     85.560  0.2923    90.458  169.84
  32   0.4778     85.960  0.2972    90.406  175.27
  33   0.4892     85.020  0.2831    90.964  180.83
  34   0.4505     86.920  0.2642    91.396  186.25
  35   0.4537     87.040  0.2612    91.642  191.69
  36   0.4589     86.580  0.2591    91.582  197.09
  37   0.4355     87.660  0.2335    92.556  202.49
  38   0.4202     87.540  0.2190    92.906  207.91
  39   0.4419     87.770  0.2178    92.978  213.34
  40   0.4590     87.120  0.2167    92.968  218.87
  41   0.4225     87.940  0.2124    93.208  224.27
  42   0.4451     87.290  0.2019    93.490  229.68
  43   0.4244     88.310  0.1988    93.580  235.11
  44   0.3935     88.330  0.1881    93.902  240.51
  45   0.4143     88.040  0.1884    93.990  245.94
  46   0.4405     87.990  0.1742    94.410  251.36
  47   0.4344     87.710  0.1718    94.358  256.89
  48   0.4480     87.910  0.1605    94.846  262.28
  49   0.4100     88.490  0.1672    94.600  267.68
  50   0.3975     88.830  0.1593    94.762  273.08
  51   0.4263     88.640  0.1574    94.958  278.48
  52   0.4446     88.230  0.1565    95.002  283.88
  53   0.4213     88.260  0.1433    95.378  289.29
  54   0.4242     88.940  0.1390    95.496  294.72
  55   0.4759     88.070  0.1412    95.492  300.24
  56   0.4317     88.350  0.1451    95.302  305.66
  57   0.4367     88.310  0.1399    95.528  311.08
  58   0.4572     88.040  0.1449    95.370  316.47
  59   0.4339     88.620  0.1383    95.506  321.87
  60   0.3949     88.930  0.1246    96.086  327.26
  61   0.4481     88.440  0.1206    96.226  332.67
  62   0.4292     88.890  0.1180    96.238  338.20
  63   0.4230     88.820  0.1207    96.160  343.62
  64   0.4430     88.540  0.1202    96.090  349.03
  65   0.4079     89.600  0.1172    96.184  354.46
  66   0.4540     88.090  0.1123    96.456  359.85
  67   0.4491     88.900  0.1119    96.468  365.27
  68   0.4146     89.250  0.1079    96.590  370.67
  69   0.4810     88.370  0.1069    96.598  376.21
  70   0.4900     87.970  0.1149    96.296  381.64
  71   0.4853     88.520  0.1067    96.582  387.03
  72   0.4598     88.590  0.1052    96.716  392.46
  73   0.4378     88.850  0.1036    96.720  397.86
  74   0.4459     88.640  0.0975    96.918  403.28
  75   0.4713     88.930  0.0978    96.906  408.69
  76   0.4452     88.940  0.0945    96.992  414.23
  77   0.4720     88.400  0.0949    96.988  419.64
  78   0.4360     89.700  0.0959    96.968  425.06
  79   0.4213     89.560  0.0928    97.040  430.46
  80   0.4633     89.320  0.0867    97.256  435.89
  81   0.4745     88.740  0.0876    97.228  441.29
  82   0.4447     88.990  0.0998    96.914  446.72
  83   0.4683     88.800  0.0934    97.006  452.28
  84   0.4267     89.390  0.0894    97.116  457.71
  85   0.4691     89.080  0.0812    97.392  463.13
  86   0.4279     89.480  0.0868    97.224  468.52
  87   0.4216     89.790  0.0822    97.468  473.96
  88   0.4616     89.660  0.0801    97.436  479.35
  89   0.4538     89.520  0.0823    97.460  484.77
  90   0.4325     89.280  0.0820    97.422  490.33
