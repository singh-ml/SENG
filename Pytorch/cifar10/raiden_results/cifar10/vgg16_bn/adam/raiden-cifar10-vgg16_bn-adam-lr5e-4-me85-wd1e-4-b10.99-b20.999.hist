Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2919     12.250  2.9832    10.392  7.11
   2   2.2087     14.790  2.2776    11.844  12.54
   3   2.1040     17.190  2.1842    14.964  18.11
   4   1.9672     20.310  2.0507    18.260  23.54
   5   1.8790     24.560  1.9276    21.734  28.96
   6   1.8669     25.900  1.8815    24.158  34.40
   7   1.7669     28.030  1.8357    26.048  39.83
   8   1.6509     35.190  1.7502    29.666  45.23
   9   1.6562     34.570  1.6520    34.938  50.67
  10   1.4666     43.030  1.5197    41.042  56.24
  11   1.4180     46.490  1.4296    45.640  61.66
  12   1.3533     51.350  1.3400    50.480  67.07
  13   1.2167     56.530  1.2332    55.088  72.52
  14   1.0567     61.020  1.1457    58.764  77.98
  15   0.9832     65.500  1.0574    62.796  83.44
  16   0.9641     66.020  0.9680    66.368  88.85
  17   0.8972     69.270  0.9245    68.250  94.41
  18   1.0555     65.580  0.9611    67.852  99.83
  19   0.8610     71.940  0.9092    69.240  105.27
  20   0.9584     68.710  0.9116    70.864  110.70
  21   0.8000     73.420  0.8603    71.444  116.15
  22   0.7276     76.040  0.7664    74.304  121.59
  23   0.9121     71.990  0.7730    75.120  127.13
  24   1.0401     67.910  1.1374    65.618  132.57
  25   0.7961     72.860  0.9381    68.950  138.00
  26   0.6695     77.190  0.7436    74.948  143.43
  27   0.6403     78.000  0.6657    77.446  148.88
  28   0.6181     78.930  0.6090    79.610  154.30
  29   0.5959     80.390  0.5894    80.376  159.72
  30   0.7323     75.840  0.8068    73.764  165.31
  31   0.6229     79.040  0.6478    78.458  170.77
  32   0.5888     80.800  0.5785    80.780  176.18
  33   0.5731     81.230  0.5492    82.004  181.61
  34   0.5536     82.150  0.5205    82.826  187.06
  35   0.5221     82.400  0.5017    83.514  192.50
  36   0.5370     82.900  0.4729    84.436  197.92
  37   0.5007     83.490  0.4584    84.972  203.42
  38   0.4952     83.900  0.4443    85.338  208.88
  39   0.5142     83.220  0.4273    85.874  214.31
  40   0.5195     83.190  0.4247    86.244  219.76
  41   0.9521     79.000  0.6282    81.496  225.18
  42   0.5445     82.880  0.5376    82.480  230.63
  43   2.0506     57.370  0.9295    77.340  236.08
  44   0.9976     66.970  1.3038    63.340  241.67
  45   0.7738     72.730  0.9679    70.076  247.10
  46   0.6787     78.850  0.7298    76.776  252.55
  47   0.5921     80.180  0.5935    80.454  258.02
  48   0.5397     82.230  0.5245    82.462  263.45
  49   0.4996     83.680  0.4769    84.120  268.91
  50   0.5075     83.140  0.4797    84.582  274.34
  51   0.4666     84.490  0.4275    85.938  279.90
  52   0.4664     85.250  0.3969    86.942  285.36
  53   0.4604     84.940  0.3745    87.716  290.80
  54   0.4427     85.510  0.3549    88.334  296.25
  55   0.4310     86.040  0.3449    88.620  301.71
  56   0.4452     85.950  0.3372    89.028  307.14
  57   0.4632     85.660  0.3220    89.454  312.55
  58   0.4443     86.160  0.3141    89.664  318.11
  59   0.4321     86.450  0.2993    90.098  323.54
  60   0.4222     86.220  0.2923    90.414  329.00
  61   0.4363     86.730  0.2875    90.584  334.42
  62   0.4413     86.470  0.2743    91.062  339.86
  63   0.4528     86.200  0.2885    90.792  345.32
  64   0.5340     83.200  0.5088    84.802  350.75
  65   0.5264     83.540  0.4333    86.412  356.32
  66   0.5054     85.470  0.3715    88.054  361.81
  67   0.8366     75.320  0.6852    83.696  367.27
  68   0.6074     80.710  0.6973    79.048  372.69
  69   0.4925     84.170  0.4738    84.666  378.14
  70   0.4474     85.660  0.3673    87.712  383.60
  71   0.4388     86.400  0.3154    89.626  389.18
  72   0.4213     86.590  0.2888    90.534  394.61
  73   0.4127     86.810  0.2754    90.986  400.05
  74   0.4221     87.060  0.2569    91.484  405.51
  75   0.4252     87.130  0.2340    92.280  410.95
  76   0.4221     87.350  0.2255    92.460  416.39
  77   0.4203     87.500  0.2198    92.776  421.81
  78   0.3951     87.830  0.2148    93.002  427.29
  79   0.4344     87.280  0.2100    93.064  432.88
  80   0.4309     87.270  0.2051    93.188  438.33
  81   0.4409     87.270  0.2005    93.376  443.79
  82   0.4263     87.610  0.1890    93.852  449.25
  83   0.4300     87.840  0.1897    93.784  454.67
  84   0.4749     86.750  0.1931    93.888  460.15
  85   0.4270     88.000  0.1748    94.438  465.58
