Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5407     41.560  1.9659    27.606  7.03
   2   1.1395     58.590  1.3852    49.626  12.45
   3   1.1085     60.340  1.1665    58.800  17.86
   4   0.9519     66.160  1.0368    63.524  23.42
   5   0.9057     67.960  0.9374    67.140  28.86
   6   0.8389     70.740  0.8535    70.260  34.26
   7   0.8208     71.780  0.7850    72.962  39.68
   8   0.7825     73.130  0.7259    75.170  45.07
   9   0.7027     75.790  0.6788    76.892  50.50
  10   0.6943     76.220  0.6348    78.392  55.95
  11   0.6590     77.660  0.5996    79.762  61.50
  12   0.6152     79.070  0.5712    80.646  66.94
  13   0.6441     77.620  0.5380    81.870  72.34
  14   0.6107     79.010  0.5079    82.782  77.79
  15   0.6196     79.220  0.4843    83.518  83.20
  16   0.6089     79.930  0.4625    84.330  88.63
  17   0.5608     81.420  0.4504    84.672  94.05
  18   0.6233     80.300  0.4268    85.662  99.53
  19   0.5408     82.230  0.3998    86.492  104.95
  20   0.5641     80.480  0.3943    86.742  110.37
  21   0.6293     80.100  0.3705    87.682  115.78
  22   0.5688     81.260  0.3592    87.900  121.21
  23   0.5167     82.800  0.3458    88.258  126.65
  24   0.5503     82.620  0.3272    88.876  132.06
  25   0.4988     84.140  0.3145    89.332  137.51
  26   0.5600     83.540  0.3023    89.804  143.05
  27   0.5606     82.630  0.2899    90.268  148.48
  28   0.5107     84.160  0.2856    90.390  153.90
  29   0.5654     83.400  0.2666    91.022  159.34
  30   0.5467     83.700  0.2649    91.050  164.75
  31   0.5289     83.500  0.2534    91.436  170.16
  32   0.4899     85.270  0.2388    91.978  175.72
  33   0.5503     84.560  0.2370    91.914  181.13
  34   0.5482     84.110  0.2273    92.314  186.55
  35   0.5353     84.200  0.2214    92.380  191.97
  36   0.5858     84.390  0.2137    92.870  197.38
  37   0.5740     84.420  0.2021    93.254  202.81
  38   0.5349     85.050  0.2011    93.154  208.23
  39   0.5682     84.270  0.1907    93.534  213.74
  40   0.5473     84.940  0.1873    93.576  219.17
  41   0.5535     84.860  0.1807    93.762  224.58
  42   0.5879     84.720  0.1746    94.080  229.99
  43   0.5401     85.850  0.1700    94.156  235.40
  44   0.5562     85.740  0.1642    94.342  240.82
  45   0.5328     86.130  0.1542    94.856  246.26
  46   0.5635     84.680  0.1534    94.708  251.83
  47   0.5809     84.700  0.1468    94.984  257.28
  48   0.5802     84.960  0.1503    94.960  262.69
  49   0.5471     85.670  0.1473    95.020  268.08
  50   0.5773     86.170  0.1366    95.396  273.49
  51   0.5934     84.720  0.1328    95.498  278.94
  52   0.5339     86.570  0.1290    95.598  284.39
  53   0.5910     85.140  0.1289    95.578  289.92
  54   0.5620     85.800  0.1205    95.906  295.34
  55   0.6030     85.400  0.1199    95.934  300.76
  56   0.6266     85.100  0.1181    96.006  306.18
  57   0.5640     86.160  0.1151    96.144  311.59
  58   0.5975     86.020  0.1111    96.250  317.01
  59   0.5541     86.910  0.1108    96.306  322.44
  60   0.5461     86.550  0.1104    96.386  328.04
  61   0.5904     86.220  0.1060    96.376  333.47
  62   0.5100     86.760  0.1040    96.496  338.92
  63   0.6278     86.100  0.1007    96.592  344.33
  64   0.6535     85.650  0.0957    96.686  349.74
  65   0.6053     86.220  0.1022    96.514  355.14
  66   0.5845     86.150  0.0961    96.762  360.70
  67   0.6164     85.710  0.0932    96.946  366.14
  68   0.5749     86.420  0.0936    96.926  371.57
  69   0.5745     86.760  0.0906    96.982  376.99
  70   0.6484     85.110  0.0939    96.928  382.41
  71   0.6268     87.250  0.0877    97.092  387.86
  72   0.5777     87.370  0.0858    97.048  393.27
  73   0.6234     85.870  0.0853    97.132  398.80
  74   0.6396     85.970  0.0842    97.148  404.23
  75   0.6333     86.740  0.0811    97.230  409.67
  76   0.6277     86.030  0.0780    97.334  415.08
  77   0.5971     86.970  0.0800    97.368  420.53
  78   0.5771     86.840  0.0764    97.346  425.97
  79   0.5807     86.650  0.0789    97.340  431.42
  80   0.5577     86.890  0.0770    97.444  436.94
  81   0.5736     86.910  0.0763    97.444  442.36
  82   0.6208     86.460  0.0735    97.588  447.80
  83   0.5844     87.080  0.0750    97.506  453.21
  84   0.5772     87.270  0.0724    97.594  458.63
  85   0.5487     86.850  0.0712    97.674  464.02
