Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2583     13.070  2.5310    11.484  7.51
   2   2.0895     17.400  2.2028    14.628  12.90
   3   1.8914     23.210  2.0310    19.128  18.33
   4   1.8111     26.560  1.8681    24.290  23.80
   5   1.6238     35.770  1.7439    29.948  29.20
   6   1.4560     43.810  1.5750    38.376  34.65
   7   1.3425     47.970  1.4349    45.132  40.21
   8   1.2351     53.830  1.3221    50.614  45.61
   9   1.1143     58.600  1.2197    55.280  51.04
  10   1.0053     63.100  1.1018    59.990  56.45
  11   0.9685     65.860  1.0170    63.704  61.90
  12   0.8959     68.410  0.9367    67.240  67.35
  13   0.8846     69.930  0.8545    70.464  72.78
  14   0.7874     72.530  0.7949    72.856  78.33
  15   0.7469     74.600  0.7424    74.454  83.77
  16   0.7197     75.550  0.6864    76.538  89.21
  17   0.6649     78.330  0.6568    77.716  94.63
  18   0.6317     79.390  0.6147    79.218  100.06
  19   0.6196     79.390  0.5910    80.304  105.46
  20   0.5990     80.460  0.5531    81.716  110.86
  21   0.6010     80.440  0.5360    82.468  116.41
  22   0.5724     81.170  0.5106    83.300  121.82
  23   0.5352     82.730  0.4825    84.062  127.26
  24   0.5756     81.660  0.4651    84.888  132.69
  25   0.5231     82.900  0.4571    84.930  138.13
  26   0.5486     82.500  0.4450    85.496  143.57
  27   0.6820     83.330  0.4298    85.650  149.00
  28   0.5495     83.410  0.4543    85.462  154.59
  29   0.5315     82.980  0.4736    85.086  160.03
  30   0.5106     83.430  0.4068    86.562  165.45
  31   0.5351     83.640  0.3888    87.266  170.92
  32   0.4939     84.510  0.3823    87.470  176.32
  33   0.5273     84.610  0.3563    88.294  181.74
  34   0.5994     83.830  0.3735    87.808  187.20
  35   0.4837     85.150  0.3781    87.796  192.74
  36   0.4534     85.440  0.3652    88.208  198.18
  37   0.4956     84.880  0.3322    89.216  203.61
  38   0.4448     85.490  0.3334    89.070  209.04
  39   0.4976     86.090  0.3032    89.982  214.48
  40   0.4895     85.160  0.3013    90.118  219.90
  41   0.4533     85.620  0.3041    90.032  225.34
  42   0.5285     86.240  0.3072    89.888  230.90
  43   0.4786     85.630  0.2971    90.530  236.30
  44   0.4180     86.740  0.2794    90.684  241.74
  45   0.4531     86.050  0.2774    90.868  247.17
  46   0.4403     87.180  0.2508    91.664  252.58
  47   0.4656     86.030  0.2335    92.346  257.99
  48   0.4732     86.600  0.2533    91.820  263.44
  49   0.4732     86.680  0.2381    92.176  269.01
  50   0.4814     86.640  0.2377    92.236  274.44
  51   0.4381     86.750  0.2429    92.064  279.88
  52   1.0847     84.930  0.2855    92.288  285.30
  53   0.4733     86.410  0.3005    90.868  290.73
  54   0.4360     86.680  0.2432    92.318  296.15
  55   0.5017     85.930  0.4188    89.204  301.57
  56   0.4139     87.290  0.2610    91.448  307.00
  57   0.3979     87.700  0.2144    92.904  312.55
  58   0.4376     87.910  0.1924    93.648  317.99
  59   0.4237     87.780  0.1912    93.778  323.42
  60   0.4287     87.570  0.1760    94.106  328.85
  61   0.4072     88.540  0.1751    94.268  334.27
  62   0.4187     87.820  0.1570    94.794  339.68
  63   0.4140     88.240  0.1580    94.832  345.28
  64   0.4379     88.100  0.1608    94.734  350.74
  65   0.4402     87.770  0.1544    94.986  356.17
  66   0.4358     87.880  0.1474    95.098  361.59
  67   0.4066     88.200  0.1446    95.220  367.04
  68   0.4350     88.280  0.1451    95.192  372.45
  69   0.4448     88.260  0.1261    95.818  377.90
  70   0.4226     88.560  0.1257    95.992  383.49
  71   0.4353     87.910  0.1237    95.836  388.93
  72   0.4415     88.410  0.1220    96.032  394.37
  73   0.4351     88.160  0.1233    96.028  399.79
  74   0.4602     88.660  0.1221    96.032  405.23
  75   0.4550     88.340  0.1233    96.012  410.67
  76   0.4543     88.410  0.1188    95.996  416.10
  77   0.4602     88.220  0.1021    96.674  421.64
  78   0.4705     88.720  0.1065    96.448  427.10
  79   0.4306     88.720  0.1036    96.646  432.50
  80   0.4344     89.180  0.0944    96.834  437.93
  81   0.4600     88.520  0.0971    96.892  443.37
  82   0.4445     89.000  0.0945    97.008  448.77
  83   0.4279     89.530  0.0931    96.998  454.24
  84   0.4813     88.480  0.0891    97.024  459.71
  85   0.5068     88.380  0.0954    96.900  465.23
  86   0.4274     89.420  0.0952    96.976  470.64
  87   0.4768     89.160  0.0868    97.198  476.07
  88   0.4294     89.240  0.0843    97.262  481.48
  89   0.4878     88.290  0.0811    97.292  486.92
  90   0.4683     89.110  0.0847    97.232  492.37
