Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4196382720 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2330     14.380  2.7531    11.888  7.16
   2   2.1872     14.410  2.2066    14.442  12.57
   3   2.0173     19.880  2.0847    17.802  18.11
   4   1.8577     24.280  1.9075    22.360  23.52
   5   1.7406     27.540  1.8295    25.034  28.93
   6   1.5635     38.210  1.7007    31.612  34.37
   7   1.4055     44.870  1.5429    40.320  39.79
   8   1.3057     50.390  1.3887    47.096  45.19
   9   1.1464     58.930  1.2339    54.652  50.70
  10   1.0667     62.970  1.1025    60.250  56.11
  11   0.9856     65.560  0.9983    64.790  61.55
  12   0.8785     69.690  0.8996    68.692  66.95
  13   0.8064     72.860  0.8455    70.990  72.36
  14   0.7621     74.250  0.7802    73.612  77.81
  15   0.8080     75.330  0.7253    75.816  83.23
  16   0.7001     76.410  0.6975    76.994  88.79
  17   0.6749     77.870  0.6510    78.680  94.19
  18   0.6726     78.650  0.6257    79.644  99.63
  19   0.7301     78.930  0.5979    80.700  105.08
  20   0.5644     81.600  0.5665    81.654  110.52
  21   0.6084     80.350  0.5219    82.950  115.93
  22   0.5589     82.170  0.4958    83.934  121.36
  23   0.7329     80.690  0.5076    83.686  126.91
  24   0.5928     81.310  0.5004    84.036  132.33
  25   0.4965     84.250  0.4971    84.106  137.77
  26   0.4668     84.580  0.4498    85.552  143.21
  27   0.4786     84.940  0.4235    86.420  148.65
  28   0.4869     84.510  0.3910    87.344  154.09
  29   0.4672     85.050  0.3772    87.832  159.53
  30   0.4520     85.640  0.3707    88.112  165.15
  31   0.5164     86.060  0.3448    88.848  170.60
  32   0.4562     86.140  0.3433    89.090  176.02
  33   0.4614     85.640  0.3341    89.376  181.45
  34   0.4722     86.030  0.3222    89.580  186.87
  35   0.4295     86.360  0.3151    89.868  192.32
  36   0.4444     86.500  0.2884    90.612  197.74
  37   0.4334     87.140  0.2849    90.786  203.28
  38   0.4309     87.590  0.2674    91.258  208.73
  39   0.4642     85.980  0.2605    91.734  214.13
  40   0.4662     87.130  0.2638    91.638  219.58
  41   0.4763     86.580  0.2507    91.936  224.97
  42   0.4112     88.100  0.2439    92.188  230.39
  43   0.4256     87.510  0.2300    92.662  235.96
  44   0.4371     87.430  0.2265    92.676  241.39
  45   0.4127     88.250  0.2085    93.244  246.83
  46   0.3873     88.520  0.2082    93.318  252.27
  47   0.4212     87.920  0.1946    93.674  257.72
  48   0.4308     88.810  0.1879    94.084  263.14
  49   0.4165     88.510  0.1944    93.828  268.58
  50   0.4068     88.370  0.1836    94.068  274.01
  51   0.4373     88.170  0.1754    94.294  279.52
  52   0.3784     88.700  0.1699    94.564  284.94
  53   0.4138     88.810  0.1755    94.400  290.36
  54   0.4352     88.860  0.1576    94.904  295.79
  55   0.4279     88.480  0.1643    94.920  301.24
  56   0.4136     88.960  0.1574    94.992  306.64
  57   0.4156     89.450  0.1587    94.952  312.18
  58   0.4040     88.690  0.1525    95.138  317.61
  59   0.4656     87.700  0.1473    95.362  323.04
  60   0.4232     88.830  0.1481    95.334  328.44
  61   0.4089     89.260  0.1471    95.392  333.86
  62   0.4092     89.680  0.1387    95.770  339.30
  63   0.4222     89.630  0.1324    95.820  344.71
  64   0.4332     88.640  0.1302    95.956  350.17
  65   0.3992     90.030  0.1271    95.982  355.57
  66   0.4340     88.920  0.1304    95.988  361.04
  67   0.4157     89.200  0.1220    96.190  366.47
  68   0.4613     89.120  0.1219    96.156  371.88
  69   0.4181     89.360  0.1234    96.284  377.30
  70   0.4447     89.100  0.1174    96.224  382.69
  71   0.4500     89.270  0.1252    96.156  388.26
  72   0.4249     89.290  0.1193    96.306  393.68
  73   0.4249     88.910  0.1207    96.270  399.11
  74   0.4306     89.490  0.1148    96.490  404.56
  75   0.3972     89.650  0.1157    96.406  409.98
  76   0.4543     89.570  0.1079    96.594  415.39
  77   0.4377     89.420  0.1109    96.512  420.96
  78   0.4629     88.670  0.1113    96.580  426.37
  79   0.3913     89.370  0.1058    96.774  431.80
  80   0.4262     89.980  0.1002    96.844  437.22
  81   0.4241     89.890  0.0956    96.960  442.64
  82   0.4200     89.360  0.1025    96.840  448.09
  83   0.4320     89.960  0.0985    96.874  453.52
  84   0.4093     89.830  0.0985    96.960  458.95
  85   0.4254     90.220  0.0926    97.134  464.50
