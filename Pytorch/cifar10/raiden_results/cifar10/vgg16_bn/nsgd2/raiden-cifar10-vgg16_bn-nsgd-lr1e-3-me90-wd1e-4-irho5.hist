Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '5', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 60273909248 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4670     47.240  1.7564    34.808  35.86
   2   1.1458     58.840  1.3188    51.820  66.45
   3   0.9738     64.880  1.1149    60.078  97.58
   4   0.8935     67.740  0.9651    65.756  129.90
   5   0.8011     71.270  0.8591    69.812  160.41
   6   0.8811     69.830  0.7815    72.540  190.93
   7   0.6885     76.010  0.7219    75.030  221.65
   8   0.6525     77.520  0.6621    77.026  252.30
   9   0.6401     78.170  0.6196    78.552  282.85
  10   0.6690     77.700  0.5842    79.998  313.58
  11   0.6019     79.570  0.5515    80.972  344.33
  12   0.5883     79.490  0.5160    82.188  374.89
  13   0.5770     80.300  0.4881    83.474  405.34
  14   0.5907     80.130  0.4664    83.946  436.02
  15   0.5917     78.750  0.4483    84.556  466.65
  16   0.5125     82.750  0.4284    85.206  497.19
  17   0.5502     80.890  0.4059    86.166  527.74
  18   0.4756     83.860  0.3905    86.482  558.55
  19   0.4972     82.790  0.3778    87.110  591.22
  20   0.5687     80.750  0.3488    88.008  623.43
  21   0.5521     81.830  0.3435    88.216  654.08
  22   0.4784     83.920  0.3290    88.910  684.94
  23   0.4613     84.470  0.3174    89.118  715.57
  24   0.4821     84.020  0.2986    89.726  746.18
  25   0.4550     85.080  0.2881    90.110  776.93
  26   0.4568     85.030  0.2791    90.308  807.75
  27   0.4903     83.930  0.2680    90.806  838.36
  28   0.4595     84.870  0.2565    91.228  868.95
  29   0.4659     84.930  0.2526    91.280  899.66
  30   0.5264     83.680  0.2339    91.904  930.37
  31   0.4922     83.940  0.2262    92.224  960.98
  32   0.4903     84.210  0.2242    92.276  991.58
  33   0.4760     84.690  0.2102    92.816  1022.44
  34   0.4982     84.090  0.2024    92.878  1053.01
  35   0.4220     86.170  0.1960    93.290  1085.22
  36   0.4320     86.260  0.1894    93.458  1115.79
  37   0.4539     85.740  0.1886    93.468  1148.15
  38   0.4807     85.210  0.1736    93.964  1180.32
  39   0.4271     86.040  0.1731    93.932  1210.94
  40   0.4376     86.590  0.1594    94.460  1241.54
  41   0.4990     84.920  0.1568    94.466  1272.35
  42   0.4688     85.720  0.1514    94.872  1304.77
  43   0.4332     86.760  0.1438    95.042  1335.37
  44   0.4948     85.670  0.1388    95.250  1366.20
  45   0.4741     85.800  0.1381    95.316  1398.67
  46   0.4819     85.980  0.1346    95.224  1429.36
  47   0.4655     86.300  0.1266    95.672  1461.63
  48   0.5068     85.450  0.1278    95.670  1492.16
  49   0.4489     86.770  0.1199    95.824  1522.91
  50   0.4806     86.020  0.1135    96.078  1553.72
  51   0.4874     85.980  0.1094    96.242  1584.35
  52   0.4503     86.980  0.1039    96.396  1614.84
  53   0.5382     85.930  0.1020    96.436  1647.13
  54   0.4724     86.850  0.1026    96.384  1677.78
  55   0.4486     87.540  0.0935    96.740  1710.02
  56   0.5021     86.670  0.0907    96.926  1740.58
  57   0.4536     87.500  0.0956    96.624  1771.44
  58   0.4811     87.400  0.0878    96.994  1802.16
  59   0.4735     87.300  0.0873    97.058  1832.67
  60   0.5097     86.810  0.0850    97.030  1863.26
  61   0.4656     87.340  0.0854    97.000  1894.12
  62   0.4866     87.400  0.0789    97.214  1924.98
