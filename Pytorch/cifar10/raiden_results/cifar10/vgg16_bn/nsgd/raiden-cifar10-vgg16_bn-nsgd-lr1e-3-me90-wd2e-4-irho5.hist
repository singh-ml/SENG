Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--batch-size=256', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '5', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 60273909248 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3846     48.560  1.7434    34.902  36.44
   2   1.1857     57.050  1.3124    52.142  69.03
   3   1.0881     61.470  1.1295    59.618  100.37
   4   0.8815     68.050  0.9776    65.320  131.23
   5   0.8926     68.500  0.8722    69.444  162.36
   6   0.7974     71.400  0.7925    72.214  193.41
   7   0.7723     72.790  0.7241    74.856  224.27
   8   0.7527     74.160  0.6727    76.782  255.13
   9   0.7457     73.930  0.6296    78.258  286.18
  10   0.6422     77.750  0.5945    79.566  317.11
  11   0.6113     78.980  0.5547    80.786  347.99
  12   0.5863     79.940  0.5271    81.958  378.86
  13   0.5980     79.380  0.5004    82.716  410.00
  14   0.5377     81.470  0.4729    83.640  441.05
  15   0.5303     81.830  0.4543    84.460  471.87
  16   0.5112     82.750  0.4292    85.212  502.80
  17   0.4898     83.100  0.4047    86.126  535.80
  18   0.5924     80.340  0.3972    86.398  566.92
  19   0.5257     82.130  0.3736    87.128  597.91
  20   0.5361     82.140  0.3624    87.572  628.74
  21   0.5210     82.360  0.3427    88.252  659.87
  22   0.4875     83.730  0.3304    88.722  691.26
  23   0.5055     83.400  0.3210    89.074  722.17
  24   0.4668     84.630  0.3022    89.670  752.98
  25   0.4820     83.710  0.2984    89.664  784.07
  26   0.4234     85.830  0.2806    90.310  815.13
  27   0.4657     84.530  0.2724    90.626  846.01
  28   0.4703     84.260  0.2590    90.876  876.87
  29   0.4475     85.110  0.2537    91.154  907.99
  30   0.4376     85.540  0.2390    91.790  938.91
  31   0.4281     85.880  0.2337    91.944  971.43
  32   0.4832     84.410  0.2223    92.296  1002.30
  33   0.4804     84.750  0.2166    92.568  1033.39
  34   0.4473     85.380  0.2047    92.996  1064.44
  35   0.4331     86.170  0.1989    93.212  1095.31
  36   0.4560     85.520  0.1966    93.210  1124.55
  37   0.4833     85.140  0.1829    93.702  1155.57
  38   0.5005     84.720  0.1786    93.786  1186.59
  39   0.4730     85.390  0.1751    93.966  1219.13
  40   0.4413     86.760  0.1616    94.322  1250.02
