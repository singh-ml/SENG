Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--batch-size=256', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 60273909248 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3699     49.580  1.7437    35.476  36.08
   2   1.1163     59.810  1.3002    53.108  67.05
   3   0.9893     64.540  1.0629    62.454  96.25
   4   0.8631     69.600  0.9182    68.076  125.44
   5   0.7672     73.430  0.7841    73.018  154.77
   6   0.7893     74.030  0.7174    75.510  185.54
   7   0.6831     76.270  0.6470    78.068  216.23
   8   0.6692     77.200  0.5916    80.012  245.64
   9   0.5872     80.130  0.5515    81.196  276.52
  10   0.6197     78.670  0.5169    82.418  305.82
  11   0.5342     81.980  0.4822    83.754  336.50
  12   0.5194     82.240  0.4482    84.922  367.38
  13   0.6110     80.170  0.4285    85.314  398.40
  14   0.5450     81.680  0.3941    86.566  430.84
  15   0.4620     84.900  0.3753    87.296  462.04
  16   0.4511     84.680  0.3589    87.690  492.97
  17   0.4531     84.570  0.3421    88.542  523.80
  18   0.4416     84.850  0.3248    89.040  554.56
  19   0.4826     83.200  0.3053    89.624  585.28
  20   0.5252     83.170  0.2891    90.062  614.45
  21   0.4362     84.970  0.2742    90.708  643.53
  22   0.4589     84.810  0.2625    91.118  674.29
  23   0.4348     85.540  0.2479    91.480  706.79
  24   0.4404     85.840  0.2356    91.984  737.74
  25   0.4314     85.740  0.2258    92.388  768.59
  26   0.4374     85.590  0.2174    92.654  799.29
  27   0.4739     85.580  0.2047    92.982  828.27
  28   0.4250     86.690  0.1941    93.356  859.15
  29   0.4211     86.880  0.1850    93.634  890.08
  30   0.4090     87.540  0.1774    93.932  920.82
  31   0.5132     84.620  0.1659    94.262  951.55
  32   0.4525     85.950  0.1631    94.420  982.32
  33   0.4249     87.050  0.1552    94.708  1013.22
  34   0.4026     87.820  0.1439    95.088  1043.91
  35   0.4114     87.340  0.1343    95.440  1074.59
  36   0.4322     87.320  0.1275    95.536  1105.49
  37   0.4394     87.450  0.1232    95.710  1134.65
  38   0.4425     87.530  0.1175    96.002  1165.36
  39   0.4289     88.010  0.1089    96.280  1196.07
  40   0.4457     87.550  0.1066    96.392  1225.20
  41   0.4023     88.620  0.1019    96.510  1256.13
  42   0.4529     87.670  0.0957    96.756  1286.95
  43   0.4515     88.070  0.0886    96.982  1317.66
  44   0.3981     88.660  0.0861    97.042  1348.60
  45   0.4398     88.420  0.0786    97.328  1379.43
  46   0.4193     88.970  0.0794    97.242  1410.22
  47   0.4182     89.410  0.0720    97.614  1439.20
  48   0.4402     89.170  0.0675    97.704  1468.15
  49   0.4146     89.240  0.0638    97.818  1498.96
  50   0.4201     89.050  0.0601    97.936  1529.72
  51   0.4605     88.930  0.0573    98.064  1560.45
  52   0.4604     89.030  0.0584    98.046  1589.80
  53   0.4256     89.320  0.0537    98.178  1620.73
