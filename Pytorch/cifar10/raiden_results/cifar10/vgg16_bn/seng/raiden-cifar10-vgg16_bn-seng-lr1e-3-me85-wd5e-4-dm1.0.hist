Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5347     41.870  1.9468    27.046  7.82
   2   1.3311     50.060  1.5242    42.628  13.66
   3   1.2193     55.220  1.3584    49.602  19.50
   4   1.1037     59.940  1.2388    54.976  25.32
   5   1.0319     62.480  1.1415    58.698  31.14
   6   0.9633     64.760  1.0622    61.938  36.94
   7   0.9116     66.400  0.9903    64.404  42.87
   8   0.8646     69.090  0.9312    66.888  48.69
   9   0.8285     70.120  0.8837    68.272  54.49
  10   0.7946     72.130  0.8358    70.046  60.33
  11   0.7595     73.120  0.8016    71.638  66.12
  12   0.7432     73.710  0.7628    73.084  71.93
  13   0.7317     73.990  0.7373    73.814  77.91
  14   0.7019     75.580  0.7001    75.280  83.72
  15   0.6939     76.040  0.6837    76.146  89.54
  16   0.6704     76.730  0.6549    76.976  95.39
  17   0.6589     77.120  0.6364    77.636  101.19
  18   0.6494     77.610  0.6141    78.468  107.01
  19   0.6473     78.250  0.5931    79.062  112.82
  20   0.6450     77.770  0.5795    79.844  118.79
  21   0.6036     79.080  0.5667    80.200  124.60
  22   0.5946     79.260  0.5469    80.882  130.43
  23   0.5926     79.610  0.5298    81.498  136.24
  24   0.5730     80.710  0.5178    81.954  142.08
  25   0.5855     79.910  0.5005    82.582  147.91
  26   0.5735     80.530  0.4887    82.918  153.71
  27   0.5640     81.100  0.4793    83.190  159.64
  28   0.5655     81.130  0.4658    83.920  165.46
  29   0.5439     81.980  0.4585    83.788  171.25
  30   0.5492     81.460  0.4474    84.366  177.06
  31   0.5405     81.900  0.4337    84.872  182.87
  32   0.5480     81.750  0.4269    85.098  188.68
  33   0.5328     82.310  0.4139    85.520  194.61
  34   0.5422     82.050  0.4120    85.614  200.42
  35   0.5290     82.350  0.3965    86.220  206.24
  36   0.5373     82.350  0.3879    86.502  212.08
  37   0.5336     82.600  0.3850    86.498  217.89
  38   0.5372     82.670  0.3741    86.914  223.68
  39   0.5314     82.860  0.3641    87.344  229.51
  40   0.5113     82.950  0.3574    87.722  235.45
  41   0.5299     83.420  0.3428    88.056  241.25
  42   0.5278     83.080  0.3384    88.260  247.07
  43   0.5201     83.220  0.3284    88.534  252.90
  44   0.5269     82.980  0.3239    88.746  258.73
  45   0.5118     83.560  0.3198    89.018  264.55
  46   0.5256     83.530  0.3110    89.234  270.43
  47   0.5155     83.670  0.2979    89.576  276.26
  48   0.5206     83.630  0.2951    89.758  282.26
  49   0.5371     83.470  0.2887    89.900  288.05
  50   0.5201     83.960  0.2871    89.946  293.81
  51   0.5145     83.910  0.2801    90.302  299.64
  52   0.5424     83.690  0.2768    90.308  305.46
  53   0.5245     83.720  0.2729    90.490  311.29
  54   0.5270     83.690  0.2615    90.906  317.26
  55   0.5347     83.730  0.2564    90.960  323.07
  56   0.5265     84.020  0.2503    91.258  328.93
  57   0.5437     83.950  0.2442    91.298  334.74
  58   0.5446     84.120  0.2358    91.880  340.56
  59   0.5435     83.940  0.2344    91.730  346.36
  60   0.5285     83.700  0.2315    91.890  352.17
  61   0.5361     84.190  0.2279    92.078  358.13
  62   0.5301     84.450  0.2229    92.198  363.98
  63   0.5352     84.600  0.2185    92.380  369.81
  64   0.5346     84.530  0.2135    92.546  375.65
  65   0.5321     84.240  0.2103    92.584  381.47
  66   0.5568     83.900  0.2050    92.770  387.31
  67   0.5417     84.980  0.2066    92.902  393.22
  68   0.5611     84.260  0.1956    93.212  399.06
  69   0.5572     83.990  0.1901    93.334  404.88
  70   0.5534     84.420  0.1894    93.494  410.69
  71   0.5462     84.480  0.1823    93.726  416.54
  72   0.5491     84.670  0.1795    93.658  422.38
  73   0.5573     84.830  0.1731    93.968  428.21
  74   0.5681     84.870  0.1737    93.938  434.16
  75   0.5580     84.720  0.1690    94.064  439.98
  76   0.5525     85.180  0.1636    94.332  445.78
  77   0.5535     85.020  0.1609    94.294  451.58
  78   0.5692     85.100  0.1589    94.476  457.39
  79   0.5591     84.720  0.1547    94.530  463.21
  80   0.5668     84.820  0.1540    94.670  469.16
  81   0.5865     84.130  0.1475    94.788  474.96
  82   0.5639     85.430  0.1532    94.582  480.76
  83   0.5694     84.860  0.1384    95.076  486.58
  84   0.5966     84.090  0.1432    95.064  492.37
  85   0.6060     85.060  0.1350    95.270  498.19
