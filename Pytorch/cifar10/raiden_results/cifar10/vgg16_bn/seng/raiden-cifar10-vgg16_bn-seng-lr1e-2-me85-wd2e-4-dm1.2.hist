Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3687     49.550  1.6860    37.474  7.79
   2   1.0567     62.900  1.2008    56.902  13.59
   3   0.8877     67.530  0.9832    65.540  19.43
   4   0.7709     72.540  0.8444    70.766  25.25
   5   0.7199     75.340  0.7496    74.216  31.06
   6   0.6576     77.700  0.6785    76.808  36.87
   7   0.6026     79.810  0.6232    78.788  42.82
   8   0.5787     80.200  0.5803    80.298  48.64
   9   0.5422     81.820  0.5350    81.936  54.46
  10   0.5250     82.310  0.5029    82.978  60.28
  11   0.5123     82.390  0.4668    84.310  66.12
  12   0.5145     82.810  0.4391    84.992  71.95
  13   0.4745     84.280  0.4194    85.822  77.86
  14   0.4742     83.940  0.3873    86.766  83.68
  15   0.4663     84.430  0.3801    87.224  89.50
  16   0.4353     85.330  0.3522    88.024  95.31
  17   0.4664     84.080  0.3384    88.514  101.13
  18   0.4498     84.740  0.3160    89.284  106.95
  19   0.4639     84.560  0.3044    89.566  112.76
  20   0.4215     86.410  0.2945    90.038  118.68
  21   0.4338     85.290  0.2792    90.522  124.50
  22   0.4467     85.180  0.2678    90.878  130.33
  23   0.4150     86.390  0.2509    91.390  136.12
  24   0.4329     85.570  0.2431    91.842  141.92
  25   0.3941     87.080  0.2323    91.998  147.71
  26   0.3934     87.080  0.2231    92.306  153.69
  27   0.4270     87.110  0.2086    92.840  159.49
  28   0.4047     87.230  0.1961    93.434  165.30
  29   0.4245     86.500  0.1910    93.650  171.11
  30   0.4178     86.840  0.1801    93.906  176.93
  31   0.4439     86.540  0.1692    94.184  182.74
  32   0.4142     87.170  0.1660    94.316  188.57
  33   0.3910     87.760  0.1569    94.592  194.54
  34   0.3948     87.520  0.1515    94.844  200.33
  35   0.3920     87.650  0.1417    95.194  206.15
  36   0.3941     88.070  0.1361    95.288  212.00
  37   0.4022     87.730  0.1304    95.498  217.79
  38   0.4107     88.080  0.1193    95.980  223.59
  39   0.3965     88.010  0.1193    95.916  229.44
  40   0.4121     87.840  0.1121    96.118  235.36
  41   0.4510     87.780  0.1035    96.474  241.16
  42   0.4262     88.140  0.1000    96.562  246.97
  43   0.4267     88.150  0.0917    96.884  252.77
  44   0.4089     88.750  0.0872    97.060  258.60
  45   0.4583     87.720  0.0855    97.054  264.41
  46   0.4194     88.760  0.0803    97.262  270.22
  47   0.4445     88.600  0.0724    97.538  276.06
  48   0.4431     88.780  0.0711    97.596  281.87
  49   0.4475     88.280  0.0676    97.704  287.68
  50   0.4814     88.130  0.0635    97.768  293.44
  51   0.4619     88.260  0.0595    97.910  299.24
  52   0.4444     88.800  0.0537    98.120  305.05
  53   0.4492     88.780  0.0513    98.218  311.01
  54   0.4746     88.310  0.0508    98.228  316.84
  55   0.4743     89.040  0.0451    98.402  322.65
  56   0.4707     88.810  0.0442    98.474  328.46
  57   0.4727     89.070  0.0415    98.586  334.31
  58   0.4756     89.290  0.0418    98.556  340.14
  59   0.4618     89.470  0.0366    98.698  345.99
  60   0.4670     89.470  0.0327    98.890  351.93
  61   0.4782     89.460  0.0304    98.944  357.73
  62   0.4706     89.400  0.0320    98.878  363.55
  63   0.4919     89.610  0.0273    99.066  369.35
  64   0.4877     89.540  0.0260    99.140  375.18
  65   0.4878     89.720  0.0227    99.232  381.01
  66   0.5002     89.630  0.0218    99.242  386.84
  67   0.5143     89.530  0.0198    99.350  392.83
  68   0.5179     89.620  0.0182    99.376  398.63
  69   0.5243     89.650  0.0187    99.390  404.47
  70   0.5255     89.480  0.0173    99.424  410.27
  71   0.5316     89.640  0.0166    99.440  416.06
  72   0.5226     89.860  0.0136    99.530  421.90
  73   0.5295     89.820  0.0148    99.508  427.72
  74   0.5370     89.910  0.0138    99.500  433.66
  75   0.5312     89.890  0.0144    99.504  439.49
  76   0.5433     89.810  0.0123    99.588  445.31
  77   0.5444     89.800  0.0122    99.628  451.12
  78   0.5396     89.870  0.0102    99.654  456.93
  79   0.5437     89.880  0.0109    99.618  462.71
  80   0.5537     90.070  0.0107    99.648  468.61
  81   0.5633     89.590  0.0101    99.656  474.43
  82   0.5538     89.620  0.0103    99.680  480.25
  83   0.5619     89.580  0.0091    99.702  486.06
  84   0.5550     90.000  0.0094    99.682  491.90
  85   0.5649     89.970  0.0083    99.744  497.71
