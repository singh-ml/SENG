Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5609     41.140  1.9382    27.148  7.64
   2   1.3442     50.490  1.5075    43.758  13.57
   3   1.1814     56.850  1.3291    51.198  19.34
   4   1.0695     61.670  1.2009    56.258  25.11
   5   0.9912     64.060  1.0997    60.122  30.88
   6   0.9495     65.760  1.0282    63.200  36.67
   7   0.8845     68.050  0.9627    65.538  42.45
   8   0.8463     69.470  0.9088    67.528  48.24
   9   0.8236     70.590  0.8590    69.404  54.16
  10   0.7768     72.280  0.8174    71.072  59.93
  11   0.7598     72.780  0.7809    72.536  65.71
  12   0.7359     74.070  0.7462    73.798  71.49
  13   0.7225     74.520  0.7128    75.040  77.29
  14   0.6930     75.360  0.6882    75.760  83.10
  15   0.6636     76.700  0.6628    76.696  89.01
  16   0.6599     76.870  0.6375    77.786  94.79
  17   0.6320     77.810  0.6171    78.442  100.57
  18   0.6266     77.870  0.5987    79.076  106.36
  19   0.6314     78.090  0.5760    80.092  112.17
  20   0.6176     78.520  0.5579    80.534  117.95
  21   0.5950     79.350  0.5444    81.108  123.74
  22   0.5996     79.340  0.5238    81.858  129.66
  23   0.5869     79.720  0.5135    82.130  135.44
  24   0.5651     80.210  0.4986    82.602  141.23
  25   0.5633     80.280  0.4823    83.278  147.02
  26   0.5611     80.930  0.4695    83.590  152.83
  27   0.5515     80.380  0.4573    84.118  158.61
  28   0.5501     80.800  0.4409    84.406  164.55
  29   0.5414     81.420  0.4370    84.658  170.34
  30   0.5445     81.100  0.4236    85.248  176.13
  31   0.5311     81.800  0.4117    85.604  181.93
  32   0.5347     81.860  0.4009    85.950  187.74
  33   0.5289     82.220  0.3922    86.306  193.52
  34   0.5226     82.870  0.3752    86.810  199.34
  35   0.5376     82.130  0.3748    87.008  205.24
  36   0.5319     82.100  0.3688    87.208  211.03
  37   0.5375     82.050  0.3520    87.874  216.83
  38   0.5206     82.510  0.3462    87.950  222.63
  39   0.5164     82.760  0.3434    88.006  228.47
  40   0.5224     82.610  0.3380    88.176  234.26
  41   0.5245     82.940  0.3248    88.696  240.07
  42   0.5257     83.020  0.3105    89.160  245.99
  43   0.5278     83.000  0.3055    89.386  251.78
  44   0.5080     83.470  0.2987    89.660  257.57
  45   0.5548     82.770  0.2939    89.690  263.38
  46   0.5226     83.420  0.2872    90.030  269.16
  47   0.5165     83.950  0.2794    90.244  274.95
  48   0.5324     82.940  0.2767    90.362  280.92
  49   0.5051     84.180  0.2690    90.704  286.72
  50   0.5034     83.900  0.2592    90.778  292.46
  51   0.5257     83.520  0.2536    91.224  298.28
  52   0.5217     83.340  0.2526    91.190  304.08
  53   0.5446     83.750  0.2508    91.186  309.91
  54   0.5294     83.900  0.2353    91.800  315.71
  55   0.5182     84.210  0.2324    91.778  321.62
  56   0.5218     83.800  0.2255    92.196  327.43
  57   0.5474     84.030  0.2257    92.134  333.21
  58   0.5130     84.650  0.2172    92.374  339.02
  59   0.5091     84.600  0.2085    92.638  344.84
  60   0.5464     83.960  0.2054    92.820  350.62
  61   0.5310     84.530  0.2008    92.904  356.54
  62   0.5331     83.910  0.1991    93.022  362.32
  63   0.5339     84.630  0.1941    93.216  368.12
  64   0.5440     84.450  0.1872    93.550  373.92
  65   0.5508     84.080  0.1848    93.566  379.70
  66   0.5521     84.620  0.1814    93.548  385.47
  67   0.5547     83.930  0.1779    93.776  391.26
  68   0.5612     84.730  0.1729    93.956  397.16
  69   0.5501     84.050  0.1709    94.006  402.94
  70   0.5376     84.480  0.1661    94.104  408.74
  71   0.5460     84.720  0.1616    94.436  414.52
  72   0.5328     84.820  0.1589    94.298  420.32
  73   0.5856     84.330  0.1494    94.782  426.13
  74   0.5783     84.510  0.1515    94.586  432.05
  75   0.5574     85.070  0.1449    95.032  437.82
  76   0.5669     84.510  0.1426    95.000  443.60
  77   0.5603     84.690  0.1399    95.040  449.37
  78   0.5797     84.460  0.1357    95.208  455.16
  79   0.5818     84.520  0.1310    95.432  460.95
  80   0.5719     85.100  0.1278    95.432  466.73
  81   0.6097     84.620  0.1275    95.542  472.54
  82   0.5600     85.130  0.1291    95.538  478.32
  83   0.5710     84.930  0.1191    95.760  484.15
  84   0.6047     84.480  0.1197    95.822  489.94
  85   0.5820     85.010  0.1160    95.946  495.72
