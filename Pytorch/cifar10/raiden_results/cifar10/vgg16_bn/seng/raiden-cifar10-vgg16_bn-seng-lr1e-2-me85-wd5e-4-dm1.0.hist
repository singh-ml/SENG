Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5816     44.810  1.7117    36.012  7.63
   2   0.9862     64.470  1.2105    56.624  13.44
   3   0.8696     68.320  0.9817    65.766  19.41
   4   0.7362     74.430  0.8446    71.120  25.23
   5   0.7291     75.670  0.7423    74.580  31.08
   6   0.6313     78.370  0.6702    77.278  36.89
   7   0.5989     79.590  0.6056    79.496  42.71
   8   0.6078     79.450  0.5640    81.056  48.53
   9   0.5502     81.150  0.5209    82.516  54.43
  10   0.5176     83.040  0.4878    83.426  60.27
  11   0.4900     83.680  0.4485    84.972  66.09
  12   0.4838     83.490  0.4242    85.582  71.90
  13   0.4661     84.110  0.4108    86.272  77.71
  14   0.4486     84.800  0.3794    87.136  83.52
  15   0.4306     85.600  0.3619    87.742  89.37
  16   0.4343     85.110  0.3410    88.524  95.26
  17   0.4371     85.310  0.3211    88.930  101.10
  18   0.4126     85.980  0.3077    89.526  106.91
  19   0.4141     86.290  0.2936    90.100  112.74
  20   0.4028     86.360  0.2826    90.378  118.61
  21   0.4064     86.660  0.2653    90.932  124.44
  22   0.3880     87.590  0.2499    91.492  130.25
  23   0.3840     87.470  0.2433    91.738  136.19
  24   0.4174     86.440  0.2337    92.120  142.02
  25   0.3884     87.100  0.2160    92.602  147.86
  26   0.3752     87.970  0.2052    93.078  153.67
  27   0.3816     87.790  0.1986    93.160  159.52
  28   0.3760     87.910  0.1886    93.692  165.34
  29   0.3899     87.860  0.1803    93.926  171.31
  30   0.4658     85.910  0.1717    94.216  177.14
  31   0.3859     88.130  0.1644    94.442  182.97
  32   0.3882     88.130  0.1611    94.280  188.79
  33   0.3951     88.190  0.1493    94.760  194.61
  34   0.3934     88.360  0.1462    95.004  200.43
  35   0.3769     88.840  0.1416    95.066  206.25
  36   0.3911     88.220  0.1284    95.628  212.24
  37   0.4073     87.810  0.1252    95.736  218.06
  38   0.3689     88.510  0.1280    95.632  223.88
  39   0.3891     88.870  0.1072    96.358  229.73
  40   0.4245     88.100  0.1047    96.332  235.57
  41   0.3929     88.790  0.1022    96.596  241.43
  42   0.3977     89.060  0.0932    96.718  247.25
  43   0.4028     89.070  0.0885    96.956  253.06
  44   0.3908     89.370  0.0893    97.024  258.89
  45   0.4145     88.930  0.0819    97.216  264.72
  46   0.3980     89.450  0.0785    97.422  270.54
  47   0.4127     89.240  0.0708    97.572  276.36
  48   0.4104     88.990  0.0688    97.660  282.32
  49   0.4368     88.920  0.0605    97.966  288.14
  50   0.4097     89.580  0.0628    97.882  293.90
  51   0.4198     89.620  0.0559    98.096  299.73
  52   0.4046     90.010  0.0514    98.230  305.58
  53   0.4387     89.670  0.0482    98.368  311.39
  54   0.4361     90.000  0.0449    98.508  317.21
  55   0.4370     90.010  0.0466    98.406  323.19
  56   0.4398     89.780  0.0387    98.668  329.03
  57   0.4411     89.800  0.0359    98.782  334.86
  58   0.4493     89.760  0.0341    98.868  340.70
  59   0.4487     89.840  0.0371    98.714  346.53
  60   0.4656     89.810  0.0291    99.032  352.38
  61   0.4538     90.100  0.0280    99.024  358.22
  62   0.4542     90.410  0.0294    98.998  364.16
  63   0.4602     90.040  0.0245    99.158  369.97
  64   0.4685     90.430  0.0219    99.268  375.79
  65   0.4770     90.330  0.0204    99.270  381.64
  66   0.4750     90.060  0.0196    99.356  387.45
  67   0.4841     90.370  0.0189    99.358  393.29
  68   0.4682     90.440  0.0163    99.420  399.14
  69   0.4895     90.400  0.0138    99.500  405.16
  70   0.4993     90.600  0.0133    99.546  410.97
  71   0.4880     90.630  0.0133    99.552  416.79
  72   0.4886     90.570  0.0116    99.622  422.63
  73   0.4966     90.510  0.0133    99.552  428.47
  74   0.4976     90.660  0.0115    99.600  434.30
  75   0.5033     90.680  0.0113    99.624  440.22
  76   0.4924     90.800  0.0092    99.678  446.04
  77   0.4968     90.810  0.0099    99.654  451.88
  78   0.4888     90.790  0.0090    99.712  457.73
  79   0.4966     90.740  0.0076    99.736  463.55
  80   0.5046     90.620  0.0088    99.692  469.36
  81   0.4930     90.900  0.0078    99.766  475.23
  82   0.5065     90.550  0.0072    99.782  481.19
  83   0.5038     90.840  0.0072    99.738  487.02
  84   0.5033     90.910  0.0065    99.790  492.88
  85   0.5009     90.970  0.0068    99.794  498.70
