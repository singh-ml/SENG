Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3400     51.230  1.7213    35.218  7.74
   2   1.0507     61.880  1.2701    53.646  13.52
   3   0.9161     67.610  1.0549    62.348  19.31
   4   0.8347     70.210  0.9266    67.142  25.10
   5   0.7986     71.820  0.8235    71.008  30.92
   6   0.7059     75.480  0.7496    73.760  36.85
   7   0.6860     76.320  0.6907    76.078  42.67
   8   0.6490     77.380  0.6425    77.848  48.50
   9   0.5994     79.140  0.6092    78.824  54.33
  10   0.5917     79.660  0.5670    80.422  60.13
  11   0.5546     81.280  0.5337    81.578  65.95
  12   0.5249     82.350  0.5081    82.506  71.75
  13   0.5452     81.190  0.4830    83.502  77.69
  14   0.5149     82.600  0.4582    84.224  83.51
  15   0.5074     82.740  0.4381    85.136  89.34
  16   0.4789     83.610  0.4226    85.434  95.14
  17   0.4804     83.880  0.4030    86.132  100.96
  18   0.4607     84.020  0.3819    86.888  106.79
  19   0.4701     83.910  0.3648    87.404  112.58
  20   0.4651     84.650  0.3528    87.876  118.52
  21   0.4601     84.530  0.3419    88.228  124.31
  22   0.4853     84.350  0.3215    88.908  130.14
  23   0.4606     84.660  0.3097    89.376  135.94
  24   0.4378     85.340  0.3004    89.676  141.73
  25   0.4447     85.460  0.2855    90.194  147.53
  26   0.4463     85.310  0.2739    90.392  153.48
  27   0.4347     85.540  0.2610    91.054  159.29
  28   0.4388     85.540  0.2562    91.180  165.11
  29   0.4252     86.220  0.2456    91.536  170.94
  30   0.4330     85.830  0.2382    91.772  176.74
  31   0.4606     85.470  0.2237    92.408  182.53
  32   0.4349     85.900  0.2134    92.698  188.32
  33   0.4416     86.160  0.2045    92.920  194.23
  34   0.4560     85.860  0.1977    93.230  200.04
  35   0.4321     86.390  0.1975    93.148  205.88
  36   0.4230     86.730  0.1870    93.550  211.71
  37   0.4114     87.160  0.1750    93.868  217.53
  38   0.4575     86.320  0.1709    94.058  223.33
  39   0.4475     86.670  0.1650    94.278  229.13
  40   0.4450     86.290  0.1603    94.390  235.09
  41   0.4382     87.060  0.1544    94.662  240.90
  42   0.4377     86.980  0.1443    94.958  246.73
  43   0.4474     87.120  0.1381    95.282  252.51
  44   0.4713     86.650  0.1328    95.386  258.35
  45   0.4722     86.960  0.1299    95.448  264.19
  46   0.4698     86.790  0.1216    95.702  270.15
  47   0.4677     87.260  0.1198    95.874  275.99
  48   0.4515     87.350  0.1143    95.978  281.79
  49   0.4822     86.880  0.1075    96.196  287.58
  50   0.4646     87.360  0.1067    96.268  293.33
  51   0.4777     87.140  0.0992    96.524  299.14
  52   0.4929     87.200  0.0918    96.910  304.98
  53   0.5161     86.970  0.0911    96.852  310.91
  54   0.4968     87.680  0.0864    96.976  316.75
  55   0.4850     87.570  0.0834    97.162  322.58
  56   0.5043     87.260  0.0805    97.226  328.39
  57   0.4879     87.830  0.0755    97.428  334.21
  58   0.4908     87.790  0.0722    97.578  340.03
  59   0.5042     87.640  0.0693    97.604  345.85
  60   0.5319     87.910  0.0621    97.886  351.80
  61   0.5264     87.280  0.0650    97.778  357.62
  62   0.5185     87.680  0.0606    97.894  363.41
  63   0.5404     87.310  0.0619    97.882  369.24
  64   0.5281     87.910  0.0531    98.142  375.06
  65   0.5432     87.630  0.0542    98.190  380.89
  66   0.5320     87.910  0.0518    98.186  386.83
  67   0.5373     88.160  0.0491    98.314  392.63
  68   0.5299     88.090  0.0515    98.270  398.43
  69   0.5472     88.070  0.0423    98.530  404.24
  70   0.5644     87.680  0.0433    98.496  410.05
  71   0.5848     87.970  0.0431    98.556  415.88
  72   0.5746     88.090  0.0404    98.572  421.75
  73   0.5771     87.970  0.0378    98.718  427.59
  74   0.5674     87.800  0.0379    98.702  433.42
  75   0.5860     88.140  0.0341    98.844  439.25
  76   0.5967     87.680  0.0353    98.808  445.07
  77   0.5932     88.250  0.0340    98.866  450.88
  78   0.5923     87.800  0.0332    98.864  456.71
  79   0.5888     87.890  0.0333    98.876  462.66
  80   0.5925     88.010  0.0314    98.902  468.46
  81   0.6204     87.800  0.0296    99.034  474.27
  82   0.6076     88.040  0.0297    99.034  480.09
  83   0.5992     88.070  0.0301    98.962  485.93
  84   0.6107     87.870  0.0301    98.996  491.76
  85   0.6075     88.050  0.0282    99.028  497.61
