Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3753     48.920  1.7096    36.110  7.88
   2   1.0610     62.060  1.2454    54.622  13.67
   3   0.8882     68.460  1.0443    62.600  19.46
   4   0.8535     69.490  0.9209    67.306  25.25
   5   0.7427     73.960  0.8239    71.144  31.04
   6   0.7008     75.900  0.7447    74.096  36.82
   7   0.6307     77.990  0.6764    76.370  42.74
   8   0.6392     77.620  0.6343    78.148  48.53
   9   0.5900     79.680  0.5876    79.716  54.32
  10   0.5832     79.870  0.5494    80.936  60.15
  11   0.5575     81.050  0.5248    81.844  65.96
  12   0.5250     81.960  0.4927    83.086  71.75
  13   0.5363     81.730  0.4676    83.758  77.60
  14   0.4984     82.990  0.4428    84.966  83.40
  15   0.5156     82.220  0.4249    85.322  89.19
  16   0.4851     83.680  0.4008    86.314  95.02
  17   0.4684     84.030  0.3848    86.898  100.82
  18   0.4542     84.610  0.3660    87.364  106.63
  19   0.4714     84.140  0.3479    88.118  112.43
  20   0.4540     84.590  0.3324    88.486  118.36
  21   0.4693     84.500  0.3188    89.102  124.16
  22   0.4520     84.930  0.3115    89.316  129.95
  23   0.4713     84.030  0.3012    89.592  135.75
  24   0.4265     85.530  0.2764    90.508  141.53
  25   0.4299     85.730  0.2666    90.812  147.33
  26   0.4352     85.260  0.2538    91.382  153.25
  27   0.4361     85.530  0.2479    91.502  159.08
  28   0.4171     86.410  0.2386    91.798  164.86
  29   0.4350     85.560  0.2266    92.128  170.66
  30   0.4147     86.740  0.2234    92.270  176.46
  31   0.4440     85.600  0.2094    92.670  182.27
  32   0.4447     86.030  0.2004    93.064  188.07
  33   0.4306     86.220  0.1913    93.300  194.02
  34   0.4319     86.300  0.1798    93.846  199.81
  35   0.4221     87.280  0.1800    93.686  205.60
  36   0.4196     87.080  0.1647    94.314  211.40
  37   0.4478     86.330  0.1609    94.476  217.22
  38   0.4543     86.240  0.1508    94.734  223.01
  39   0.4454     86.400  0.1479    94.902  228.94
  40   0.4319     87.100  0.1407    95.100  234.75
  41   0.4560     86.790  0.1347    95.326  240.57
  42   0.4514     87.020  0.1282    95.626  246.37
  43   0.4433     87.600  0.1241    95.690  252.17
  44   0.4476     86.990  0.1203    95.780  257.96
  45   0.4417     87.420  0.1096    96.240  263.83
  46   0.4228     88.000  0.1039    96.398  269.63
  47   0.4553     87.380  0.1003    96.634  275.46
  48   0.4722     87.030  0.0983    96.550  281.24
  49   0.4896     86.870  0.0941    96.710  287.02
  50   0.4805     87.630  0.0874    96.968  292.73
  51   0.4782     87.450  0.0864    97.016  298.54
  52   0.4757     87.570  0.0816    97.218  304.47
  53   0.4790     87.780  0.0758    97.472  310.26
  54   0.4947     87.850  0.0707    97.522  316.08
  55   0.4650     88.380  0.0721    97.422  321.88
  56   0.4523     88.200  0.0675    97.742  327.69
  57   0.5176     87.140  0.0602    97.952  333.48
  58   0.4855     88.350  0.0579    98.042  339.30
  59   0.4995     87.820  0.0593    97.970  345.21
  60   0.4907     88.150  0.0534    98.178  351.00
  61   0.4810     88.710  0.0513    98.170  356.83
  62   0.5217     88.400  0.0474    98.436  362.65
  63   0.5046     88.520  0.0455    98.448  368.46
  64   0.5334     88.300  0.0411    98.578  374.24
  65   0.5332     88.420  0.0409    98.624  380.16
  66   0.5247     88.440  0.0394    98.690  385.98
  67   0.5324     88.310  0.0365    98.712  391.79
  68   0.5321     88.650  0.0350    98.818  397.57
  69   0.5507     88.520  0.0335    98.892  403.36
  70   0.5539     88.250  0.0328    98.896  409.17
  71   0.5579     88.390  0.0330    98.828  414.96
  72   0.5402     88.440  0.0332    98.866  420.93
  73   0.5304     88.790  0.0302    98.974  426.74
  74   0.5394     88.920  0.0282    99.060  432.57
  75   0.5594     88.840  0.0245    99.196  438.36
  76   0.5487     88.750  0.0250    99.158  444.18
  77   0.5822     88.270  0.0241    99.178  449.98
  78   0.5582     88.740  0.0247    99.152  455.77
  79   0.5848     88.630  0.0234    99.176  461.71
  80   0.5599     88.940  0.0211    99.288  467.52
  81   0.5779     88.560  0.0206    99.284  473.34
  82   0.5879     88.880  0.0200    99.326  479.12
  83   0.5742     88.950  0.0220    99.256  484.93
  84   0.5708     88.720  0.0210    99.278  490.74
  85   0.5750     88.720  0.0215    99.258  496.55
