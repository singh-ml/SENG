Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3383     50.900  1.7140    35.846  7.61
   2   1.0260     62.980  1.2393    55.094  13.39
   3   0.9253     66.790  1.0419    62.724  19.20
   4   0.8228     70.770  0.9095    67.700  25.01
   5   0.7330     73.990  0.8168    71.310  30.84
   6   0.7033     75.390  0.7387    74.178  36.80
   7   0.6424     77.290  0.6822    76.408  42.62
   8   0.6378     78.150  0.6448    77.658  48.44
   9   0.5957     80.200  0.5932    79.444  54.22
  10   0.5663     80.440  0.5617    80.630  60.04
  11   0.5366     81.450  0.5330    81.428  65.83
  12   0.5365     81.480  0.5029    82.786  71.76
  13   0.5234     82.250  0.4780    83.662  77.59
  14   0.5043     82.740  0.4543    84.404  83.41
  15   0.5129     82.530  0.4302    85.158  89.21
  16   0.4938     82.990  0.4126    85.884  95.01
  17   0.4952     83.320  0.3942    86.582  100.81
  18   0.4778     83.610  0.3725    87.318  106.77
  19   0.4656     84.300  0.3665    87.374  112.56
  20   0.4767     84.240  0.3433    88.344  118.36
  21   0.4500     84.800  0.3295    88.642  124.18
  22   0.4643     84.490  0.3180    89.050  129.96
  23   0.4588     84.850  0.2993    89.640  135.77
  24   0.4477     85.110  0.2863    90.204  141.56
  25   0.4305     85.910  0.2814    90.318  147.49
  26   0.4516     85.570  0.2699    90.658  153.29
  27   0.4399     85.790  0.2589    91.204  159.09
  28   0.4330     85.680  0.2479    91.610  164.89
  29   0.4241     86.240  0.2357    91.762  170.70
  30   0.4305     86.290  0.2276    92.088  176.49
  31   0.4347     86.500  0.2180    92.608  182.29
  32   0.4453     86.310  0.2058    92.858  188.21
  33   0.4381     86.230  0.2061    92.936  193.99
  34   0.4414     86.280  0.1932    93.288  199.81
  35   0.4373     86.290  0.1884    93.524  205.63
  36   0.4382     86.680  0.1784    93.860  211.42
  37   0.4744     86.160  0.1735    94.008  217.22
  38   0.4484     87.020  0.1641    94.304  223.14
  39   0.4569     86.500  0.1541    94.676  228.93
  40   0.4606     86.580  0.1511    94.770  234.74
  41   0.4592     86.480  0.1490    94.788  240.55
  42   0.4627     86.910  0.1378    95.308  246.37
  43   0.4600     86.730  0.1371    95.278  252.17
  44   0.4584     87.140  0.1284    95.624  257.96
  45   0.4581     87.350  0.1184    95.874  263.90
  46   0.4935     86.690  0.1198    95.790  269.67
  47   0.4648     87.270  0.1120    96.160  275.48
  48   0.4724     87.930  0.1052    96.312  281.29
  49   0.4727     87.520  0.1056    96.378  287.06
  50   0.4621     87.650  0.0958    96.706  292.78
  51   0.4673     87.760  0.0958    96.698  298.72
  52   0.4751     87.920  0.0848    97.068  304.51
  53   0.4817     87.630  0.0895    96.868  310.32
  54   0.5000     87.650  0.0787    97.322  316.12
  55   0.4862     87.510  0.0811    97.248  321.91
  56   0.4946     88.150  0.0747    97.352  327.75
  57   0.4870     87.710  0.0693    97.582  333.60
  58   0.4924     88.220  0.0670    97.706  339.55
  59   0.5081     87.790  0.0650    97.752  345.37
  60   0.5119     87.950  0.0606    97.892  351.17
  61   0.5129     87.810  0.0536    98.152  356.97
  62   0.5051     88.370  0.0495    98.338  362.77
  63   0.5151     88.170  0.0533    98.230  368.56
  64   0.5185     88.300  0.0512    98.234  374.54
  65   0.5273     88.120  0.0464    98.412  380.33
  66   0.5149     88.270  0.0458    98.436  386.13
  67   0.5250     88.480  0.0413    98.526  391.96
  68   0.5361     88.410  0.0446    98.464  397.77
  69   0.5510     88.140  0.0352    98.782  403.58
  70   0.5570     88.210  0.0367    98.722  409.37
  71   0.5336     88.670  0.0369    98.768  415.35
  72   0.5537     88.600  0.0327    98.952  421.17
  73   0.5721     88.460  0.0322    98.912  426.97
  74   0.5774     88.470  0.0320    98.882  432.76
  75   0.5676     88.520  0.0293    99.014  438.56
  76   0.5681     88.300  0.0287    99.038  444.35
  77   0.5832     88.700  0.0271    99.074  450.33
  78   0.5649     88.840  0.0298    99.018  456.12
  79   0.5624     88.910  0.0261    99.092  461.93
  80   0.5593     88.990  0.0252    99.120  467.71
  81   0.5713     88.640  0.0251    99.164  473.53
  82   0.5771     88.700  0.0257    99.100  479.35
  83   0.5812     88.860  0.0220    99.252  485.17
  84   0.5957     88.720  0.0229    99.272  491.12
  85   0.5852     88.840  0.0227    99.238  496.91
