Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3133     51.730  1.7194    35.822  7.86
   2   1.0443     62.430  1.2595    54.324  13.68
   3   0.9235     66.750  1.0582    62.404  19.50
   4   0.8148     70.780  0.9206    67.506  25.32
   5   0.7637     73.130  0.8243    70.998  31.14
   6   0.6852     76.470  0.7435    74.240  36.98
   7   0.6260     78.120  0.6895    76.200  42.81
   8   0.6293     78.510  0.6343    78.252  48.75
   9   0.5944     79.980  0.5898    79.570  54.58
  10   0.5665     80.720  0.5528    81.154  60.40
  11   0.5251     81.810  0.5171    82.406  66.22
  12   0.5200     82.150  0.4941    82.980  72.05
  13   0.5307     81.940  0.4691    83.726  77.84
  14   0.5130     82.550  0.4431    84.792  83.68
  15   0.4808     83.600  0.4232    85.452  89.64
  16   0.4830     83.760  0.4014    86.238  95.48
  17   0.4809     83.730  0.3823    87.040  101.29
  18   0.4698     83.910  0.3682    87.348  107.10
  19   0.4561     84.510  0.3478    87.920  112.89
  20   0.4722     84.100  0.3398    88.272  118.73
  21   0.4568     84.860  0.3265    88.894  124.54
  22   0.4282     85.760  0.3049    89.590  130.49
  23   0.4681     84.620  0.2941    89.932  136.31
  24   0.4376     85.480  0.2837    90.354  142.13
  25   0.4416     85.240  0.2698    90.706  147.94
  26   0.4349     85.510  0.2620    90.994  153.78
  27   0.4326     85.530  0.2534    91.274  159.61
  28   0.4366     85.470  0.2375    91.800  165.44
  29   0.4469     85.470  0.2270    92.076  171.40
  30   0.4189     86.390  0.2205    92.428  177.24
  31   0.4125     86.680  0.2101    92.720  183.08
  32   0.4146     86.790  0.1992    93.164  188.92
  33   0.4183     86.710  0.1933    93.302  194.72
  34   0.4141     86.710  0.1813    93.800  200.53
  35   0.4226     86.390  0.1759    93.972  206.49
  36   0.4200     86.680  0.1680    94.216  212.33
  37   0.4380     86.570  0.1643    94.374  218.18
  38   0.4056     86.930  0.1546    94.700  223.99
  39   0.4149     87.280  0.1491    94.940  229.80
  40   0.4784     86.060  0.1403    95.156  235.61
  41   0.4110     87.560  0.1389    95.168  241.48
  42   0.4270     87.400  0.1284    95.614  247.32
  43   0.4324     87.580  0.1205    95.834  253.16
  44   0.4414     87.460  0.1241    95.662  258.98
  45   0.4323     87.410  0.1172    95.974  264.81
  46   0.4381     87.640  0.1094    96.166  270.62
  47   0.4557     87.060  0.1082    96.238  276.43
  48   0.4768     86.900  0.1025    96.430  282.38
  49   0.4633     87.440  0.0960    96.666  288.19
  50   0.4633     87.740  0.0901    96.880  293.94
  51   0.4607     87.740  0.0863    96.988  299.77
  52   0.4772     87.880  0.0842    97.036  305.61
  53   0.4765     87.920  0.0750    97.380  311.43
  54   0.4729     88.180  0.0731    97.508  317.37
  55   0.4737     88.150  0.0711    97.590  323.19
  56   0.4875     87.910  0.0672    97.676  329.03
  57   0.5091     87.940  0.0655    97.730  334.84
  58   0.4985     87.700  0.0690    97.666  340.65
  59   0.4890     87.930  0.0603    97.922  346.48
  60   0.4986     88.140  0.0534    98.206  352.30
  61   0.5055     88.260  0.0533    98.156  358.27
  62   0.5268     88.060  0.0493    98.274  364.09
  63   0.5335     88.280  0.0484    98.346  369.93
  64   0.5321     87.920  0.0477    98.388  375.78
  65   0.5352     87.960  0.0429    98.568  381.58
  66   0.5156     88.370  0.0429    98.538  387.38
  67   0.5195     88.470  0.0409    98.612  393.19
  68   0.5279     88.360  0.0372    98.704  399.15
  69   0.5291     88.350  0.0379    98.700  404.96
  70   0.5381     88.530  0.0355    98.784  410.77
  71   0.5570     88.370  0.0347    98.798  416.60
  72   0.5439     88.350  0.0342    98.886  422.42
  73   0.5571     88.380  0.0327    98.886  428.23
  74   0.5458     88.530  0.0317    98.884  434.17
  75   0.5579     88.360  0.0275    99.054  440.00
  76   0.5592     88.550  0.0275    99.102  445.84
  77   0.5652     88.390  0.0267    99.092  451.64
  78   0.5615     88.590  0.0265    99.026  457.47
  79   0.5790     88.430  0.0269    99.062  463.28
  80   0.5659     88.630  0.0224    99.226  469.23
  81   0.5720     88.830  0.0219    99.238  475.09
  82   0.5738     88.800  0.0244    99.172  480.91
  83   0.5838     88.520  0.0214    99.246  486.71
  84   0.5825     88.710  0.0222    99.250  492.53
  85   0.5794     88.490  0.0216    99.254  498.34
