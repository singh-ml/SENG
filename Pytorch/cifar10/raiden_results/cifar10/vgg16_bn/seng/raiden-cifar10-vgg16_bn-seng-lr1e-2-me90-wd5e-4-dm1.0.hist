Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'vgg16_bn', '--lr-decay-epoch', '90', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7544058368 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3111     51.710  1.6751    37.624  7.67
   2   1.0241     64.120  1.1667    58.484  13.48
   3   0.8200     71.150  0.9608    66.660  19.31
   4   0.7834     72.710  0.8244    71.822  25.28
   5   0.6773     77.140  0.7336    75.138  31.09
   6   0.6667     77.380  0.6619    77.536  36.90
   7   0.6064     79.030  0.6022    79.580  42.74
   8   0.6035     79.020  0.5555    81.200  48.59
   9   0.5262     82.310  0.5189    82.568  54.43
  10   0.4987     82.470  0.4858    83.756  60.37
  11   0.5051     83.050  0.4535    84.738  66.17
  12   0.4531     84.740  0.4343    85.360  71.98
  13   0.4858     83.520  0.3998    86.538  77.82
  14   0.4475     84.310  0.3776    87.326  83.61
  15   0.4454     85.190  0.3636    87.888  89.40
  16   0.4323     85.830  0.3376    88.388  95.36
  17   0.4257     86.130  0.3245    89.096  101.17
  18   0.4128     85.790  0.3066    89.690  107.02
  19   0.4015     86.310  0.2909    90.282  112.81
  20   0.4534     84.920  0.2786    90.532  118.64
  21   0.4282     85.940  0.2661    90.980  124.47
  22   0.3865     87.140  0.2513    91.456  130.27
  23   0.3920     87.060  0.2385    91.982  136.23
  24   0.3874     86.980  0.2285    92.226  142.02
  25   0.3854     87.700  0.2205    92.510  147.82
  26   0.3745     87.460  0.2097    92.942  153.64
  27   0.4080     87.020  0.1981    93.304  159.43
  28   0.3819     87.380  0.1910    93.462  165.24
  29   0.4038     87.420  0.1807    93.792  171.03
  30   0.4027     87.130  0.1718    94.196  176.85
  31   0.3728     88.260  0.1632    94.440  182.64
  32   0.3615     88.340  0.1582    94.610  188.43
  33   0.3749     88.290  0.1517    94.724  194.24
  34   0.4177     87.800  0.1427    95.204  200.03
  35   0.3727     88.450  0.1366    95.334  205.83
  36   0.4051     87.820  0.1309    95.544  211.77
  37   0.4182     87.370  0.1234    95.798  217.58
  38   0.3625     88.610  0.1220    95.758  223.39
  39   0.3954     88.680  0.1126    96.112  229.18
  40   0.3905     88.740  0.1113    96.140  234.97
  41   0.4116     88.420  0.1017    96.452  240.79
  42   0.4159     88.430  0.0922    96.896  246.80
  43   0.3919     89.170  0.0923    96.826  252.63
  44   0.3859     89.040  0.0886    96.928  258.44
  45   0.4546     87.850  0.0825    97.140  264.27
  46   0.4364     88.140  0.0802    97.252  270.07
  47   0.4193     88.430  0.0777    97.362  275.89
  48   0.4008     89.340  0.0739    97.472  281.83
  49   0.4074     89.690  0.0671    97.726  287.63
  50   0.4126     89.560  0.0615    97.932  293.38
  51   0.3886     89.880  0.0585    97.948  299.19
  52   0.4294     89.120  0.0568    98.054  305.04
  53   0.4358     89.250  0.0554    98.112  310.89
  54   0.4458     89.040  0.0506    98.282  316.69
  55   0.3880     90.040  0.0446    98.412  322.65
  56   0.4165     89.620  0.0457    98.414  328.46
  57   0.4273     89.600  0.0404    98.614  334.28
  58   0.4189     89.980  0.0378    98.668  340.09
  59   0.4378     89.820  0.0374    98.714  345.92
  60   0.4157     90.150  0.0330    98.882  351.72
  61   0.4380     90.120  0.0311    98.940  357.57
  62   0.4585     89.870  0.0285    99.018  363.54
  63   0.4523     90.160  0.0266    99.086  369.34
  64   0.4646     89.830  0.0272    99.072  375.17
  65   0.4589     90.310  0.0233    99.268  380.97
  66   0.4412     90.710  0.0219    99.252  386.80
  67   0.4553     90.180  0.0216    99.260  392.60
  68   0.4606     90.510  0.0167    99.436  398.40
  69   0.4568     90.350  0.0168    99.464  404.25
  70   0.4558     90.670  0.0162    99.450  410.06
  71   0.4657     90.510  0.0151    99.522  415.88
  72   0.4635     90.510  0.0160    99.444  421.70
  73   0.4666     90.370  0.0135    99.568  427.54
  74   0.4594     90.780  0.0117    99.616  433.35
  75   0.4606     90.610  0.0112    99.622  439.32
  76   0.4686     90.720  0.0096    99.682  445.17
  77   0.4780     90.580  0.0102    99.674  451.00
  78   0.4693     90.730  0.0095    99.688  456.81
  79   0.4878     90.620  0.0098    99.696  462.63
  80   0.4761     90.650  0.0066    99.762  468.45
  81   0.4926     90.810  0.0069    99.772  474.41
  82   0.4823     90.690  0.0068    99.798  480.22
  83   0.4828     90.830  0.0066    99.800  486.03
  84   0.4912     90.910  0.0061    99.820  491.87
  85   0.4866     90.920  0.0059    99.806  497.67
  86   0.4924     90.940  0.0064    99.798  503.46
  87   0.4845     90.900  0.0067    99.784  509.39
  88   0.4863     90.750  0.0053    99.834  515.20
  89   0.4927     90.930  0.0047    99.858  521.01
  90   0.4886     90.960  0.0056    99.810  526.82
