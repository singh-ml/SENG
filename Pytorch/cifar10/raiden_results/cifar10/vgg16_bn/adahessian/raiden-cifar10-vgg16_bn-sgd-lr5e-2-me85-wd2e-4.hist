Use GPU: 0 for training
==> Running with ['main_adahessian.py', '--epoch', '85', '--arch', 'vgg16_bn', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7815981568 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2971     10.540  8.1864    10.388  22.32
   2   2.2975     11.250  2.3095    10.670  42.59
   3   2.2933     11.440  2.3049    10.668  62.82
   4   2.2910     11.600  2.2935    11.312  83.09
   5   2.2861     11.890  2.2908    11.304  103.33
   6   2.2853     11.040  2.2893    11.710  123.62
   7   2.2814     12.170  2.2858    11.958  143.87
   8   2.2521     15.250  2.2810    12.290  164.11
   9   1.8802     23.890  2.0552    19.488  184.37
  10   1.8386     25.290  1.9115    24.434  204.60
  11   1.6868     33.420  1.7384    30.434  224.86
  12   1.4424     43.800  1.5876    38.336  245.10
  13   1.3567     49.470  1.4529    45.478  265.38
  14   1.3500     54.440  1.2904    53.384  285.67
  15   1.1180     61.560  1.1391    60.048  305.91
  16   0.9735     66.600  1.0182    64.716  326.15
  17   0.9840     65.460  0.9390    67.566  346.39
  18   1.0467     64.920  0.8762    70.192  366.64
  19   0.8029     73.000  0.7967    73.218  386.88
  20   0.8061     72.800  0.7480    75.180  407.15
  21   0.7369     75.320  0.7338    75.816  427.40
  22   0.7199     74.770  0.6475    78.602  447.71
  23   0.8996     70.310  0.6453    78.934  468.02
  24   0.6461     78.080  0.5896    80.566  488.28
  25   0.6656     78.290  0.5525    81.890  508.54
  26   0.6232     78.910  0.5850    81.212  528.80
  27   0.5900     80.620  0.5181    82.984  549.05
  28   0.7048     80.190  0.4743    84.536  569.36
  29   0.5105     82.710  0.4432    85.496  589.60
  30   0.5537     81.940  0.4194    86.154  609.86
  31   0.5671     80.660  0.4276    86.204  630.13
  32   0.5233     83.340  0.3981    87.026  650.40
  33   0.4985     83.950  0.3756    87.732  670.64
  34   1.0228     72.050  0.3597    88.216  690.93
  35   0.5139     83.160  0.3561    88.278  711.18
  36   0.4394     85.220  0.3174    89.624  731.45
  37   0.5274     82.580  0.3165    89.612  751.72
  38   0.4739     84.430  0.3022    90.056  771.98
  39   0.4905     84.160  0.2798    90.782  792.26
  40   0.4995     84.860  0.2743    91.164  812.53
  41   0.4733     85.250  0.2565    91.518  832.81
  42   0.4565     85.840  0.2850    90.860  853.06
  43   0.4517     85.600  0.2238    92.706  873.35
  44   0.4139     86.950  0.2104    93.078  893.58
  45   0.4283     86.710  0.2052    93.280  913.90
  46   0.4258     86.630  0.1878    93.826  934.16
  47   0.4187     87.460  0.1825    93.992  954.43
  48   0.4066     87.390  0.1685    94.556  974.68
  49   0.4165     87.640  0.1568    94.792  994.94
  50   0.4204     87.020  0.1535    94.936  1015.24
  51   0.4466     87.240  0.1467    95.128  1035.49
  52   0.4463     86.930  0.1388    95.290  1055.74
  53   0.4527     86.660  0.1300    95.690  1076.01
  54   0.4413     87.240  0.1227    95.934  1096.25
  55   0.5025     87.170  0.1103    96.410  1116.52
  56   0.4463     88.180  0.1517    94.984  1136.76
  57   0.4485     87.970  0.1024    96.656  1157.05
  58   0.4470     88.450  0.0881    97.132  1177.31
  59   0.4519     88.440  0.0815    97.358  1197.58
  60   0.4508     88.600  0.0772    97.438  1217.85
  61   0.4609     88.200  0.0690    97.786  1238.11
  62   0.4816     88.190  0.0643    97.846  1258.37
  63   0.4659     88.860  0.0589    98.126  1278.61
  64   0.4721     88.600  0.0545    98.220  1298.88
  65   0.5011     88.640  0.0544    98.230  1319.12
  66   0.4878     88.730  0.0507    98.376  1339.37
  67   0.4770     88.820  0.0439    98.638  1359.61
  68   0.4977     88.920  0.0417    98.654  1379.85
  69   0.4846     88.860  0.0412    98.676  1400.10
  70   0.5061     88.980  0.0385    98.782  1420.34
  71   0.5086     88.920  0.0355    98.898  1440.62
  72   0.5003     89.140  0.0325    98.972  1460.84
  73   0.5188     88.830  0.0317    98.976  1481.09
  74   0.5104     88.920  0.0312    99.034  1501.36
  75   0.5288     88.960  0.0284    99.092  1521.62
  76   0.5256     89.070  0.0285    99.110  1541.88
  77   0.5244     88.850  0.0260    99.172  1562.13
  78   0.5382     89.270  0.0248    99.210  1582.40
  79   0.5350     88.880  0.0244    99.196  1602.63
  80   0.5436     88.980  0.0249    99.256  1622.93
  81   0.5412     88.950  0.0229    99.264  1643.22
  82   0.5482     88.970  0.0230    99.316  1663.45
  83   0.5472     89.030  0.0225    99.282  1683.73
  84   0.5374     89.110  0.0220    99.288  1703.99
  85   0.5527     88.950  0.0230    99.298  1724.24
