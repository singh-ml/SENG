Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.0732     62.110  1.4109    48.088  8.88
   2   0.7715     72.880  0.9151    67.620  15.93
   3   0.7399     74.530  0.6988    75.488  23.07
   4   0.6331     78.700  0.5961    79.240  30.16
   5   0.5326     82.340  0.5108    82.456  37.21
   6   0.4766     84.270  0.4672    83.874  44.30
   7   0.5227     83.260  0.4205    85.514  51.38
   8   0.4816     84.090  0.3853    86.612  58.50
   9   0.4650     84.810  0.3575    87.592  65.55
  10   0.4303     85.880  0.3318    88.570  72.62
  11   0.4241     86.280  0.3068    89.414  79.70
  12   0.4200     87.070  0.2807    90.200  86.75
  13   0.4511     86.190  0.2579    91.028  93.79
  14   0.4184     87.240  0.2417    91.598  100.98
  15   0.4250     86.330  0.2309    91.908  108.08
  16   0.3469     89.370  0.2178    92.394  115.13
  17   0.4154     86.960  0.1949    93.196  122.18
  18   0.3746     88.230  0.1907    93.298  129.25
  19   0.3554     89.060  0.1750    93.774  136.36
  20   0.3917     88.560  0.1604    94.400  143.48
  21   0.4159     87.840  0.1549    94.544  150.55
  22   0.3433     90.060  0.1464    94.780  157.66
  23   0.3833     89.050  0.1405    94.998  164.70
  24   0.3870     88.970  0.1300    95.366  171.77
  25   0.3572     89.400  0.1166    95.962  178.84
  26   0.3539     90.310  0.1160    95.852  185.90
  27   0.3672     89.830  0.0988    96.648  192.96
  28   0.3836     90.070  0.0970    96.540  200.04
  29   0.3462     90.880  0.0919    96.668  207.18
  30   0.3667     90.520  0.0863    96.992  214.26
  31   0.4137     90.020  0.0773    97.250  221.32
  32   0.3940     90.140  0.0763    97.324  228.41
  33   0.3705     90.950  0.0737    97.326  235.45
  34   0.3430     91.570  0.0637    97.782  242.54
  35   0.3677     90.770  0.0659    97.660  249.61
  36   0.3831     90.420  0.0536    98.076  256.69
  37   0.3647     91.310  0.0517    98.178  263.79
  38   0.3519     91.390  0.0467    98.366  270.92
  39   0.3584     91.650  0.0444    98.458  277.96
  40   0.3487     91.680  0.0381    98.726  285.03
  41   0.3764     91.780  0.0376    98.704  292.08
  42   0.3813     91.320  0.0302    99.006  299.15
  43   0.3682     91.390  0.0282    99.056  306.19
  44   0.3862     91.580  0.0267    99.094  313.32
  45   0.3649     91.990  0.0216    99.272  320.39
  46   0.3820     91.510  0.0187    99.394  327.46
  47   0.3945     91.440  0.0207    99.290  334.54
  48   0.3916     91.490  0.0163    99.500  341.61
  49   0.4058     91.460  0.0129    99.600  348.74
  50   0.3591     92.260  0.0132    99.592  355.74
  51   0.3633     92.440  0.0134    99.558  362.81
  52   0.3622     92.590  0.0090    99.710  369.89
  53   0.3509     92.820  0.0074    99.784  376.93
  54   0.3557     92.930  0.0070    99.798  384.04
  55   0.3633     92.670  0.0050    99.870  391.11
  56   0.3512     92.650  0.0053    99.856  398.18
  57   0.3527     92.770  0.0041    99.890  405.25
  58   0.3568     92.870  0.0032    99.922  412.31
  59   0.3448     92.740  0.0029    99.932  419.38
  60   0.3459     92.950  0.0027    99.952  426.44
  61   0.3451     92.960  0.0026    99.942  433.47
  62   0.3450     93.070  0.0023    99.948  440.51
  63   0.3452     92.900  0.0022    99.960  447.57
  64   0.3420     92.990  0.0020    99.962  454.70
  65   0.3381     93.180  0.0017    99.974  461.77
  66   0.3350     93.110  0.0015    99.984  468.84
  67   0.3376     93.210  0.0015    99.970  475.90
  68   0.3361     93.270  0.0014    99.982  482.94
  69   0.3371     93.190  0.0013    99.980  490.04
  70   0.3433     93.270  0.0012    99.984  497.10
  71   0.3356     93.230  0.0014    99.978  504.18
  72   0.3349     93.200  0.0010    99.994  511.24
  73   0.3346     93.100  0.0011    99.980  518.32
  74   0.3342     93.130  0.0011    99.986  525.45
  75   0.3357     93.160  0.0010    99.998  532.52
  76   0.3325     93.250  0.0009    99.992  539.54
  77   0.3307     93.260  0.0009    99.990  546.61
  78   0.3314     93.180  0.0008    99.994  553.68
  79   0.3305     93.300  0.0009    99.994  560.77
  80   0.3302     93.300  0.0008    99.996  567.90
  81   0.3291     93.300  0.0009    99.988  574.97
  82   0.3306     93.190  0.0010    99.986  582.04
  83   0.3293     93.210  0.0008    99.988  589.11
  84   0.3300     93.210  0.0010    99.986  596.19
  85   0.3289     93.310  0.0008    99.998  603.22
