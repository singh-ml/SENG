Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4094     48.460  1.6229    39.828  8.87
   2   1.0577     62.000  1.1749    57.514  15.82
   3   0.8691     69.400  0.9551    65.890  22.74
   4   0.7737     72.910  0.8161    71.006  29.71
   5   0.7306     74.510  0.7130    75.020  36.68
   6   0.6467     77.790  0.6288    77.980  43.69
   7   0.5769     80.630  0.5703    80.004  50.60
   8   0.5412     81.260  0.5238    81.708  57.52
   9   0.5683     81.430  0.4836    83.106  64.49
  10   0.4905     83.530  0.4481    84.382  71.55
  11   0.4927     83.360  0.4175    85.386  78.53
  12   0.4645     84.660  0.3910    86.394  85.44
  13   0.4708     84.740  0.3625    87.356  92.39
  14   0.4622     85.070  0.3524    87.820  99.33
  15   0.4426     85.480  0.3304    88.578  106.27
  16   0.4514     85.250  0.3137    89.138  113.28
  17   0.4548     85.800  0.2963    89.682  120.20
  18   0.4324     86.300  0.2820    90.136  127.16
  19   0.4234     86.330  0.2644    90.744  134.12
  20   0.4306     86.770  0.2494    91.306  141.19
  21   0.4005     86.860  0.2370    91.642  148.15
  22   0.4195     86.560  0.2306    91.900  155.11
  23   0.4227     87.080  0.2135    92.394  162.04
  24   0.4456     87.100  0.2070    92.672  168.97
  25   0.4054     87.640  0.1947    93.202  175.91
  26   0.4234     87.570  0.1842    93.458  182.92
  27   0.4346     87.540  0.1752    93.984  189.89
  28   0.4396     87.210  0.1645    94.234  196.84
  29   0.3990     88.320  0.1579    94.366  203.74
  30   0.4287     87.670  0.1458    94.908  210.69
  31   0.3965     88.650  0.1385    95.132  217.74
  32   0.4184     88.150  0.1338    95.346  224.69
  33   0.4170     88.150  0.1243    95.608  231.62
  34   0.4377     88.360  0.1201    95.872  238.60
  35   0.4742     87.870  0.1125    96.010  245.58
  36   0.4160     88.800  0.1063    96.342  252.56
  37   0.4178     88.570  0.1008    96.486  259.52
  38   0.4242     88.700  0.0924    96.756  266.51
  39   0.4785     88.050  0.0844    97.144  273.50
  40   0.4152     89.470  0.0790    97.364  280.46
  41   0.4053     89.020  0.0752    97.322  287.48
  42   0.4642     88.940  0.0701    97.576  294.46
  43   0.4339     89.000  0.0692    97.594  301.37
  44   0.4129     89.830  0.0646    97.752  308.30
  45   0.4228     89.380  0.0579    98.064  315.24
  46   0.4106     89.370  0.0537    98.186  322.25
  47   0.4108     89.790  0.0519    98.240  329.23
  48   0.4342     89.510  0.0488    98.376  336.20
  49   0.4160     89.520  0.0417    98.594  343.15
  50   0.3921     90.390  0.0386    98.744  350.08
  51   0.4228     89.930  0.0352    98.916  357.09
  52   0.4495     89.680  0.0325    98.992  364.02
  53   0.4185     89.940  0.0312    98.998  370.98
  54   0.4084     90.600  0.0278    99.168  377.93
  55   0.4216     90.420  0.0239    99.350  384.95
  56   0.4230     90.500  0.0239    99.284  391.90
  57   0.4157     90.720  0.0216    99.378  398.84
  58   0.4231     90.500  0.0206    99.406  405.82
  59   0.4228     90.770  0.0174    99.540  412.76
  60   0.4060     90.820  0.0166    99.594  419.77
  61   0.4223     90.710  0.0147    99.624  426.71
  62   0.4208     90.760  0.0130    99.694  433.70
  63   0.4160     90.760  0.0139    99.660  440.71
  64   0.4074     90.880  0.0118    99.722  447.67
  65   0.4159     90.940  0.0114    99.746  454.68
  66   0.4125     90.890  0.0109    99.728  461.64
  67   0.4130     90.830  0.0095    99.800  468.63
  68   0.4099     91.060  0.0089    99.820  475.64
  69   0.4072     91.020  0.0099    99.784  482.54
  70   0.4186     91.190  0.0080    99.856  489.53
  71   0.4039     91.030  0.0076    99.870  496.48
  72   0.4203     90.890  0.0074    99.874  503.44
  73   0.4044     91.230  0.0068    99.890  510.39
  74   0.4085     91.080  0.0066    99.896  517.41
  75   0.4128     91.120  0.0070    99.886  524.44
  76   0.4112     91.300  0.0063    99.910  531.46
  77   0.4173     91.110  0.0065    99.898  538.39
  78   0.4111     91.230  0.0060    99.916  545.32
  79   0.4232     91.160  0.0058    99.918  552.27
  80   0.4081     91.000  0.0055    99.932  559.23
  81   0.4108     91.200  0.0058    99.914  566.18
  82   0.4107     91.320  0.0052    99.946  573.11
  83   0.4174     91.150  0.0058    99.926  580.04
  84   0.4089     91.250  0.0053    99.924  586.97
  85   0.4124     91.270  0.0050    99.926  594.02
