Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7492985856 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7536     35.030  2.0150    24.930  8.72
   2   1.5511     42.690  1.6808    37.640  15.61
   3   1.4460     46.810  1.5343    43.086  22.52
   4   1.3381     50.690  1.4277    47.492  29.41
   5   1.2597     53.930  1.3289    51.598  36.44
   6   1.1890     56.810  1.2494    54.750  43.34
   7   1.1235     59.480  1.1772    57.420  50.25
   8   1.0570     62.270  1.1187    59.624  57.13
   9   1.0136     63.320  1.0628    61.890  64.03
  10   0.9836     64.670  1.0126    63.766  70.98
  11   0.9398     66.320  0.9681    65.474  77.85
  12   0.9026     67.470  0.9339    66.658  84.77
  13   0.8768     68.260  0.8990    67.842  91.65
  14   0.8570     69.570  0.8664    69.036  98.60
  15   0.8283     70.610  0.8398    69.726  105.54
  16   0.8062     70.960  0.8140    71.106  112.43
  17   0.7933     71.620  0.7942    71.848  119.32
  18   0.7854     72.060  0.7714    72.582  126.19
  19   0.7617     72.870  0.7493    73.400  133.08
  20   0.7563     73.160  0.7287    73.938  140.00
  21   0.7316     73.830  0.7102    74.842  146.99
  22   0.7183     74.840  0.6943    75.524  153.93
  23   0.7054     75.220  0.6723    76.140  160.82
  24   0.6965     75.270  0.6551    76.630  167.75
  25   0.6809     76.170  0.6395    77.390  174.67
  26   0.6679     76.590  0.6237    77.946  181.62
  27   0.6793     76.580  0.6046    78.754  188.51
  28   0.6548     77.370  0.5920    79.066  195.43
  29   0.6360     77.970  0.5753    79.606  202.32
  30   0.6283     78.690  0.5621    80.136  209.20
  31   0.6139     78.940  0.5506    80.478  216.22
  32   0.6155     79.240  0.5371    81.144  223.09
  33   0.5999     79.500  0.5187    81.780  230.01
  34   0.5816     80.140  0.5086    82.212  236.90
  35   0.6015     79.830  0.5002    82.424  243.77
  36   0.5765     80.940  0.4891    83.084  250.71
  37   0.5649     81.180  0.4776    83.172  257.60
  38   0.5625     81.310  0.4679    83.380  264.52
  39   0.5564     81.230  0.4573    83.976  271.45
  40   0.5505     81.880  0.4407    84.494  278.34
  41   0.5424     82.010  0.4358    84.694  285.29
  42   0.5441     82.170  0.4275    85.050  292.17
  43   0.5433     82.090  0.4204    85.264  299.04
  44   0.5352     82.750  0.4127    85.472  305.95
  45   0.5292     82.620  0.4005    85.826  312.84
  46   0.5322     82.470  0.3975    86.014  319.82
  47   0.5084     82.970  0.3874    86.288  326.72
  48   0.5103     83.170  0.3739    86.782  333.58
  49   0.5231     83.140  0.3743    86.694  340.47
  50   0.5061     83.550  0.3601    87.330  347.26
  51   0.5008     83.790  0.3547    87.454  354.30
  52   0.4974     83.800  0.3480    87.790  361.20
  53   0.5018     83.740  0.3397    88.116  368.09
  54   0.4999     83.790  0.3337    88.248  375.01
  55   0.4964     84.030  0.3334    88.332  381.93
  56   0.5077     83.810  0.3265    88.604  388.89
  57   0.4911     84.190  0.3176    88.976  395.80
  58   0.5017     83.780  0.3097    89.216  402.71
  59   0.5046     83.720  0.3049    89.142  409.60
  60   0.4989     84.520  0.2984    89.480  416.58
  61   0.5079     84.040  0.2942    89.650  423.46
  62   0.4939     84.040  0.2864    89.942  430.35
  63   0.4868     84.960  0.2805    90.186  437.20
  64   0.4939     84.880  0.2775    90.248  444.08
  65   0.4868     84.600  0.2721    90.464  450.97
  66   0.4937     84.820  0.2669    90.594  457.90
  67   0.4856     84.930  0.2590    90.956  464.80
  68   0.4875     84.950  0.2605    90.772  471.69
  69   0.4828     85.270  0.2535    91.110  478.62
  70   0.4750     85.300  0.2481    91.300  485.52
  71   0.4905     85.070  0.2407    91.536  492.49
  72   0.4930     85.440  0.2350    91.858  499.36
  73   0.4809     85.190  0.2342    91.726  506.26
  74   0.5075     84.610  0.2322    91.748  513.14
  75   0.5041     84.920  0.2227    92.090  520.02
  76   0.4809     85.830  0.2201    92.226  526.96
  77   0.4969     85.150  0.2140    92.514  533.86
  78   0.5109     85.280  0.2072    92.630  540.77
  79   0.4966     85.380  0.2051    92.884  547.64
  80   0.4817     85.760  0.2040    92.844  554.55
  81   0.4888     85.710  0.1987    93.072  561.52
  82   0.5141     85.330  0.1981    93.020  568.45
  83   0.5041     85.640  0.1938    93.268  575.38
  84   0.5024     85.740  0.1891    93.264  582.30
  85   0.5031     85.570  0.1853    93.548  589.21
