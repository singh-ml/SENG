Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7492985856 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4406     47.600  1.7069    36.030  8.87
   2   1.1455     58.170  1.2806    53.276  15.81
   3   0.9416     66.290  1.0522    62.030  22.79
   4   0.8396     70.000  0.9016    67.984  29.75
   5   0.7625     73.120  0.7974    71.892  36.71
   6   0.6934     75.600  0.7081    75.102  43.80
   7   0.6507     77.720  0.6459    77.230  50.74
   8   0.5978     79.240  0.5979    79.048  57.68
   9   0.5940     79.780  0.5503    80.978  64.64
  10   0.5355     81.610  0.5135    82.164  71.57
  11   0.5670     81.320  0.4794    83.392  78.60
  12   0.4970     82.940  0.4610    83.930  85.52
  13   0.5061     83.190  0.4305    85.134  92.46
  14   0.4824     83.730  0.4148    85.538  99.42
  15   0.4957     83.930  0.3884    86.386  106.38
  16   0.4792     84.620  0.3719    87.056  113.31
  17   0.4578     84.640  0.3524    87.648  120.36
  18   0.4445     85.040  0.3368    88.268  127.31
  19   0.4317     86.100  0.3230    88.652  134.26
  20   0.4342     85.420  0.3060    89.202  141.23
  21   0.4251     86.010  0.2992    89.650  148.17
  22   0.4283     86.200  0.2763    90.378  155.21
  23   0.4335     86.270  0.2675    90.756  162.18
  24   0.4330     85.890  0.2586    90.880  169.16
  25   0.4371     86.250  0.2481    91.352  176.08
  26   0.4344     86.540  0.2388    91.658  183.04
  27   0.4196     86.730  0.2276    92.044  190.08
  28   0.4259     87.110  0.2225    92.142  197.01
  29   0.4216     87.020  0.2049    92.818  203.96
  30   0.4089     87.260  0.2029    92.844  210.90
  31   0.4007     87.440  0.1917    93.354  217.84
  32   0.4040     87.720  0.1845    93.432  224.85
  33   0.4140     87.640  0.1729    93.990  231.82
  34   0.4227     87.560  0.1636    94.288  238.75
  35   0.4290     87.820  0.1585    94.436  245.68
  36   0.4051     87.910  0.1560    94.460  252.61
  37   0.4208     87.970  0.1450    94.882  259.64
  38   0.3949     88.560  0.1432    94.948  266.58
  39   0.4445     87.170  0.1326    95.380  273.53
  40   0.4155     88.150  0.1276    95.554  280.50
  41   0.3977     88.550  0.1181    95.794  287.44
  42   0.4183     88.220  0.1102    96.196  294.44
  43   0.4218     88.360  0.1065    96.278  301.36
  44   0.4338     88.330  0.1025    96.400  308.28
  45   0.4323     89.020  0.1047    96.262  315.23
  46   0.4242     88.790  0.0979    96.656  322.18
  47   0.4327     88.700  0.0881    96.968  329.14
  48   0.4486     88.070  0.0830    97.100  336.18
  49   0.4539     88.420  0.0811    97.138  343.13
  50   0.4213     89.010  0.0821    97.058  349.94
  51   0.4354     88.910  0.0766    97.362  356.86
  52   0.4426     88.940  0.0705    97.546  363.81
  53   0.4502     88.750  0.0649    97.764  370.79
  54   0.4237     89.400  0.0636    97.870  377.73
  55   0.4671     88.710  0.0591    98.028  384.68
  56   0.4431     89.300  0.0570    98.024  391.64
  57   0.4540     89.050  0.0548    98.124  398.57
  58   0.4483     89.320  0.0551    98.094  405.58
  59   0.4450     89.240  0.0489    98.396  412.51
  60   0.4494     89.390  0.0470    98.394  419.47
  61   0.4576     89.300  0.0430    98.608  426.39
  62   0.4808     89.050  0.0415    98.692  433.39
  63   0.4537     89.530  0.0397    98.718  440.34
  64   0.4613     89.410  0.0383    98.778  447.31
  65   0.4698     89.440  0.0340    98.904  454.28
  66   0.4573     89.760  0.0327    98.952  461.21
  67   0.4795     89.540  0.0340    98.880  468.17
  68   0.4546     89.900  0.0291    99.070  475.18
  69   0.4699     89.650  0.0285    99.108  482.11
  70   0.4670     89.310  0.0268    99.124  489.07
  71   0.4825     89.380  0.0245    99.248  496.03
  72   0.4639     89.570  0.0244    99.252  502.99
  73   0.4780     89.570  0.0243    99.294  509.96
  74   0.4795     89.760  0.0233    99.340  516.91
  75   0.4776     89.690  0.0214    99.336  523.87
  76   0.4928     89.540  0.0215    99.354  530.85
  77   0.4840     89.630  0.0204    99.422  537.81
  78   0.4769     89.600  0.0201    99.420  544.83
  79   0.4866     89.830  0.0175    99.472  551.78
  80   0.4759     89.940  0.0181    99.444  558.75
  81   0.4801     89.770  0.0172    99.506  565.68
  82   0.4900     89.760  0.0165    99.530  572.62
  83   0.4784     90.080  0.0173    99.502  579.63
  84   0.4819     89.830  0.0163    99.532  586.56
  85   0.4919     90.030  0.0156    99.584  593.53
  86   0.4946     90.050  0.0157    99.550  600.48
  87   0.4856     90.160  0.0154    99.600  607.41
  88   0.4971     89.670  0.0145    99.608  614.42
  89   0.4895     89.750  0.0145    99.620  621.35
  90   0.4968     89.900  0.0156    99.580  628.33
