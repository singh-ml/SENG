Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7493510144 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2751     54.090  1.5917    41.038  8.70
   2   1.0441     62.930  1.1056    60.308  15.60
   3   0.8252     70.630  0.8889    68.276  22.60
   4   0.7040     75.660  0.7492    73.578  29.53
   5   0.6211     79.040  0.6412    77.520  36.46
   6   0.5925     80.150  0.5766    79.990  43.34
   7   0.5415     81.600  0.5243    81.770  50.21
   8   0.5539     81.360  0.4784    83.432  57.16
   9   0.5197     82.140  0.4502    84.250  64.10
  10   0.5011     83.350  0.4149    85.680  71.00
  11   0.4608     84.320  0.3880    86.486  77.88
  12   0.4748     84.650  0.3571    87.534  84.77
  13   0.4474     85.460  0.3421    88.038  91.67
  14   0.4597     85.130  0.3242    88.702  98.65
  15   0.4412     85.720  0.3048    89.406  105.53
  16   0.4234     86.490  0.2846    90.068  112.43
  17   0.4188     86.500  0.2704    90.570  119.29
  18   0.4036     87.030  0.2602    90.912  126.19
  19   0.4434     86.190  0.2472    91.322  133.19
  20   0.4057     87.210  0.2295    92.110  140.10
  21   0.4066     87.240  0.2220    92.048  147.01
  22   0.4042     87.400  0.2091    92.748  153.94
  23   0.4206     87.000  0.2006    92.924  160.81
  24   0.3960     88.200  0.1867    93.412  167.75
  25   0.4142     87.850  0.1747    93.782  174.64
  26   0.4068     87.570  0.1720    93.916  181.53
  27   0.3897     88.510  0.1565    94.460  188.43
  28   0.4127     87.880  0.1484    94.832  195.30
  29   0.4060     88.110  0.1393    95.144  202.25
  30   0.3911     88.560  0.1368    95.166  209.16
  31   0.4114     88.660  0.1269    95.542  216.05
  32   0.4071     88.890  0.1168    95.908  222.94
  33   0.3760     89.100  0.1186    95.782  229.90
  34   0.4034     89.190  0.1064    96.224  236.89
  35   0.4354     88.290  0.1028    96.384  243.83
  36   0.4008     88.970  0.0954    96.658  250.74
  37   0.3865     89.220  0.0853    97.022  257.65
  38   0.4332     88.600  0.0777    97.280  264.57
  39   0.4191     88.790  0.0719    97.550  271.51
  40   0.3914     89.670  0.0735    97.530  278.39
  41   0.4091     89.240  0.0687    97.626  285.29
  42   0.4155     89.390  0.0666    97.606  292.19
  43   0.3936     89.800  0.0558    98.130  299.08
  44   0.3856     90.320  0.0566    98.060  305.99
  45   0.3916     90.080  0.0498    98.360  312.95
  46   0.4040     89.740  0.0428    98.586  319.83
  47   0.4019     89.860  0.0399    98.654  326.72
  48   0.4053     90.160  0.0385    98.738  333.65
  49   0.4157     90.070  0.0363    98.800  340.51
  50   0.4093     90.430  0.0311    99.062  347.37
  51   0.4458     89.570  0.0306    99.064  354.26
  52   0.3910     90.290  0.0302    99.080  361.19
  53   0.4036     90.480  0.0271    99.144  368.11
  54   0.4030     90.490  0.0226    99.344  375.03
  55   0.4306     90.170  0.0207    99.368  381.99
  56   0.3906     90.780  0.0184    99.496  388.87
  57   0.4088     90.520  0.0172    99.528  395.80
  58   0.3971     90.700  0.0160    99.542  402.73
  59   0.4069     90.880  0.0147    99.612  409.64
  60   0.3964     90.740  0.0138    99.628  416.65
  61   0.3936     91.050  0.0104    99.748  423.59
  62   0.3967     90.910  0.0105    99.752  430.52
  63   0.3833     91.090  0.0101    99.772  437.45
  64   0.3948     91.150  0.0089    99.816  444.38
  65   0.3934     91.350  0.0083    99.824  451.38
  66   0.3899     91.270  0.0074    99.838  458.27
  67   0.3893     91.110  0.0079    99.824  465.16
  68   0.3874     91.450  0.0068    99.876  472.05
  69   0.3826     91.330  0.0061    99.900  478.94
  70   0.3865     91.170  0.0060    99.880  485.87
  71   0.3830     91.410  0.0058    99.902  492.88
  72   0.3846     91.340  0.0055    99.922  499.77
  73   0.3828     91.270  0.0050    99.938  506.65
  74   0.3878     91.170  0.0052    99.910  513.58
  75   0.3855     91.310  0.0051    99.928  520.48
  76   0.3806     91.240  0.0046    99.954  527.41
  77   0.3818     91.280  0.0048    99.912  534.39
  78   0.3834     91.400  0.0048    99.928  541.33
  79   0.3867     91.260  0.0044    99.952  548.23
  80   0.3768     91.550  0.0044    99.930  555.16
  81   0.3801     91.440  0.0041    99.950  562.06
  82   0.3863     91.220  0.0042    99.932  569.04
  83   0.3824     91.330  0.0041    99.962  575.93
  84   0.3868     91.540  0.0036    99.974  582.85
  85   0.3850     91.270  0.0037    99.954  589.77
