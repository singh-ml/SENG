Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4649     45.700  1.7654    34.342  8.74
   2   1.2406     54.770  1.3726    49.668  15.74
   3   1.0904     60.400  1.1780    57.314  22.71
   4   0.9626     65.730  1.0287    62.812  29.71
   5   0.8681     68.890  0.9270    66.684  36.72
   6   0.8141     71.120  0.8437    69.816  43.69
   7   0.7473     73.370  0.7745    72.558  50.71
   8   0.7190     75.080  0.7138    74.820  57.68
   9   0.6894     75.890  0.6644    76.558  64.67
  10   0.6405     77.610  0.6230    78.116  71.72
  11   0.6042     79.130  0.5837    79.592  78.69
  12   0.5923     79.330  0.5510    80.604  85.68
  13   0.5623     80.530  0.5187    81.772  92.69
  14   0.5491     81.510  0.4921    82.830  99.73
  15   0.5357     81.440  0.4702    83.632  106.69
  16   0.5535     81.440  0.4468    84.268  113.67
  17   0.5051     83.070  0.4254    85.144  120.70
  18   0.4895     83.440  0.4107    85.556  127.69
  19   0.4983     83.740  0.3936    86.206  134.74
  20   0.4794     84.150  0.3747    86.838  141.71
  21   0.4836     84.470  0.3577    87.592  148.67
  22   0.4807     84.470  0.3477    87.790  155.65
  23   0.4746     84.600  0.3351    88.246  162.63
  24   0.4806     84.510  0.3240    88.740  169.70
  25   0.4540     85.790  0.3080    89.220  176.65
  26   0.4809     85.160  0.2997    89.502  183.60
  27   0.4523     85.810  0.2846    90.068  190.59
  28   0.4824     85.100  0.2797    90.242  197.59
  29   0.4509     85.940  0.2677    90.576  204.66
  30   0.4513     86.170  0.2582    90.904  211.65
  31   0.4540     86.360  0.2464    91.418  218.59
  32   0.4559     86.230  0.2380    91.616  225.56
  33   0.4510     86.620  0.2316    91.938  232.56
  34   0.4385     86.650  0.2191    92.288  239.64
  35   0.4722     86.140  0.2115    92.692  246.64
  36   0.4437     86.790  0.2123    92.464  253.61
  37   0.4601     86.410  0.1964    93.006  260.59
  38   0.4688     86.660  0.1893    93.240  267.59
  39   0.4673     86.330  0.1866    93.474  274.64
  40   0.4543     87.160  0.1760    93.858  281.59
  41   0.4571     86.980  0.1715    93.926  288.55
  42   0.4391     87.390  0.1699    93.928  295.54
  43   0.4660     86.540  0.1563    94.560  302.52
  44   0.4654     86.910  0.1510    94.688  309.59
  45   0.4490     87.430  0.1469    94.840  316.57
  46   0.4811     86.890  0.1366    95.310  323.53
  47   0.4685     87.320  0.1359    95.206  330.49
  48   0.4795     87.330  0.1297    95.518  337.47
  49   0.4873     87.190  0.1270    95.582  344.46
  50   0.4792     87.370  0.1204    95.836  351.36
  51   0.4656     87.610  0.1156    96.036  358.31
  52   0.4744     87.570  0.1140    95.934  365.27
  53   0.4789     87.690  0.1057    96.330  372.22
  54   0.4794     87.650  0.0999    96.574  379.26
  55   0.5075     87.320  0.1019    96.434  386.25
  56   0.4783     87.920  0.0947    96.768  393.21
  57   0.4854     88.030  0.0904    96.898  400.19
  58   0.4871     87.820  0.0880    96.906  407.15
  59   0.5061     87.750  0.0821    97.168  414.16
  60   0.4887     88.360  0.0812    97.200  421.14
  61   0.4933     88.060  0.0761    97.366  428.10
  62   0.4884     88.050  0.0720    97.578  435.09
  63   0.4925     88.230  0.0716    97.518  442.04
  64   0.5254     87.880  0.0691    97.588  449.08
  65   0.5053     88.030  0.0661    97.786  456.04
  66   0.5021     87.860  0.0646    97.872  463.02
  67   0.5093     88.040  0.0581    98.054  470.00
  68   0.5094     88.190  0.0583    98.048  476.95
  69   0.5141     88.320  0.0561    98.104  484.03
  70   0.5170     88.480  0.0534    98.210  491.00
  71   0.5240     88.120  0.0520    98.288  498.01
  72   0.5096     88.450  0.0525    98.250  504.98
  73   0.5056     88.640  0.0477    98.424  512.02
  74   0.5078     88.340  0.0477    98.506  519.01
  75   0.5384     88.520  0.0478    98.462  525.98
  76   0.5260     88.220  0.0459    98.504  532.98
  77   0.5295     88.170  0.0425    98.680  539.93
  78   0.5295     88.310  0.0432    98.578  546.91
  79   0.5302     88.240  0.0400    98.748  553.96
  80   0.5297     88.440  0.0371    98.866  560.98
  81   0.5249     88.520  0.0366    98.892  567.98
  82   0.5302     88.480  0.0367    98.876  575.00
  83   0.5422     88.460  0.0362    98.860  582.00
  84   0.5473     88.290  0.0341    98.962  589.09
  85   0.5402     88.580  0.0357    98.864  596.08
  86   0.5439     88.610  0.0330    98.992  603.07
  87   0.5474     88.520  0.0323    99.010  610.04
  88   0.5486     88.310  0.0331    98.982  617.00
  89   0.5346     88.610  0.0300    99.110  624.06
  90   0.5426     88.660  0.0300    99.098  631.00
