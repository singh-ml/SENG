Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4331     46.850  1.7674    34.520  8.78
   2   1.1909     57.140  1.3353    51.228  15.84
   3   1.0443     63.170  1.1293    58.938  22.77
   4   0.9157     67.960  0.9828    65.126  29.71
   5   0.8356     70.620  0.8712    68.830  36.68
   6   0.7593     73.310  0.7839    72.232  43.79
   7   0.6961     75.020  0.7202    74.618  50.73
   8   0.6556     77.360  0.6560    76.852  57.71
   9   0.6422     77.960  0.6082    78.654  64.64
  10   0.5974     79.520  0.5629    80.332  71.57
  11   0.5690     80.690  0.5267    81.496  78.54
  12   0.5762     80.580  0.4983    82.648  85.60
  13   0.5535     81.170  0.4764    83.298  92.53
  14   0.5352     81.590  0.4514    84.282  99.48
  15   0.5007     83.330  0.4284    85.036  106.46
  16   0.5189     83.020  0.4106    85.494  113.39
  17   0.4857     83.640  0.3948    86.116  120.41
  18   0.4792     84.340  0.3702    87.150  127.35
  19   0.4581     84.450  0.3602    87.344  134.27
  20   0.4686     84.800  0.3403    88.094  141.22
  21   0.4701     84.880  0.3298    88.462  148.19
  22   0.4617     85.100  0.3163    89.078  155.15
  23   0.4624     85.540  0.3082    89.266  162.19
  24   0.4548     85.640  0.2906    89.814  169.13
  25   0.4549     85.320  0.2781    90.254  176.14
  26   0.4609     85.590  0.2689    90.648  183.10
  27   0.4730     85.490  0.2577    90.942  190.16
  28   0.4469     86.060  0.2516    91.148  197.14
  29   0.4348     86.310  0.2413    91.754  204.06
  30   0.4351     86.850  0.2292    92.036  211.01
  31   0.4306     86.610  0.2229    92.072  217.94
  32   0.4275     87.260  0.2134    92.410  224.89
  33   0.4404     86.920  0.2058    92.712  231.82
  34   0.4488     86.230  0.1967    93.060  238.79
  35   0.4631     86.480  0.1895    93.360  245.76
  36   0.4309     87.330  0.1821    93.568  252.72
  37   0.4452     86.950  0.1705    94.052  259.72
  38   0.4342     87.320  0.1720    94.018  266.70
  39   0.4599     87.270  0.1599    94.476  273.67
  40   0.4657     86.940  0.1577    94.494  280.60
  41   0.4605     87.150  0.1501    94.736  287.58
  42   0.4413     87.250  0.1406    95.102  294.55
  43   0.4558     87.700  0.1304    95.444  301.48
  44   0.4593     87.500  0.1336    95.268  308.47
  45   0.4778     86.940  0.1241    95.558  315.39
  46   0.4802     87.550  0.1151    96.006  322.35
  47   0.4731     87.410  0.1159    96.018  329.29
  48   0.4603     88.040  0.1072    96.228  336.31
  49   0.4682     87.900  0.1018    96.394  343.24
  50   0.4690     87.840  0.1020    96.478  350.09
  51   0.4638     87.930  0.0925    96.812  357.02
  52   0.4771     87.870  0.0932    96.850  363.94
  53   0.4873     87.790  0.0884    96.866  370.92
  54   0.4874     87.760  0.0823    97.202  377.91
  55   0.4717     88.170  0.0787    97.242  384.85
  56   0.4949     87.860  0.0777    97.264  391.81
  57   0.5063     88.300  0.0728    97.526  398.76
  58   0.4898     88.270  0.0684    97.620  405.69
  59   0.4926     88.090  0.0664    97.750  412.70
  60   0.4920     88.230  0.0614    97.944  419.68
  61   0.4899     88.560  0.0617    97.908  426.64
  62   0.4878     88.480  0.0567    98.146  433.60
  63   0.5166     88.270  0.0537    98.238  440.56
  64   0.5262     87.960  0.0525    98.234  447.62
  65   0.4997     88.440  0.0510    98.300  454.54
  66   0.4922     88.630  0.0502    98.324  461.46
  67   0.5005     88.870  0.0478    98.458  468.38
  68   0.5048     88.440  0.0447    98.584  475.31
  69   0.5180     88.230  0.0426    98.638  482.34
  70   0.5058     88.490  0.0408    98.644  489.29
  71   0.5027     88.700  0.0386    98.796  496.23
  72   0.5152     88.610  0.0366    98.880  503.20
  73   0.5035     88.770  0.0358    98.888  510.17
  74   0.5078     88.810  0.0360    98.876  517.22
  75   0.5196     88.740  0.0337    98.888  524.18
  76   0.4994     89.070  0.0316    99.080  531.15
  77   0.5196     88.510  0.0323    98.992  538.08
  78   0.5252     88.510  0.0299    99.116  545.07
  79   0.5236     88.690  0.0298    99.080  552.10
  80   0.5197     88.740  0.0302    99.026  559.07
  81   0.5452     88.630  0.0275    99.162  566.03
  82   0.5164     88.870  0.0281    99.184  572.98
  83   0.5257     88.710  0.0265    99.206  579.94
  84   0.5327     89.060  0.0282    99.142  586.86
  85   0.5336     88.670  0.0241    99.286  593.91
  86   0.5249     88.780  0.0250    99.280  600.86
  87   0.5303     88.830  0.0236    99.294  607.80
  88   0.5303     88.850  0.0251    99.216  614.75
  89   0.5255     89.230  0.0221    99.392  621.70
  90   0.5343     88.910  0.0236    99.270  628.72
