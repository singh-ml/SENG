Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3934     48.680  1.7232    35.882  8.69
   2   1.1655     57.930  1.2903    52.872  15.70
   3   0.9987     64.140  1.0677    61.364  22.70
   4   0.8609     69.160  0.9199    67.070  29.75
   5   0.7752     72.290  0.8190    70.656  36.69
   6   0.7231     74.810  0.7364    73.792  43.65
   7   0.6646     77.120  0.6664    76.456  50.61
   8   0.6401     77.380  0.6158    78.314  57.61
   9   0.6034     79.230  0.5734    79.868  64.57
  10   0.5745     80.480  0.5362    81.228  71.57
  11   0.5636     80.700  0.5009    82.486  78.53
  12   0.5195     82.310  0.4726    83.446  85.53
  13   0.5356     81.570  0.4416    84.498  92.52
  14   0.5132     82.620  0.4229    85.290  99.53
  15   0.4920     83.410  0.4001    85.906  106.51
  16   0.4875     83.350  0.3806    86.710  113.51
  17   0.4799     84.120  0.3621    87.316  120.51
  18   0.4742     84.070  0.3491    87.804  127.56
  19   0.4439     85.550  0.3331    88.442  134.53
  20   0.4722     84.560  0.3136    89.132  141.55
  21   0.4449     85.560  0.2999    89.504  148.52
  22   0.4391     85.670  0.2896    89.738  155.49
  23   0.4514     85.360  0.2723    90.532  162.52
  24   0.4602     85.610  0.2684    90.566  169.53
  25   0.4690     84.820  0.2571    90.972  176.49
  26   0.4219     86.710  0.2468    91.324  183.45
  27   0.4502     86.190  0.2335    91.848  190.44
  28   0.4413     86.100  0.2264    92.066  197.49
  29   0.4297     86.770  0.2154    92.408  204.48
  30   0.4131     87.170  0.2059    92.702  211.45
  31   0.4325     86.840  0.1914    93.328  218.44
  32   0.4467     86.680  0.1836    93.580  225.44
  33   0.4447     86.710  0.1777    93.820  232.54
  34   0.4318     87.550  0.1721    93.920  239.49
  35   0.4273     87.410  0.1600    94.390  246.47
  36   0.4230     87.460  0.1615    94.344  253.44
  37   0.4448     87.520  0.1518    94.654  260.47
  38   0.4360     87.570  0.1409    95.192  267.42
  39   0.4479     87.390  0.1351    95.274  274.42
  40   0.4413     87.760  0.1305    95.418  281.41
  41   0.4393     88.080  0.1256    95.408  288.40
  42   0.4616     87.810  0.1134    96.046  295.40
  43   0.4651     87.190  0.1152    95.854  302.35
  44   0.4760     87.430  0.1066    96.290  309.35
  45   0.4532     88.220  0.1029    96.462  316.34
  46   0.4623     88.140  0.0969    96.598  323.31
  47   0.4593     88.050  0.0904    96.836  330.29
  48   0.4634     88.030  0.0854    96.978  337.34
  49   0.4939     87.620  0.0871    96.904  344.34
  50   0.4595     88.430  0.0792    97.280  351.23
  51   0.4639     88.210  0.0756    97.436  358.22
  52   0.4790     88.350  0.0742    97.402  365.21
  53   0.4863     88.310  0.0689    97.596  372.26
  54   0.4823     88.380  0.0646    97.822  379.22
  55   0.4801     88.290  0.0622    97.822  386.22
  56   0.4754     88.410  0.0580    98.012  393.18
  57   0.4900     88.480  0.0518    98.280  400.17
  58   0.4887     88.580  0.0494    98.358  407.20
  59   0.5021     88.180  0.0510    98.236  414.21
  60   0.4894     88.600  0.0472    98.464  421.18
  61   0.4873     88.490  0.0454    98.556  428.16
  62   0.5032     88.800  0.0401    98.708  435.20
  63   0.5032     88.720  0.0385    98.760  442.18
  64   0.4962     88.890  0.0384    98.738  449.14
  65   0.5038     88.860  0.0362    98.892  456.13
  66   0.5077     89.070  0.0340    98.932  463.12
  67   0.5112     88.880  0.0320    98.986  470.17
  68   0.4993     88.960  0.0330    98.902  477.16
  69   0.5019     88.530  0.0272    99.204  484.16
  70   0.5167     88.920  0.0277    99.172  491.16
  71   0.5156     88.870  0.0262    99.196  498.14
  72   0.5225     88.790  0.0266    99.170  505.19
  73   0.5148     89.070  0.0251    99.286  512.19
  74   0.5199     88.910  0.0244    99.324  519.15
  75   0.5230     89.280  0.0234    99.290  526.14
  76   0.5215     89.200  0.0218    99.392  533.10
  77   0.5221     89.110  0.0227    99.348  540.15
  78   0.5194     89.240  0.0209    99.428  547.13
  79   0.5167     89.250  0.0201    99.438  554.13
  80   0.5241     89.240  0.0202    99.444  561.11
  81   0.5247     89.540  0.0195    99.424  568.18
  82   0.5229     89.490  0.0190    99.444  575.18
  83   0.5260     89.200  0.0181    99.484  582.17
  84   0.5416     88.910  0.0180    99.502  589.16
  85   0.5350     89.320  0.0179    99.482  596.18
