Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7493510144 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4448     46.420  1.7554    34.760  8.68
   2   1.2306     55.430  1.3497    50.632  15.75
   3   1.0771     60.900  1.1567    58.436  22.74
   4   0.9242     66.780  0.9960    64.228  29.71
   5   0.8341     71.090  0.8846    68.812  36.70
   6   0.7793     73.260  0.7921    71.962  43.69
   7   0.6997     75.590  0.7243    74.464  50.71
   8   0.6596     77.130  0.6521    77.056  57.70
   9   0.6155     78.490  0.6066    78.660  64.69
  10   0.6000     79.080  0.5627    80.200  71.66
  11   0.5736     80.470  0.5361    81.376  78.70
  12   0.5538     81.400  0.4997    82.584  85.68
  13   0.5439     82.090  0.4660    83.684  92.66
  14   0.5257     82.400  0.4419    84.586  99.63
  15   0.4967     83.130  0.4224    85.224  106.62
  16   0.4902     83.280  0.4020    86.042  113.68
  17   0.4897     83.280  0.3789    86.770  120.68
  18   0.4775     84.360  0.3706    87.130  127.64
  19   0.4536     85.030  0.3484    87.848  134.60
  20   0.4573     85.010  0.3372    88.200  141.59
  21   0.4618     85.140  0.3266    88.588  148.65
  22   0.4620     85.440  0.3114    89.050  155.70
  23   0.4595     85.480  0.2993    89.528  162.67
  24   0.4287     85.920  0.2853    90.002  169.63
  25   0.4303     86.190  0.2716    90.578  176.63
  26   0.4353     86.340  0.2556    91.118  183.68
  27   0.4325     86.420  0.2488    91.324  190.64
  28   0.4200     86.870  0.2429    91.594  197.62
  29   0.4363     86.270  0.2292    91.952  204.60
  30   0.4199     86.720  0.2231    92.148  211.61
  31   0.4215     86.390  0.2109    92.568  218.64
  32   0.4086     87.200  0.2046    92.858  225.60
  33   0.4244     86.970  0.1939    93.304  232.56
  34   0.4018     87.540  0.1857    93.508  239.52
  35   0.4282     86.960  0.1774    93.850  246.46
  36   0.4120     87.740  0.1723    93.970  253.50
  37   0.4284     87.550  0.1636    94.336  260.48
  38   0.4221     87.680  0.1571    94.576  267.48
  39   0.4186     88.400  0.1521    94.628  274.43
  40   0.4129     87.910  0.1440    95.020  281.43
  41   0.4215     88.510  0.1383    95.208  288.52
  42   0.4393     87.790  0.1362    95.254  295.50
  43   0.4342     88.100  0.1259    95.626  302.47
  44   0.4311     87.850  0.1226    95.680  309.45
  45   0.4576     87.980  0.1146    96.032  316.44
  46   0.4351     88.250  0.1102    96.188  323.51
  47   0.4503     87.610  0.1074    96.316  330.51
  48   0.4358     88.290  0.0996    96.444  337.48
  49   0.4465     88.500  0.0930    96.632  344.44
  50   0.4424     88.460  0.0904    96.896  351.33
  51   0.4392     88.740  0.0840    96.996  358.38
  52   0.4591     88.130  0.0849    97.030  365.37
  53   0.4432     88.670  0.0787    97.258  372.32
  54   0.4493     88.350  0.0749    97.414  379.28
  55   0.4617     88.400  0.0730    97.518  386.23
  56   0.4468     88.520  0.0678    97.638  393.27
  57   0.4545     88.940  0.0646    97.800  400.22
  58   0.4784     88.010  0.0634    97.830  407.16
  59   0.4594     88.530  0.0592    98.056  414.16
  60   0.4567     88.650  0.0549    98.254  421.13
  61   0.4598     88.600  0.0555    98.186  428.19
  62   0.4570     88.840  0.0513    98.298  435.14
  63   0.4665     88.790  0.0490    98.402  442.10
  64   0.4967     88.310  0.0447    98.596  449.09
  65   0.4770     88.830  0.0424    98.622  456.12
  66   0.4880     89.100  0.0413    98.728  463.08
  67   0.4848     88.850  0.0370    98.930  470.04
  68   0.4740     89.090  0.0357    98.912  477.00
  69   0.4853     89.040  0.0347    98.942  483.99
  70   0.4937     88.870  0.0334    99.010  491.04
  71   0.4952     88.990  0.0339    98.964  498.03
  72   0.4797     89.130  0.0313    99.076  504.97
  73   0.4819     89.110  0.0301    99.112  512.01
  74   0.4847     89.340  0.0301    99.052  519.03
  75   0.4906     89.430  0.0272    99.220  526.10
  76   0.4868     89.110  0.0266    99.250  533.08
  77   0.4906     89.400  0.0263    99.254  540.08
  78   0.4863     89.020  0.0251    99.244  547.06
  79   0.4792     89.500  0.0244    99.306  554.05
  80   0.4856     89.250  0.0239    99.324  561.10
  81   0.4918     89.690  0.0221    99.432  568.07
  82   0.5033     89.350  0.0216    99.404  575.06
  83   0.4982     89.470  0.0210    99.466  582.06
  84   0.5030     89.270  0.0207    99.462  589.04
  85   0.4958     89.280  0.0212    99.400  596.08
