Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7493510144 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4974     45.640  1.7399    35.068  8.95
   2   1.1694     57.480  1.3185    51.916  16.00
   3   1.0106     63.420  1.1028    60.414  22.93
   4   0.8783     68.980  0.9599    65.564  29.89
   5   0.8212     71.260  0.8601    69.308  36.84
   6   0.7418     73.640  0.7737    72.542  43.80
   7   0.6845     76.280  0.7044    75.194  50.84
   8   0.6597     77.480  0.6485    77.306  57.82
   9   0.6345     78.310  0.6038    78.772  64.81
  10   0.5897     79.590  0.5678    80.162  71.74
  11   0.5560     80.620  0.5319    81.410  78.72
  12   0.5697     80.480  0.5069    82.272  85.75
  13   0.5356     81.680  0.4798    83.318  92.75
  14   0.5205     82.550  0.4542    84.260  99.71
  15   0.5026     82.680  0.4340    84.854  106.69
  16   0.5014     83.020  0.4112    85.726  113.69
  17   0.4834     83.940  0.3946    86.232  120.64
  18   0.4833     83.940  0.3741    87.006  127.61
  19   0.4885     83.750  0.3537    87.662  134.57
  20   0.4539     84.810  0.3390    88.170  141.54
  21   0.4567     85.090  0.3319    88.412  148.48
  22   0.4557     84.830  0.3152    88.966  155.53
  23   0.4581     85.150  0.3054    89.392  162.48
  24   0.4774     85.100  0.2921    89.864  169.46
  25   0.4599     85.520  0.2829    90.060  176.39
  26   0.4442     85.870  0.2669    90.746  183.32
  27   0.4528     85.710  0.2541    91.072  190.39
  28   0.4400     85.900  0.2450    91.266  197.36
  29   0.4302     86.340  0.2403    91.644  204.30
  30   0.4317     86.760  0.2255    92.042  211.26
  31   0.4321     86.440  0.2216    92.258  218.21
  32   0.4216     86.750  0.2124    92.486  225.18
  33   0.4347     87.170  0.2059    92.922  232.22
  34   0.4345     86.830  0.1976    92.902  239.15
  35   0.4382     87.330  0.1865    93.338  246.13
  36   0.4289     87.400  0.1833    93.584  253.11
  37   0.4334     87.070  0.1722    93.882  260.15
  38   0.4459     87.340  0.1655    94.206  267.09
  39   0.4354     87.130  0.1575    94.452  274.01
  40   0.4468     87.240  0.1491    94.720  280.99
  41   0.4636     86.720  0.1430    95.040  287.97
  42   0.4651     86.910  0.1432    94.968  294.99
  43   0.4598     87.470  0.1325    95.290  301.93
  44   0.4513     87.600  0.1295    95.470  308.86
  45   0.4712     87.300  0.1231    95.716  315.84
  46   0.4454     87.570  0.1190    95.782  322.81
  47   0.4520     88.190  0.1109    96.084  329.89
  48   0.4468     88.060  0.1033    96.414  336.86
  49   0.4629     87.660  0.1004    96.496  343.84
  50   0.4425     88.430  0.0973    96.620  350.68
  51   0.4512     88.020  0.0930    96.766  357.66
  52   0.4744     87.760  0.0882    96.958  364.63
  53   0.4535     88.370  0.0835    97.132  371.64
  54   0.4497     88.630  0.0881    96.862  378.60
  55   0.4587     88.400  0.0791    97.252  385.52
  56   0.4666     88.290  0.0776    97.304  392.46
  57   0.4604     88.390  0.0697    97.648  399.40
  58   0.4702     88.390  0.0670    97.726  406.42
  59   0.4653     88.610  0.0654    97.792  413.36
  60   0.4761     88.400  0.0630    97.890  420.30
  61   0.4661     88.810  0.0594    98.042  427.27
  62   0.4698     88.550  0.0568    98.042  434.21
  63   0.4867     88.490  0.0527    98.212  441.28
  64   0.4854     88.640  0.0498    98.390  448.22
  65   0.4843     88.580  0.0479    98.342  455.14
  66   0.4791     88.760  0.0464    98.504  462.12
  67   0.4905     88.820  0.0442    98.518  469.07
  68   0.4904     88.650  0.0414    98.724  476.10
  69   0.4992     88.800  0.0427    98.560  483.04
  70   0.4897     88.870  0.0419    98.620  490.00
  71   0.4965     88.910  0.0375    98.824  496.97
  72   0.5096     88.740  0.0361    98.864  503.92
  73   0.4967     88.770  0.0380    98.730  510.88
  74   0.5073     88.660  0.0334    98.980  517.89
  75   0.5050     88.800  0.0329    98.974  524.87
  76   0.5066     89.070  0.0319    99.026  531.84
  77   0.5160     88.800  0.0303    99.018  538.81
  78   0.5204     88.780  0.0283    99.180  545.87
  79   0.5145     88.710  0.0280    99.132  552.84
  80   0.5156     88.890  0.0283    99.132  559.82
  81   0.5147     88.790  0.0270    99.184  566.79
  82   0.5210     88.760  0.0263    99.258  573.75
  83   0.5297     88.670  0.0281    99.144  580.78
  84   0.5275     88.860  0.0268    99.192  587.73
  85   0.5169     88.980  0.0253    99.246  594.69
