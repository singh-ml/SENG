Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3796     48.830  1.7042    36.710  8.81
   2   1.1240     60.000  1.2559    54.206  15.78
   3   0.9537     66.120  1.0378    62.610  22.74
   4   0.8454     70.110  0.8967    67.764  29.75
   5   0.7742     72.980  0.7966    71.744  36.75
   6   0.6911     75.430  0.7169    74.508  43.80
   7   0.6610     77.030  0.6516    76.848  50.74
   8   0.6165     78.680  0.6037    78.714  57.69
   9   0.5817     80.270  0.5558    80.546  64.66
  10   0.5714     80.620  0.5235    81.776  71.65
  11   0.5560     81.500  0.4913    82.924  78.62
  12   0.5238     82.220  0.4650    83.580  85.63
  13   0.5194     82.430  0.4329    84.890  92.58
  14   0.4819     83.560  0.4136    85.562  99.54
  15   0.4689     84.460  0.3922    86.432  106.47
  16   0.4583     84.990  0.3738    86.886  113.46
  17   0.4596     84.650  0.3561    87.648  120.51
  18   0.4441     85.560  0.3411    88.128  127.51
  19   0.4550     85.170  0.3220    88.742  134.53
  20   0.4381     85.740  0.3085    89.208  141.53
  21   0.4369     86.080  0.2904    89.842  148.48
  22   0.4598     85.460  0.2797    90.174  155.51
  23   0.4455     86.060  0.2717    90.406  162.44
  24   0.4373     86.110  0.2640    90.736  169.38
  25   0.4339     86.690  0.2478    91.264  176.33
  26   0.4261     86.910  0.2366    91.628  183.30
  27   0.4322     86.650  0.2253    91.900  190.35
  28   0.4456     86.250  0.2178    92.376  197.29
  29   0.4102     87.380  0.2112    92.586  204.30
  30   0.4167     87.130  0.1939    93.124  211.28
  31   0.4172     87.410  0.1857    93.368  218.23
  32   0.4290     87.200  0.1790    93.738  225.28
  33   0.4520     87.240  0.1706    93.976  232.24
  34   0.4216     87.570  0.1723    93.922  239.22
  35   0.4406     87.460  0.1520    94.606  246.20
  36   0.4284     87.880  0.1528    94.588  253.16
  37   0.4301     87.840  0.1460    94.836  260.22
  38   0.4495     87.520  0.1358    95.224  267.18
  39   0.4431     87.760  0.1281    95.430  274.14
  40   0.4298     87.870  0.1244    95.620  281.08
  41   0.4321     88.520  0.1171    95.812  288.01
  42   0.4471     88.220  0.1148    95.966  295.06
  43   0.4429     88.310  0.1058    96.306  302.03
  44   0.4485     88.410  0.1013    96.406  308.98
  45   0.4497     88.440  0.0992    96.438  315.94
  46   0.4378     88.750  0.0949    96.672  322.91
  47   0.4621     88.180  0.0862    97.022  329.83
  48   0.4688     88.020  0.0856    96.984  336.88
  49   0.4543     88.640  0.0836    97.102  343.85
  50   0.4631     88.570  0.0760    97.356  350.67
  51   0.4583     88.500  0.0744    97.422  357.60
  52   0.4611     88.680  0.0684    97.600  364.54
  53   0.4838     88.720  0.0657    97.814  371.57
  54   0.4741     88.580  0.0601    97.942  378.51
  55   0.4999     88.490  0.0561    98.120  385.49
  56   0.4783     89.000  0.0505    98.354  392.47
  57   0.4964     88.720  0.0522    98.232  399.43
  58   0.4721     89.310  0.0510    98.290  406.48
  59   0.4768     89.180  0.0476    98.396  413.42
  60   0.4745     89.250  0.0466    98.474  420.40
  61   0.4902     89.160  0.0456    98.416  427.36
  62   0.4835     89.080  0.0398    98.700  434.30
  63   0.5010     89.040  0.0390    98.710  441.30
  64   0.4794     89.210  0.0371    98.802  448.25
  65   0.5056     89.250  0.0344    98.880  455.23
  66   0.4854     89.470  0.0326    98.994  462.20
  67   0.5052     89.180  0.0306    99.076  469.14
  68   0.4838     89.390  0.0300    99.050  476.12
  69   0.5049     89.150  0.0276    99.160  483.14
  70   0.4994     89.410  0.0295    99.046  490.14
  71   0.5018     89.550  0.0268    99.188  497.07
  72   0.4931     89.520  0.0268    99.188  504.05
  73   0.5004     89.700  0.0236    99.304  511.09
  74   0.4990     89.330  0.0238    99.280  518.07
  75   0.5015     89.640  0.0234    99.276  524.98
  76   0.4961     89.680  0.0235    99.280  531.92
  77   0.5062     89.580  0.0202    99.438  538.90
  78   0.5061     89.530  0.0194    99.480  546.17
  79   0.5070     89.800  0.0200    99.436  553.12
  80   0.5158     89.630  0.0194    99.432  560.05
  81   0.5000     89.610  0.0189    99.468  567.04
  82   0.5076     89.760  0.0182    99.510  573.98
  83   0.4975     89.990  0.0192    99.446  580.96
  84   0.5092     89.900  0.0185    99.480  588.04
  85   0.5135     89.670  0.0185    99.472  594.99
