Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4155     49.540  1.6297    39.256  8.78
   2   1.0356     63.060  1.1769    57.334  15.87
   3   0.8842     68.820  0.9683    65.280  22.89
   4   0.7802     72.540  0.8309    70.572  29.96
   5   0.7288     74.410  0.7319    74.250  36.97
   6   0.6335     77.750  0.6526    77.148  43.99
   7   0.5941     79.670  0.5904    79.362  51.01
   8   0.5608     80.820  0.5341    81.310  58.03
   9   0.5384     81.840  0.5019    82.662  65.12
  10   0.5111     82.790  0.4639    83.822  72.11
  11   0.5048     82.600  0.4346    84.922  79.10
  12   0.4967     82.920  0.4082    85.782  86.11
  13   0.4654     84.340  0.3829    86.770  93.10
  14   0.4831     83.840  0.3643    87.356  100.22
  15   0.4460     84.680  0.3417    88.188  107.20
  16   0.4500     84.920  0.3233    88.740  114.22
  17   0.4549     85.060  0.3105    89.236  121.21
  18   0.4264     85.660  0.2927    89.726  128.33
  19   0.4006     86.500  0.2748    90.438  135.34
  20   0.4218     86.440  0.2623    90.778  142.34
  21   0.4341     85.440  0.2540    91.136  149.35
  22   0.4225     86.340  0.2347    91.816  156.34
  23   0.4198     86.370  0.2258    92.198  163.36
  24   0.4025     86.910  0.2227    92.268  170.51
  25   0.4031     87.560  0.2039    92.832  177.51
  26   0.4087     87.130  0.1925    93.274  184.49
  27   0.3826     87.920  0.1826    93.612  191.49
  28   0.4173     87.360  0.1730    93.896  198.46
  29   0.4153     87.820  0.1661    94.236  205.53
  30   0.3909     87.910  0.1634    94.216  212.55
  31   0.4059     88.100  0.1496    94.750  219.56
  32   0.4162     87.610  0.1450    94.892  226.59
  33   0.4072     88.140  0.1404    95.036  233.59
  34   0.4033     88.280  0.1267    95.518  240.68
  35   0.3994     88.390  0.1193    95.834  247.71
  36   0.3864     88.790  0.1105    96.130  254.71
  37   0.4151     88.530  0.1046    96.314  261.73
  38   0.4245     87.900  0.1042    96.368  268.73
  39   0.4190     88.640  0.0968    96.604  275.82
  40   0.4145     88.730  0.0920    96.782  282.85
  41   0.4149     88.620  0.0856    97.056  289.82
  42   0.4212     88.900  0.0803    97.220  296.81
  43   0.3774     89.550  0.0749    97.394  303.82
  44   0.4041     89.380  0.0685    97.706  310.90
  45   0.4257     88.840  0.0644    97.824  317.90
  46   0.4118     89.370  0.0612    97.950  324.92
  47   0.4403     89.170  0.0662    97.770  331.93
  48   0.3923     89.590  0.0554    98.166  338.97
  49   0.4057     89.720  0.0537    98.196  346.09
  50   0.3987     89.980  0.0431    98.616  352.99
  51   0.4183     89.400  0.0402    98.746  360.00
  52   0.3991     90.300  0.0391    98.770  367.02
  53   0.4058     90.030  0.0377    98.834  374.02
  54   0.3910     90.130  0.0335    98.960  381.01
  55   0.4006     90.250  0.0298    99.106  387.99
  56   0.3950     90.290  0.0266    99.234  395.01
  57   0.4065     90.250  0.0262    99.244  402.00
  58   0.3836     90.580  0.0238    99.344  409.01
  59   0.3943     90.520  0.0224    99.396  416.10
  60   0.3999     90.490  0.0196    99.468  423.12
  61   0.4016     90.470  0.0196    99.450  430.14
  62   0.3988     90.470  0.0168    99.614  437.15
  63   0.4057     90.630  0.0149    99.682  444.15
  64   0.4072     90.140  0.0142    99.694  451.15
  65   0.3997     90.780  0.0134    99.730  458.20
  66   0.3901     90.730  0.0123    99.734  465.26
  67   0.4047     90.610  0.0126    99.736  472.26
  68   0.3945     90.610  0.0121    99.762  479.25
  69   0.3980     90.660  0.0108    99.784  486.27
  70   0.3958     90.910  0.0109    99.768  493.34
  71   0.3980     90.730  0.0108    99.782  500.34
  72   0.3993     90.820  0.0092    99.838  507.33
  73   0.3985     90.880  0.0091    99.842  514.36
  74   0.3994     90.820  0.0081    99.872  521.42
  75   0.3942     90.980  0.0082    99.864  528.54
  76   0.3974     90.960  0.0084    99.876  535.55
  77   0.3995     91.020  0.0081    99.872  542.57
  78   0.4007     91.100  0.0076    99.890  549.58
  79   0.3954     91.040  0.0077    99.868  556.60
  80   0.3981     91.010  0.0074    99.886  563.67
  81   0.3993     90.840  0.0070    99.900  570.66
  82   0.3979     90.960  0.0070    99.912  577.66
  83   0.3925     90.920  0.0068    99.924  584.69
  84   0.3959     91.050  0.0069    99.880  591.72
  85   0.3947     90.940  0.0072    99.886  598.81
