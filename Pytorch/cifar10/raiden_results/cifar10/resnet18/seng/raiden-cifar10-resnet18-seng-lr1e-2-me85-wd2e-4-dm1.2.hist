Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3567     49.660  1.6737    37.470  8.67
   2   1.0752     61.800  1.2112    56.254  15.60
   3   0.9393     67.020  0.9890    64.528  22.55
   4   0.7993     71.800  0.8453    70.002  29.48
   5   0.7577     73.630  0.7417    73.726  36.39
   6   0.6573     77.270  0.6669    76.576  43.34
   7   0.6252     78.410  0.6010    78.980  50.23
   8   0.5866     79.610  0.5545    80.630  57.14
   9   0.5501     81.350  0.5023    82.606  64.01
  10   0.5501     81.510  0.4759    83.418  70.90
  11   0.5023     83.250  0.4438    84.446  77.87
  12   0.4834     83.970  0.4211    85.358  84.77
  13   0.4978     83.440  0.3922    86.124  91.65
  14   0.4924     84.460  0.3708    86.978  98.51
  15   0.4727     84.010  0.3526    87.772  105.40
  16   0.4502     85.330  0.3331    88.416  112.34
  17   0.4469     85.520  0.3158    88.934  119.25
  18   0.4293     86.130  0.3019    89.456  126.13
  19   0.4220     86.510  0.2837    89.990  133.01
  20   0.4501     85.260  0.2760    90.196  139.88
  21   0.4746     84.930  0.2597    90.982  146.86
  22   0.4374     86.210  0.2473    91.270  153.78
  23   0.4217     86.590  0.2366    91.730  160.67
  24   0.4024     87.330  0.2285    91.992  167.60
  25   0.4406     86.820  0.2164    92.408  174.47
  26   0.4025     87.410  0.2041    92.808  181.45
  27   0.4161     87.060  0.1909    93.140  188.35
  28   0.4194     87.380  0.1864    93.462  195.22
  29   0.4229     87.730  0.1765    93.606  202.11
  30   0.3985     88.320  0.1637    94.350  209.02
  31   0.4044     88.110  0.1568    94.496  215.91
  32   0.4048     87.790  0.1499    94.764  222.89
  33   0.4095     88.120  0.1415    94.982  229.80
  34   0.4087     88.300  0.1360    95.138  236.68
  35   0.4632     87.710  0.1245    95.624  243.56
  36   0.4158     88.370  0.1239    95.668  250.45
  37   0.4518     87.850  0.1158    95.856  257.42
  38   0.4280     88.220  0.1065    96.244  264.32
  39   0.4110     88.520  0.1038    96.390  271.24
  40   0.4207     88.850  0.0923    96.846  278.14
  41   0.4057     88.920  0.0845    97.072  285.01
  42   0.4252     88.950  0.0818    97.084  291.97
  43   0.4514     88.520  0.0816    97.152  298.87
  44   0.4248     88.830  0.0773    97.362  305.80
  45   0.4309     89.060  0.0684    97.632  312.69
  46   0.4365     89.250  0.0663    97.740  319.58
  47   0.4321     89.100  0.0592    97.972  326.56
  48   0.4350     89.290  0.0537    98.196  333.41
  49   0.4384     89.060  0.0531    98.228  340.33
  50   0.4249     89.260  0.0487    98.322  347.10
  51   0.4582     89.310  0.0477    98.416  353.98
  52   0.4391     89.210  0.0421    98.582  360.94
  53   0.4454     89.340  0.0393    98.678  367.86
  54   0.4405     89.650  0.0369    98.758  374.77
  55   0.4463     89.520  0.0348    98.856  381.68
  56   0.4666     89.460  0.0327    98.996  388.67
  57   0.4429     89.830  0.0301    99.052  395.61
  58   0.4501     89.980  0.0284    99.136  402.51
  59   0.4420     90.010  0.0268    99.184  409.42
  60   0.4617     89.660  0.0261    99.188  416.33
  61   0.4549     89.880  0.0231    99.334  423.34
  62   0.4603     89.770  0.0215    99.342  430.25
  63   0.4534     89.860  0.0200    99.442  437.16
  64   0.4486     89.940  0.0193    99.464  444.06
  65   0.4491     89.980  0.0180    99.510  450.97
  66   0.4504     89.960  0.0158    99.616  457.95
  67   0.4470     90.270  0.0175    99.528  464.84
  68   0.4507     90.240  0.0151    99.586  471.70
  69   0.4583     90.330  0.0130    99.696  478.58
  70   0.4519     90.210  0.0130    99.690  485.49
  71   0.4596     90.140  0.0128    99.682  492.48
  72   0.4563     90.110  0.0123    99.704  499.38
  73   0.4598     90.260  0.0116    99.716  506.28
  74   0.4484     90.280  0.0119    99.718  513.19
  75   0.4438     90.230  0.0115    99.734  520.05
  76   0.4580     90.220  0.0111    99.750  527.03
  77   0.4520     90.190  0.0103    99.780  533.90
  78   0.4555     90.290  0.0099    99.796  540.79
  79   0.4667     90.200  0.0093    99.798  547.70
  80   0.4569     90.310  0.0092    99.814  554.62
  81   0.4586     90.280  0.0092    99.804  561.57
  82   0.4613     90.260  0.0094    99.812  568.50
  83   0.4635     90.230  0.0095    99.800  575.42
  84   0.4609     90.410  0.0095    99.798  582.32
  85   0.4595     90.310  0.0087    99.812  589.22
