Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7491412992 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2992     52.180  1.6074    40.132  8.65
   2   1.0109     64.390  1.1522    58.232  15.57
   3   0.8662     69.440  0.9312    66.798  22.49
   4   0.7547     73.350  0.8089    71.312  29.37
   5   0.7187     75.000  0.7096    74.878  36.30
   6   0.6526     77.650  0.6310    77.674  43.19
   7   0.6167     79.030  0.5723    79.912  50.07
   8   0.6029     79.120  0.5215    81.848  56.99
   9   0.5267     81.910  0.4850    83.074  63.88
  10   0.4929     83.190  0.4490    84.348  70.86
  11   0.4830     83.770  0.4197    85.378  77.78
  12   0.4705     84.130  0.3955    86.308  84.71
  13   0.4690     84.380  0.3729    86.948  91.63
  14   0.4725     84.560  0.3465    87.966  98.50
  15   0.4404     85.780  0.3270    88.570  105.48
  16   0.4556     85.540  0.3083    89.172  112.37
  17   0.4414     85.930  0.2894    89.836  119.25
  18   0.4384     86.720  0.2818    90.146  126.16
  19   0.4258     85.770  0.2667    90.636  133.07
  20   0.4422     85.960  0.2494    91.354  140.10
  21   0.4147     86.760  0.2404    91.686  147.02
  22   0.4217     86.530  0.2278    92.016  153.95
  23   0.4137     87.340  0.2132    92.464  160.89
  24   0.4046     87.900  0.1990    93.068  167.75
  25   0.4328     87.010  0.1952    93.236  174.72
  26   0.3965     88.060  0.1814    93.492  181.63
  27   0.4113     87.820  0.1708    93.940  188.55
  28   0.4267     87.650  0.1651    94.124  195.47
  29   0.4174     87.800  0.1540    94.612  202.41
  30   0.5074     86.780  0.1457    94.730  209.34
  31   0.4280     87.990  0.1389    95.126  216.22
  32   0.4227     88.370  0.1347    95.290  223.10
  33   0.4127     88.260  0.1228    95.678  230.03
  34   0.4330     88.510  0.1199    95.698  236.92
  35   0.4192     88.530  0.1101    96.190  243.82
  36   0.4381     88.130  0.1034    96.414  250.79
  37   0.4296     88.920  0.0959    96.662  257.68
  38   0.4086     89.000  0.0963    96.558  264.57
  39   0.4362     88.940  0.0859    96.906  271.50
  40   0.4251     89.210  0.0882    96.916  278.37
  41   0.4413     88.920  0.0746    97.290  285.35
  42   0.5372     87.260  0.0704    97.520  292.24
  43   0.4464     89.420  0.0681    97.666  299.12
  44   0.4274     89.620  0.0647    97.662  306.05
  45   0.4245     89.500  0.0615    97.866  312.93
  46   0.4205     89.800  0.0530    98.174  319.86
  47   0.4309     89.610  0.0503    98.328  326.80
  48   0.4721     89.040  0.0494    98.288  333.68
  49   0.4795     89.340  0.0482    98.344  340.56
  50   0.4540     89.700  0.0411    98.582  347.32
  51   0.4583     89.540  0.0387    98.688  354.31
  52   0.4592     89.870  0.0365    98.756  361.19
  53   0.4540     89.890  0.0344    98.860  368.10
  54   0.4492     90.150  0.0288    99.100  375.01
  55   0.4472     89.880  0.0318    98.914  381.91
  56   0.4544     90.000  0.0263    99.180  388.89
  57   0.4674     90.070  0.0239    99.292  395.79
  58   0.4552     90.320  0.0246    99.216  402.68
  59   0.4483     90.120  0.0221    99.320  409.59
  60   0.4522     90.030  0.0202    99.388  416.47
  61   0.4669     90.150  0.0186    99.454  423.45
  62   0.4538     90.090  0.0185    99.430  430.37
  63   0.4701     90.390  0.0170    99.522  437.30
  64   0.4710     90.090  0.0174    99.498  444.19
  65   0.4583     90.310  0.0140    99.642  451.10
  66   0.4723     90.390  0.0147    99.578  457.99
  67   0.4676     90.280  0.0128    99.692  465.01
  68   0.4717     90.050  0.0131    99.670  471.96
  69   0.4657     90.380  0.0122    99.678  478.85
  70   0.4719     90.340  0.0118    99.682  485.75
  71   0.4779     90.330  0.0108    99.758  492.67
  72   0.4750     90.310  0.0101    99.758  499.69
  73   0.4700     90.350  0.0097    99.800  506.56
  74   0.4757     90.250  0.0092    99.800  513.44
  75   0.4713     90.380  0.0088    99.818  520.32
  76   0.4734     90.460  0.0084    99.802  527.21
  77   0.4771     90.440  0.0090    99.790  534.12
  78   0.4745     90.520  0.0089    99.790  541.10
  79   0.4768     90.450  0.0081    99.836  548.01
  80   0.4820     90.500  0.0077    99.830  554.95
  81   0.4751     90.570  0.0076    99.860  561.85
  82   0.4788     90.420  0.0080    99.818  568.77
  83   0.4798     90.590  0.0082    99.808  575.73
  84   0.4812     90.540  0.0072    99.848  582.67
  85   0.4828     90.430  0.0074    99.836  589.61
