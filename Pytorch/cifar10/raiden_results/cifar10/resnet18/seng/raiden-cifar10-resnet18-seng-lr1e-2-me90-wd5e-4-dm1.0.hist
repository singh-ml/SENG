Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7493510144 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3659     50.090  1.6197    39.692  8.71
   2   1.0122     63.560  1.1453    58.458  15.70
   3   0.8556     69.880  0.9221    67.022  22.74
   4   0.7427     73.710  0.7854    72.288  29.70
   5   0.6505     76.870  0.6847    75.960  36.67
   6   0.6109     78.870  0.6160    78.470  43.66
   7   0.5682     80.430  0.5639    80.322  50.64
   8   0.5497     81.460  0.5145    81.986  57.67
   9   0.5202     82.200  0.4781    83.190  64.64
  10   0.5178     82.650  0.4468    84.426  71.63
  11   0.4987     83.790  0.4169    85.356  78.58
  12   0.4741     84.610  0.3909    86.484  85.53
  13   0.4743     84.130  0.3701    87.150  92.59
  14   0.4676     84.760  0.3455    87.860  99.58
  15   0.4450     85.130  0.3311    88.526  106.58
  16   0.4338     85.350  0.3093    89.264  113.60
  17   0.4449     85.690  0.2929    89.652  120.61
  18   0.4279     86.190  0.2802    90.344  127.64
  19   0.4143     86.820  0.2703    90.482  134.59
  20   0.4317     86.260  0.2528    91.164  141.58
  21   0.4246     86.660  0.2378    91.652  148.54
  22   0.4073     86.700  0.2296    91.946  155.48
  23   0.4222     87.160  0.2132    92.494  162.46
  24   0.3906     88.170  0.1975    93.048  169.52
  25   0.4442     86.750  0.1980    93.028  176.47
  26   0.4455     87.260  0.1877    93.416  183.46
  27   0.3970     87.580  0.1779    93.786  190.43
  28   0.4409     87.270  0.1657    94.112  197.37
  29   0.4212     87.970  0.1578    94.402  204.39
  30   0.4375     87.330  0.1547    94.612  211.37
  31   0.4039     88.240  0.1431    94.992  218.31
  32   0.3972     88.790  0.1378    95.258  225.30
  33   0.4474     87.310  0.1287    95.488  232.22
  34   0.4000     89.070  0.1205    95.756  239.16
  35   0.3907     88.980  0.1175    95.822  246.12
  36   0.4195     88.690  0.1054    96.226  253.09
  37   0.4206     89.090  0.1031    96.326  260.08
  38   0.4183     89.170  0.0947    96.734  267.03
  39   0.4045     89.070  0.0929    96.732  274.07
  40   0.4248     89.070  0.0866    97.008  281.03
  41   0.4108     89.520  0.0850    97.024  287.96
  42   0.3886     89.300  0.0803    97.178  294.95
  43   0.3976     89.550  0.0713    97.666  301.94
  44   0.4044     89.570  0.0695    97.586  308.89
  45   0.4247     89.530  0.0582    97.996  315.90
  46   0.4048     90.110  0.0634    97.818  322.83
  47   0.4363     89.280  0.0572    97.996  329.77
  48   0.4166     89.730  0.0501    98.318  336.76
  49   0.3979     90.100  0.0454    98.458  343.74
  50   0.4192     89.850  0.0441    98.518  350.64
  51   0.4238     89.670  0.0400    98.722  357.58
  52   0.4104     89.950  0.0392    98.686  364.53
  53   0.4225     90.220  0.0341    98.938  371.46
  54   0.4201     90.140  0.0320    98.994  378.42
  55   0.4389     89.920  0.0314    99.014  385.47
  56   0.4067     90.100  0.0276    99.148  392.43
  57   0.4108     90.370  0.0240    99.330  399.39
  58   0.4122     90.400  0.0216    99.396  406.35
  59   0.4007     90.610  0.0200    99.424  413.30
  60   0.4094     90.910  0.0186    99.476  420.31
  61   0.4052     90.630  0.0169    99.546  427.31
  62   0.4069     90.570  0.0161    99.584  434.26
  63   0.4217     90.530  0.0146    99.602  441.25
  64   0.4018     90.930  0.0142    99.648  448.23
  65   0.3989     91.040  0.0128    99.710  455.24
  66   0.4097     91.030  0.0119    99.728  462.18
  67   0.4100     90.780  0.0102    99.786  469.18
  68   0.4015     90.980  0.0095    99.812  476.17
  69   0.4165     90.890  0.0093    99.820  483.10
  70   0.4124     90.990  0.0081    99.846  490.13
  71   0.4055     91.030  0.0083    99.822  497.08
  72   0.4057     91.100  0.0073    99.876  504.04
  73   0.4083     90.980  0.0074    99.866  511.01
  74   0.4037     91.250  0.0068    99.898  517.96
  75   0.4081     91.080  0.0066    99.874  524.98
  76   0.4024     91.130  0.0064    99.884  531.94
  77   0.4006     91.300  0.0058    99.904  538.90
  78   0.4045     91.200  0.0057    99.904  545.89
  79   0.4013     91.360  0.0051    99.930  552.85
  80   0.3997     91.330  0.0052    99.940  559.81
  81   0.4023     91.180  0.0050    99.930  566.79
  82   0.3998     91.230  0.0047    99.946  573.76
  83   0.4046     91.170  0.0050    99.934  580.75
  84   0.3997     91.280  0.0047    99.944  587.70
  85   0.4082     91.180  0.0048    99.934  594.68
  86   0.4067     91.420  0.0045    99.944  601.71
  87   0.4063     91.350  0.0045    99.934  608.68
  88   0.4083     91.200  0.0048    99.944  615.67
  89   0.4070     91.320  0.0046    99.940  622.66
  90   0.4120     91.200  0.0045    99.948  629.66
