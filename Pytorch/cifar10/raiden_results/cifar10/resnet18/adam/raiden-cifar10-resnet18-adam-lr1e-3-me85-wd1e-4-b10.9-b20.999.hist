Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.1179     58.820  1.3688    49.744  7.69
   2   0.9576     67.560  0.9024    67.784  13.78
   3   0.9023     69.880  0.7090    75.040  19.89
   4   1.3499     60.720  0.6021    79.038  25.98
   5   0.7087     76.540  0.5298    81.854  32.11
   6   0.6181     79.400  0.4799    83.490  38.19
   7   0.6552     78.560  0.4350    85.102  44.33
   8   0.4661     84.330  0.4063    85.898  50.40
   9   0.5520     81.780  0.3846    86.790  56.48
  10   0.6738     78.200  0.3555    87.686  62.55
  11   0.4873     84.340  0.3291    88.480  68.65
  12   0.5549     82.210  0.3178    89.014  74.73
  13   0.7030     79.180  0.2985    89.714  80.91
  14   0.3970     86.820  0.2853    90.158  86.96
  15   0.4733     84.850  0.2685    90.760  93.06
  16   0.4401     86.450  0.2543    91.270  99.15
  17   0.3712     87.670  0.2522    91.236  105.22
  18   0.4420     86.460  0.2377    91.850  111.30
  19   0.4540     85.830  0.2238    92.336  117.43
  20   0.4280     86.530  0.2152    92.598  123.55
  21   0.3665     88.710  0.2083    92.868  129.65
  22   0.3378     88.960  0.2031    93.098  135.74
  23   0.3645     88.810  0.1926    93.232  141.82
  24   0.3513     88.970  0.1860    93.530  147.88
  25   0.4844     86.340  0.1797    93.716  154.00
  26   0.3836     87.940  0.1729    93.954  160.07
  27   0.4445     87.090  0.1681    94.120  166.14
  28   0.3163     90.530  0.1626    94.282  172.19
  29   0.4169     88.030  0.1594    94.416  178.26
  30   0.4509     87.370  0.1522    94.758  184.33
  31   0.3584     89.540  0.1503    94.622  190.42
  32   0.3308     90.120  0.1433    95.062  196.49
  33   0.3570     89.830  0.1375    95.348  202.58
  34   0.3236     89.620  0.1376    95.222  208.64
  35   0.3482     90.430  0.1336    95.338  214.68
  36   0.3843     88.770  0.1275    95.544  220.75
  37   0.3688     89.600  0.1292    95.494  226.93
  38   0.4256     88.370  0.1268    95.626  232.99
  39   0.3511     90.230  0.1212    95.824  239.11
  40   0.3691     89.500  0.1207    95.746  245.16
  41   0.3605     90.170  0.1212    95.832  251.27
  42   0.3819     89.440  0.1165    96.032  257.41
  43   0.4218     88.410  0.1144    96.064  263.47
  44   0.3835     89.570  0.1058    96.400  269.57
  45   0.3531     90.130  0.1150    95.890  275.65
  46   0.3551     90.520  0.1086    96.224  281.75
  47   0.4980     87.620  0.1044    96.346  287.86
  48   0.3770     89.900  0.1053    96.326  293.99
  49   0.3616     90.280  0.1081    96.248  300.04
  50   0.3727     89.710  0.0989    96.638  306.14
  51   0.3728     89.820  0.1012    96.582  312.24
  52   0.3586     90.230  0.0957    96.596  318.32
  53   0.3662     90.050  0.1054    96.308  324.38
  54   0.3486     90.680  0.0958    96.720  330.49
  55   0.3311     90.810  0.0971    96.644  336.58
  56   0.3997     89.340  0.0988    96.548  342.64
  57   0.3590     90.610  0.0950    96.646  348.74
  58   0.3900     89.810  0.0938    96.746  354.83
  59   0.4140     89.840  0.0886    96.956  360.93
  60   0.4060     89.070  0.0919    96.886  367.10
  61   0.3881     89.790  0.0869    96.978  373.21
  62   0.3544     90.860  0.0893    96.898  379.31
  63   0.4722     88.760  0.0903    96.882  385.43
  64   0.3829     89.890  0.0864    96.982  391.49
  65   0.3933     90.200  0.0888    96.896  397.59
  66   0.3631     90.190  0.0851    97.006  403.66
  67   0.3628     90.930  0.0894    96.856  409.79
  68   0.3634     90.960  0.0884    96.856  415.91
  69   0.4558     88.730  0.0848    97.060  421.98
  70   0.3763     90.170  0.0865    97.040  428.07
  71   0.3909     90.290  0.0810    97.086  434.14
  72   0.3547     90.750  0.0840    97.054  440.25
  73   0.3874     90.490  0.0851    97.128  446.42
  74   0.3961     90.510  0.0830    97.138  452.48
  75   0.3870     90.570  0.0782    97.248  458.58
  76   0.4054     90.060  0.0812    97.304  464.67
  77   0.3947     90.180  0.0815    97.230  470.77
  78   0.3555     90.410  0.0870    96.962  476.97
  79   0.3411     91.130  0.0774    97.268  483.04
  80   0.3709     90.810  0.0773    97.294  489.12
  81   0.4289     89.390  0.0832    97.092  495.19
  82   0.3521     91.310  0.0763    97.404  501.28
  83   0.3565     90.670  0.0841    97.054  507.39
  84   0.3442     90.890  0.0787    97.230  513.55
  85   0.3550     91.110  0.0783    97.270  519.67
