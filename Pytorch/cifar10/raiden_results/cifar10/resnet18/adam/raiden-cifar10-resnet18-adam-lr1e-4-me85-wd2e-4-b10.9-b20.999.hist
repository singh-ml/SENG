Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3259     51.860  1.5652    42.272  7.68
   2   1.1376     59.590  1.2010    56.544  13.85
   3   1.0103     64.330  1.0131    63.722  19.97
   4   1.0366     64.190  0.8922    68.246  26.14
   5   0.8983     68.800  0.7999    71.732  32.30
   6   0.7813     72.730  0.7214    74.756  38.41
   7   0.6969     75.760  0.6696    76.614  44.57
   8   0.6701     76.150  0.6136    78.484  50.69
   9   0.7189     75.440  0.5705    79.964  56.85
  10   0.6257     78.170  0.5327    81.322  62.98
  11   0.6019     79.320  0.5015    82.426  69.14
  12   0.6196     78.750  0.4739    83.412  75.34
  13   0.6453     77.870  0.4487    84.462  81.49
  14   0.5848     80.290  0.4250    85.224  87.63
  15   0.5416     81.690  0.4007    86.072  93.78
  16   0.5424     81.660  0.3795    86.830  99.91
  17   0.5444     81.900  0.3594    87.420  106.12
  18   0.5734     81.410  0.3439    87.946  112.25
  19   0.5201     82.990  0.3307    88.578  118.39
  20   0.5083     83.370  0.3147    88.956  124.52
  21   0.5964     81.470  0.3004    89.450  130.65
  22   0.4761     84.380  0.2813    90.138  136.80
  23   0.5562     82.810  0.2730    90.528  143.01
  24   0.5095     83.450  0.2576    91.076  149.13
  25   0.5624     82.910  0.2519    91.166  155.29
  26   0.5266     83.420  0.2378    91.782  161.41
  27   0.5161     84.210  0.2259    92.132  167.52
  28   0.4959     84.150  0.2201    92.270  173.67
  29   0.4986     84.810  0.2043    92.826  179.83
  30   0.4851     85.090  0.2005    92.890  185.94
  31   0.5400     84.040  0.1850    93.582  192.04
  32   0.5279     84.730  0.1755    93.882  198.17
  33   0.4983     85.560  0.1693    94.104  204.30
  34   0.4814     85.720  0.1727    93.922  210.53
  35   0.5009     85.410  0.1567    94.634  216.67
  36   0.4987     85.930  0.1492    94.832  222.83
  37   0.5483     84.150  0.1489    94.778  228.96
  38   0.5212     85.320  0.1400    95.176  235.10
  39   0.4967     85.500  0.1346    95.238  241.27
  40   0.5126     85.720  0.1278    95.542  247.46
  41   0.5040     86.320  0.1234    95.728  253.59
  42   0.4993     86.060  0.1152    95.960  259.74
  43   0.5866     84.310  0.1110    96.152  265.85
  44   0.5555     85.490  0.1061    96.238  271.98
  45   0.5318     86.000  0.1114    96.118  278.14
  46   0.5513     85.890  0.1056    96.226  284.36
  47   0.5213     86.170  0.0954    96.682  290.46
  48   0.4882     87.040  0.0957    96.660  296.58
  49   0.4989     86.750  0.0933    96.804  302.74
  50   0.5484     86.110  0.0888    96.936  308.86
  51   0.4992     86.710  0.0781    97.336  314.99
  52   0.5358     86.080  0.0884    96.914  321.17
  53   0.5226     86.400  0.0839    97.146  327.31
  54   0.5587     86.030  0.0819    97.132  333.44
  55   0.5750     85.470  0.0780    97.338  339.58
  56   0.5242     87.260  0.0725    97.532  345.71
  57   0.6738     84.820  0.0724    97.446  351.94
  58   0.5192     86.750  0.0776    97.260  358.09
  59   0.5521     86.350  0.0666    97.702  364.21
  60   0.5616     86.060  0.0703    97.582  370.35
  61   0.5211     87.090  0.0690    97.642  376.51
  62   0.5547     86.940  0.0646    97.778  382.71
  63   0.5642     86.490  0.0663    97.778  388.85
  64   0.5412     86.610  0.0645    97.812  394.98
  65   0.5427     87.090  0.0601    97.994  401.09
  66   0.5418     87.660  0.0605    97.912  407.22
  67   0.5595     86.680  0.0580    98.008  413.33
  68   0.5405     86.630  0.0598    97.900  419.53
  69   0.5988     86.290  0.0587    97.984  425.67
  70   0.5376     87.090  0.0551    98.048  431.81
  71   0.5516     86.970  0.0531    98.158  437.97
  72   0.5615     87.320  0.0538    98.144  444.08
  73   0.5154     87.740  0.0535    98.172  450.27
  74   0.5317     87.880  0.0524    98.148  456.42
  75   0.5766     86.830  0.0496    98.322  462.54
  76   0.5388     87.660  0.0522    98.216  468.70
  77   0.5851     86.990  0.0524    98.230  474.83
  78   0.5636     87.170  0.0493    98.364  481.03
  79   0.6626     85.860  0.0494    98.314  487.15
  80   0.5499     87.240  0.0491    98.360  493.27
  81   0.6094     86.150  0.0491    98.334  499.44
  82   0.5434     87.400  0.0431    98.562  505.64
  83   0.5835     87.320  0.0430    98.538  511.80
  84   0.5292     87.930  0.0459    98.446  518.02
  85   0.5654     87.610  0.0450    98.534  524.13
