Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8216     33.820  1.9094    29.590  7.71
   2   1.4118     48.580  1.5093    43.856  13.80
   3   1.2983     52.820  1.3106    51.810  19.91
   4   1.2015     57.790  1.1381    58.996  26.02
   5   1.0548     63.010  1.0176    63.742  32.10
   6   0.8948     69.020  0.9056    67.770  38.23
   7   0.8160     71.190  0.7981    72.126  44.30
   8   0.7807     72.970  0.7327    74.438  50.38
   9   0.7275     75.150  0.6724    76.540  56.45
  10   0.6971     75.900  0.6418    77.632  62.58
  11   0.6456     77.760  0.6115    78.722  68.76
  12   0.8201     73.260  0.6027    79.178  74.84
  13   0.6338     78.220  0.5812    79.836  80.96
  14   0.6307     77.780  0.5660    80.572  87.08
  15   0.5911     79.390  0.5676    80.458  93.17
  16   0.5971     79.480  0.5477    81.180  99.30
  17   0.6779     77.880  0.5394    81.346  105.46
  18   0.6905     77.560  0.5368    81.430  111.54
  19   0.6125     79.340  0.5211    81.944  117.63
  20   0.6098     79.300  0.5203    82.160  123.74
  21   0.6711     77.940  0.5137    82.346  129.82
  22   0.6027     78.880  0.5011    82.750  135.95
  23   0.6291     78.810  0.5056    82.620  142.08
  24   0.5501     81.080  0.5005    82.790  148.16
  25   0.7411     76.160  0.4942    83.042  154.29
  26   0.5235     81.880  0.4940    83.156  160.42
  27   0.7164     77.660  0.4839    83.246  166.52
  28   0.5877     80.660  0.4919    83.006  172.67
  29   0.5883     80.730  0.4800    83.600  178.77
  30   0.5242     82.420  0.4778    83.624  184.89
  31   0.5976     80.720  0.4818    83.448  190.98
  32   0.5843     80.130  0.4752    83.662  197.11
  33   0.5227     82.080  0.4739    83.696  203.31
  34   0.5393     82.000  0.4747    83.650  209.42
  35   0.5841     80.850  0.4695    83.978  215.51
  36   0.5718     81.910  0.4644    83.920  221.60
  37   0.5251     82.850  0.4615    84.078  227.71
  38   0.6132     79.700  0.4641    84.118  233.78
  39   0.5571     81.710  0.4592    84.244  239.91
  40   0.4991     83.270  0.4665    84.024  246.04
  41   0.5996     80.310  0.4593    84.134  252.14
  42   0.5523     81.640  0.4605    84.160  258.26
  43   0.5455     82.390  0.4506    84.348  264.39
  44   0.6342     78.910  0.4549    84.402  270.58
  45   0.5273     82.410  0.4520    84.568  276.65
  46   0.6150     79.390  0.4585    84.058  282.77
  47   0.5737     81.020  0.4551    84.458  288.84
  48   0.6018     79.900  0.4542    84.652  294.94
  49   0.4944     83.400  0.4443    84.712  301.08
  50   0.4930     83.130  0.4457    84.714  307.16
  51   0.5820     80.800  0.4471    84.496  313.27
  52   0.5018     82.900  0.4444    84.696  319.35
  53   0.5372     81.550  0.4414    84.822  325.43
  54   0.5256     82.350  0.4478    84.826  331.53
  55   0.5754     80.770  0.4412    84.818  337.61
  56   0.5577     81.180  0.4402    85.024  343.68
  57   0.5219     82.450  0.4484    84.540  349.75
  58   0.6011     80.690  0.4402    84.862  355.85
  59   0.4813     83.670  0.4347    85.146  361.94
  60   0.5205     82.680  0.4363    85.142  368.11
  61   0.5309     82.620  0.4401    84.832  374.23
  62   0.5314     82.320  0.4394    84.702  380.40
  63   0.6079     79.880  0.4425    84.918  386.50
  64   0.5271     82.620  0.4397    84.850  392.60
  65   0.5435     81.840  0.4338    85.344  398.72
  66   0.5214     82.570  0.4396    85.010  404.79
  67   0.5604     81.820  0.4256    85.476  410.90
  68   0.5709     81.010  0.4403    84.840  417.04
  69   0.5521     81.690  0.4345    85.130  423.15
  70   0.5052     82.950  0.4356    85.098  429.22
  71   0.5356     81.930  0.4304    85.198  435.30
  72   0.5006     83.340  0.4321    85.266  441.53
  73   0.5851     80.600  0.4247    85.540  447.65
  74   0.5210     82.500  0.4396    84.788  453.73
  75   0.5672     80.720  0.4308    85.210  459.84
  76   0.4950     83.520  0.4364    85.060  465.91
  77   0.5138     82.570  0.4334    85.038  472.06
  78   0.5368     82.550  0.4233    85.382  478.14
  79   0.5464     81.650  0.4276    85.304  484.22
  80   0.4934     83.680  0.4276    85.238  490.32
  81   0.5437     82.110  0.4312    85.236  496.44
  82   0.5767     81.080  0.4298    85.282  502.55
  83   0.5168     82.950  0.4282    85.246  508.77
  84   0.6056     80.730  0.4257    85.428  514.91
  85   0.5246     82.740  0.4218    85.660  521.06
