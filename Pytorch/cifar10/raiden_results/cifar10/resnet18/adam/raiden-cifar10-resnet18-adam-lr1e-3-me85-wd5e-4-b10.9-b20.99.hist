Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4859     50.710  1.4072    48.660  7.99
   2   1.1354     61.830  0.9499    66.198  14.17
   3   1.0214     67.040  0.7720    73.364  20.35
   4   0.9298     70.190  0.6667    77.066  26.50
   5   0.8418     71.910  0.5949    79.618  32.67
   6   0.7539     74.600  0.5455    81.422  38.88
   7   0.6342     78.960  0.4944    83.052  45.01
   8   0.7393     76.490  0.4640    84.296  51.17
   9   0.6712     78.670  0.4293    85.376  57.29
  10   0.6098     80.550  0.4079    86.040  63.46
  11   0.5133     83.160  0.3863    86.664  69.69
  12   0.6295     80.640  0.3754    87.172  75.84
  13   0.8088     76.310  0.3515    88.080  82.01
  14   0.6156     80.100  0.3443    88.268  88.18
  15   0.4857     84.390  0.3268    89.012  94.35
  16   0.4529     85.150  0.3151    89.444  100.48
  17   0.4898     84.380  0.3081    89.522  106.79
  18   0.7218     78.520  0.2971    89.930  112.96
  19   0.4752     84.580  0.2833    90.324  119.14
  20   0.4725     84.660  0.2809    90.316  125.32
  21   0.4733     84.790  0.2689    90.954  131.48
  22   0.5155     83.470  0.2640    90.800  137.67
  23   0.4602     85.670  0.2617    91.058  143.80
  24   0.5348     84.460  0.2535    91.398  149.95
  25   0.4544     86.000  0.2471    91.668  156.09
  26   0.4556     86.080  0.2427    91.698  162.23
  27   0.4706     85.760  0.2366    91.974  168.39
  28   0.3878     87.670  0.2359    91.946  174.53
  29   0.3745     88.170  0.2299    92.028  180.69
  30   0.3919     87.910  0.2283    92.170  186.82
  31   0.4180     86.930  0.2252    92.264  192.97
  32   0.4195     87.070  0.2169    92.596  199.11
  33   0.3547     88.620  0.2216    92.538  205.27
  34   0.3832     87.830  0.2162    92.666  211.46
  35   0.4829     85.560  0.2030    93.072  217.67
  36   0.4234     86.640  0.2055    93.036  223.82
  37   0.4679     86.090  0.2070    92.868  230.01
  38   0.3846     88.210  0.2070    93.012  236.16
  39   0.4054     87.650  0.2008    93.136  242.34
  40   0.3811     87.780  0.1969    93.310  248.57
  41   0.3857     88.480  0.1961    93.312  254.72
  42   0.3551     89.310  0.1950    93.378  260.89
  43   0.6137     82.290  0.1919    93.398  267.06
  44   0.4894     85.590  0.1888    93.514  273.18
  45   0.3677     89.070  0.1912    93.446  279.32
  46   0.3737     88.820  0.1839    93.764  285.53
  47   0.3347     89.530  0.1864    93.658  291.69
  48   0.3857     88.050  0.1814    93.788  297.85
  49   0.3135     90.070  0.1823    93.730  304.00
  50   0.4135     87.580  0.1783    93.974  310.12
  51   0.3387     89.570  0.1784    93.816  316.36
  52   0.3999     88.320  0.1812    93.762  322.52
  53   0.3641     89.110  0.1745    93.996  328.66
  54   0.4141     87.890  0.1773    93.954  334.79
  55   0.3450     89.050  0.1725    94.104  340.95
  56   0.3849     88.330  0.1747    93.962  347.12
  57   0.3483     89.180  0.1684    94.300  353.27
  58   0.3875     88.510  0.1714    94.158  359.45
  59   0.3740     88.870  0.1688    94.226  365.60
  60   0.3393     89.620  0.1705    94.236  371.72
  61   0.3623     88.860  0.1675    94.332  377.87
  62   0.3567     88.520  0.1643    94.352  384.05
  63   0.3719     88.370  0.1675    94.290  390.28
  64   0.3658     88.810  0.1645    94.374  396.46
  65   0.3530     89.130  0.1635    94.328  402.60
  66   0.4213     87.940  0.1596    94.444  408.74
  67   0.3271     89.990  0.1642    94.400  414.88
  68   0.3565     89.210  0.1584    94.466  421.15
  69   0.3616     89.310  0.1615    94.588  427.31
  70   0.3420     89.420  0.1592    94.552  433.48
  71   0.3304     89.580  0.1650    94.278  439.61
  72   0.3490     89.720  0.1527    94.806  445.76
  73   0.3691     88.810  0.1614    94.358  451.95
  74   0.4725     87.180  0.1551    94.702  458.09
  75   0.3256     89.940  0.1557    94.644  464.23
  76   0.3996     88.300  0.1562    94.712  470.37
  77   0.3420     90.100  0.1558    94.698  476.55
  78   0.3635     88.940  0.1559    94.728  482.70
  79   0.3606     89.250  0.1527    94.880  488.87
  80   0.3254     90.140  0.1569    94.664  495.09
  81   0.4298     87.560  0.1509    94.792  501.26
  82   0.3890     88.750  0.1560    94.586  507.43
  83   0.3892     88.870  0.1506    94.834  513.59
  84   0.4055     88.210  0.1545    94.684  519.77
  85   0.3721     89.050  0.1547    94.756  526.01
