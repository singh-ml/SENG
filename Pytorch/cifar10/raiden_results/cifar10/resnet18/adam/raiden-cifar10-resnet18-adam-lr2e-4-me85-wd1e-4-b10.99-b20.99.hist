Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3208     51.490  1.5593    42.360  7.58
   2   1.1096     60.820  1.1750    57.684  13.74
   3   0.9021     67.920  0.9743    64.950  19.96
   4   0.7966     72.050  0.8304    70.710  26.05
   5   0.7257     74.680  0.7197    74.468  32.12
   6   0.6758     76.970  0.6485    77.114  38.22
   7   0.6170     78.670  0.5772    79.598  44.36
   8   0.5999     79.890  0.5341    81.380  50.51
   9   0.5422     81.300  0.4937    82.858  56.57
  10   0.5387     81.790  0.4549    84.162  62.69
  11   0.5189     82.790  0.4255    85.134  68.80
  12   0.5086     83.170  0.3978    86.154  74.87
  13   0.4827     84.020  0.3671    87.428  80.93
  14   0.5008     83.450  0.3427    88.014  86.99
  15   0.4534     85.270  0.3257    88.580  93.12
  16   0.4408     85.510  0.3032    89.354  99.19
  17   0.4244     86.350  0.2837    90.078  105.26
  18   0.4346     86.430  0.2633    90.820  111.37
  19   0.4269     86.160  0.2560    91.090  117.46
  20   0.4176     86.760  0.2426    91.528  123.59
  21   0.4262     86.720  0.2312    91.874  129.68
  22   0.4271     86.580  0.2183    92.316  135.75
  23   0.4435     86.330  0.2060    92.862  141.88
  24   0.4274     87.170  0.1967    93.124  147.95
  25   0.4230     87.430  0.1822    93.554  154.02
  26   0.4324     87.170  0.1791    93.594  160.08
  27   0.4163     87.760  0.1708    93.880  166.25
  28   0.4006     88.130  0.1654    94.112  172.37
  29   0.4205     87.790  0.1599    94.416  178.44
  30   0.4134     87.760  0.1466    94.842  184.52
  31   0.4116     87.720  0.1384    95.106  190.58
  32   0.4095     88.540  0.1354    95.260  196.66
  33   0.4262     87.910  0.1310    95.350  202.78
  34   0.4365     87.550  0.1258    95.568  208.85
  35   0.4090     88.600  0.1243    95.620  214.93
  36   0.4346     87.970  0.1098    96.250  221.06
  37   0.4393     88.580  0.1101    96.116  227.15
  38   0.3882     89.240  0.1074    96.228  233.25
  39   0.4378     88.250  0.1097    96.086  239.38
  40   0.4256     88.860  0.0991    96.506  245.47
  41   0.4244     89.200  0.0965    96.526  251.55
  42   0.4303     88.610  0.1005    96.470  257.66
  43   0.4434     88.770  0.0855    96.962  263.75
  44   0.4196     89.020  0.0853    97.048  269.91
  45   0.4208     89.330  0.0883    96.892  275.99
  46   0.4023     89.680  0.0797    97.228  282.09
  47   0.4485     89.040  0.0762    97.328  288.18
  48   0.4395     89.420  0.0823    97.054  294.26
  49   0.4297     88.910  0.0774    97.306  300.38
  50   0.4124     89.620  0.0742    97.360  306.49
  51   0.4393     89.420  0.0744    97.390  312.57
  52   0.4391     89.210  0.0681    97.582  318.62
  53   0.4544     88.960  0.0653    97.758  324.67
  54   0.4183     89.560  0.0688    97.632  330.77
  55   0.4123     89.810  0.0630    97.866  336.86
  56   0.4584     89.260  0.0606    97.820  343.08
  57   0.4313     89.250  0.0698    97.534  349.16
  58   0.4093     89.710  0.0654    97.746  355.21
  59   0.4460     89.390  0.0661    97.690  361.27
  60   0.4422     89.910  0.0630    97.782  367.31
  61   0.4455     90.160  0.0580    98.006  373.42
  62   0.4595     89.580  0.0551    98.032  379.48
  63   0.4223     90.370  0.0532    98.164  385.55
  64   0.4342     90.150  0.0583    97.934  391.63
  65   0.4452     89.910  0.0518    98.196  397.68
  66   0.4391     89.810  0.0541    98.064  403.73
  67   0.4332     90.070  0.0550    98.036  409.80
  68   0.4366     89.920  0.0539    98.140  415.94
  69   0.4201     90.230  0.0506    98.240  422.01
  70   0.4715     89.500  0.0528    98.150  428.09
  71   0.4465     89.820  0.0542    98.136  434.19
  72   0.4430     90.160  0.0481    98.296  440.24
  73   0.4469     89.640  0.0506    98.264  446.32
  74   0.4342     89.850  0.0459    98.396  452.41
  75   0.4485     90.320  0.0497    98.256  458.57
  76   0.4487     90.230  0.0479    98.352  464.64
  77   0.4423     89.970  0.0460    98.344  470.69
  78   0.4466     89.870  0.0427    98.516  476.74
  79   0.4320     90.240  0.0446    98.442  482.82
  80   0.4317     90.270  0.0449    98.420  488.93
  81   0.4204     90.660  0.0402    98.596  495.06
  82   0.4318     90.230  0.0405    98.586  501.13
  83   0.4483     90.390  0.0405    98.646  507.20
  84   0.4658     90.050  0.0394    98.628  513.31
  85   0.4195     90.270  0.0456    98.430  519.38
