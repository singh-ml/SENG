Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2832     54.070  1.4858    45.144  7.67
   2   1.0054     65.150  1.0757    61.374  13.76
   3   0.8225     70.640  0.8796    68.696  19.90
   4   0.8248     72.830  0.7391    73.892  26.04
   5   0.7843     73.780  0.6415    77.582  32.19
   6   0.6412     77.990  0.5846    79.430  38.35
   7   0.5792     80.150  0.5279    81.722  44.46
   8   0.5944     79.520  0.4866    82.976  50.56
   9   0.6008     79.900  0.4495    84.270  56.66
  10   0.5509     81.540  0.4170    85.490  62.81
  11   0.5542     81.780  0.3921    86.168  68.94
  12   0.5054     83.380  0.3658    87.310  75.07
  13   0.5285     83.350  0.3474    87.860  81.15
  14   0.4638     84.820  0.3228    88.848  87.26
  15   0.5467     82.590  0.3039    89.610  93.35
  16   0.5294     83.930  0.2839    90.178  99.51
  17   0.4397     85.860  0.2742    90.506  105.63
  18   0.4952     84.840  0.2560    91.066  111.74
  19   0.4610     85.890  0.2449    91.390  117.88
  20   0.5450     83.650  0.2304    91.868  123.98
  21   0.4957     84.930  0.2207    92.254  130.06
  22   0.4692     85.910  0.2074    92.792  136.22
  23   0.4117     86.820  0.1954    93.214  142.33
  24   0.4348     86.190  0.1905    93.412  148.44
  25   0.4309     87.270  0.1785    93.818  154.57
  26   0.4376     87.260  0.1756    93.870  160.68
  27   0.4947     85.960  0.1654    94.154  166.80
  28   0.5642     84.310  0.1556    94.438  173.04
  29   0.4574     86.910  0.1521    94.654  179.14
  30   0.4434     87.850  0.1444    94.920  185.31
  31   0.5427     85.760  0.1393    95.106  191.46
  32   0.4588     87.320  0.1325    95.434  197.56
  33   0.5731     85.350  0.1304    95.396  203.76
  34   0.4328     87.590  0.1280    95.460  209.89
  35   0.4215     88.100  0.1160    95.902  215.99
  36   0.4367     87.600  0.1177    95.830  222.12
  37   0.4160     88.490  0.1141    95.988  228.26
  38   0.4197     88.620  0.1068    96.298  234.44
  39   0.4744     87.440  0.1069    96.262  240.51
  40   0.4241     88.760  0.1059    96.270  246.65
  41   0.4559     88.530  0.0958    96.750  252.73
  42   0.4845     87.740  0.0982    96.536  258.84
  43   0.4224     88.410  0.0907    96.858  265.02
  44   0.5065     87.420  0.0907    96.804  271.11
  45   0.5039     86.630  0.0858    97.028  277.25
  46   0.5747     86.350  0.0870    96.974  283.33
  47   0.4393     87.830  0.0860    96.998  289.45
  48   0.4678     88.570  0.0798    97.218  295.55
  49   0.4440     88.760  0.0812    97.086  301.68
  50   0.5080     87.660  0.0795    97.214  307.81
  51   0.4185     88.520  0.0744    97.528  313.94
  52   0.4976     87.740  0.0719    97.490  320.12
  53   0.4019     89.790  0.0759    97.314  326.29
  54   0.4292     89.480  0.0729    97.434  332.51
  55   0.4303     89.480  0.0720    97.490  338.63
  56   0.4510     88.270  0.0701    97.506  344.76
  57   0.4334     89.700  0.0663    97.644  350.90
  58   0.5756     87.490  0.0689    97.600  357.01
  59   0.5160     87.990  0.0674    97.676  363.11
  60   0.4874     87.840  0.0631    97.846  369.23
  61   0.4570     88.430  0.0645    97.770  375.31
  62   0.4293     89.540  0.0592    97.986  381.42
  63   0.4329     89.330  0.0601    97.888  387.55
  64   0.4518     88.600  0.0621    97.868  393.71
  65   0.4628     88.910  0.0607    97.910  399.82
  66   0.4622     88.650  0.0618    97.924  405.98
  67   0.4844     88.630  0.0561    98.106  412.11
  68   0.4310     89.590  0.0586    97.986  418.23
  69   0.4100     89.660  0.0555    98.092  424.35
  70   0.5037     87.620  0.0561    98.072  430.44
  71   0.4655     88.750  0.0540    98.128  436.55
  72   0.4338     89.790  0.0573    98.068  442.77
  73   0.4703     89.500  0.0505    98.286  448.87
  74   0.4290     89.670  0.0516    98.238  455.00
  75   0.5198     87.960  0.0527    98.086  461.13
  76   0.4958     88.660  0.0513    98.182  467.26
  77   0.4626     89.240  0.0536    98.180  473.40
  78   0.4880     88.760  0.0506    98.228  479.58
  79   0.4670     89.180  0.0494    98.300  485.71
  80   0.4078     90.040  0.0481    98.384  491.82
  81   0.5434     88.060  0.0479    98.398  497.93
  82   0.5266     87.870  0.0477    98.356  504.07
  83   0.4295     89.420  0.0507    98.210  510.23
  84   0.4455     89.750  0.0501    98.224  516.39
  85   0.4722     88.980  0.0464    98.434  522.50
