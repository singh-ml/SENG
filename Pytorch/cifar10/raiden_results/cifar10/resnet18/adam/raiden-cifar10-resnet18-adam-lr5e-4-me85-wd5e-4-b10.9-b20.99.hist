Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.1903     58.960  1.3828    49.560  7.73
   2   0.9697     66.300  0.9397    66.620  13.82
   3   0.9107     69.900  0.7378    74.268  19.95
   4   0.7439     74.550  0.6363    78.056  26.06
   5   0.6966     76.420  0.5661    80.546  32.18
   6   0.6869     77.880  0.5113    82.552  38.27
   7   0.5496     81.420  0.4712    83.738  44.35
   8   0.5371     82.160  0.4317    85.306  50.49
   9   0.5021     82.780  0.4123    85.866  56.56
  10   0.4906     83.610  0.3854    86.860  62.74
  11   0.5508     82.080  0.3622    87.708  68.85
  12   0.5312     82.920  0.3404    88.458  74.92
  13   0.5769     81.890  0.3280    88.612  81.04
  14   0.5016     83.780  0.3059    89.490  87.18
  15   0.5105     83.750  0.2967    89.942  93.28
  16   0.4509     85.870  0.2798    90.342  99.47
  17   0.7200     80.070  0.2670    90.876  105.56
  18   0.4767     85.450  0.2575    91.146  111.71
  19   0.4027     87.140  0.2573    91.318  117.79
  20   0.4038     87.290  0.2408    91.808  123.93
  21   0.4082     87.190  0.2321    92.000  130.04
  22   0.5215     84.320  0.2192    92.536  136.18
  23   0.4742     85.400  0.2226    92.396  142.26
  24   0.4656     85.480  0.2120    92.776  148.35
  25   0.5368     83.570  0.2029    93.050  154.49
  26   0.3724     87.930  0.2009    93.176  160.57
  27   0.4436     86.750  0.1947    93.382  166.65
  28   0.4527     86.580  0.1856    93.716  172.84
  29   0.4003     87.980  0.1795    93.772  178.94
  30   0.4846     86.060  0.1781    93.934  185.06
  31   0.3712     88.420  0.1769    93.838  191.19
  32   0.3463     89.880  0.1690    94.216  197.33
  33   0.3897     88.130  0.1673    94.220  203.50
  34   0.3736     89.010  0.1597    94.644  209.59
  35   0.3895     88.450  0.1585    94.620  215.73
  36   0.3715     88.370  0.1534    94.826  221.86
  37   0.3936     88.380  0.1521    94.900  227.96
  38   0.3777     88.500  0.1536    94.754  234.17
  39   0.3355     89.480  0.1482    94.906  240.28
  40   0.3585     88.930  0.1474    94.916  246.42
  41   0.3791     88.900  0.1419    95.134  252.51
  42   0.3617     89.490  0.1378    95.242  258.64
  43   0.3770     89.250  0.1341    95.416  264.72
  44   0.3622     89.430  0.1350    95.430  270.96
  45   0.3311     90.010  0.1320    95.562  277.04
  46   0.3809     88.670  0.1326    95.492  283.13
  47   0.3705     89.360  0.1267    95.658  289.23
  48   0.3976     89.050  0.1280    95.682  295.35
  49   0.4040     89.100  0.1253    95.704  301.53
  50   0.4424     87.680  0.1266    95.702  307.61
  51   0.3626     89.640  0.1224    95.756  313.72
  52   0.3805     89.510  0.1206    95.828  319.85
  53   0.3926     88.730  0.1206    96.012  325.98
  54   0.3486     89.870  0.1204    95.856  332.14
  55   0.4110     88.820  0.1152    95.966  338.27
  56   0.3678     89.510  0.1176    95.966  344.37
  57   0.3399     90.370  0.1130    96.102  350.47
  58   0.3379     90.220  0.1174    95.906  356.62
  59   0.3941     89.000  0.1077    96.306  362.72
  60   0.4208     88.890  0.1094    96.258  368.87
  61   0.3410     90.720  0.1129    96.158  374.97
  62   0.4600     88.370  0.1097    96.222  381.08
  63   0.3744     89.850  0.1079    96.332  387.21
  64   0.3772     89.500  0.1103    96.214  393.31
  65   0.3995     88.950  0.1047    96.524  399.41
  66   0.3709     89.340  0.1087    96.348  405.58
  67   0.3817     89.780  0.0969    96.690  411.71
  68   0.4039     89.210  0.1068    96.388  417.80
  69   0.3842     90.300  0.1021    96.444  423.91
  70   0.3451     90.670  0.1016    96.506  430.02
  71   0.3546     90.210  0.1011    96.588  436.12
  72   0.3790     89.840  0.0969    96.788  442.31
  73   0.3722     89.620  0.1001    96.620  448.41
  74   0.3566     90.110  0.1048    96.442  454.51
  75   0.3554     89.790  0.1008    96.588  460.63
  76   0.4236     88.730  0.0971    96.778  466.72
  77   0.3273     91.170  0.0919    96.806  472.86
  78   0.3858     89.680  0.0968    96.622  478.97
  79   0.3692     90.090  0.0981    96.548  485.11
  80   0.3750     89.920  0.0923    96.860  491.25
  81   0.3661     89.950  0.0957    96.840  497.36
  82   0.4035     88.940  0.0977    96.760  503.50
  83   0.3819     90.000  0.0925    96.888  509.69
  84   0.3720     90.530  0.0972    96.800  515.79
  85   0.3989     89.680  0.0905    96.906  522.00
