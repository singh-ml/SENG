Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2311     56.360  1.4788    45.742  7.63
   2   1.0838     62.620  1.0676    61.682  13.71
   3   0.9036     68.850  0.8820    68.596  19.82
   4   0.8218     72.180  0.7458    73.756  25.90
   5   0.7567     75.510  0.6445    77.396  31.99
   6   0.6689     77.340  0.5839    79.500  38.07
   7   0.6401     78.580  0.5334    81.468  44.14
   8   0.5622     80.980  0.4849    83.046  50.22
   9   0.5480     82.120  0.4497    84.244  56.31
  10   0.5679     81.370  0.4190    85.578  62.36
  11   0.5332     82.510  0.3919    86.334  68.43
  12   0.5059     83.060  0.3677    87.074  74.51
  13   0.4735     84.440  0.3430    88.034  80.63
  14   0.5322     83.080  0.3233    88.774  86.73
  15   0.4875     83.390  0.3057    89.302  92.89
  16   0.4648     85.010  0.2863    90.038  98.96
  17   0.4476     85.700  0.2704    90.622  105.06
  18   0.4574     85.370  0.2570    90.988  111.15
  19   0.4776     85.290  0.2423    91.544  117.21
  20   0.5394     83.910  0.2310    91.958  123.29
  21   0.4682     85.740  0.2172    92.484  129.40
  22   0.5290     84.420  0.2031    92.918  135.48
  23   0.5521     84.420  0.2007    92.976  141.56
  24   0.4143     87.750  0.1860    93.400  147.69
  25   0.4704     86.260  0.1778    93.690  153.80
  26   0.4379     86.920  0.1722    93.862  160.00
  27   0.4557     86.380  0.1633    94.280  166.14
  28   0.4576     86.910  0.1565    94.548  172.24
  29   0.5418     84.930  0.1481    94.794  178.32
  30   0.4637     86.900  0.1408    95.070  184.39
  31   0.4191     88.300  0.1305    95.408  190.47
  32   0.4607     86.930  0.1296    95.288  196.62
  33   0.4924     87.380  0.1219    95.734  202.69
  34   0.5438     85.570  0.1283    95.464  208.77
  35   0.5139     85.930  0.1151    95.942  214.89
  36   0.4569     87.580  0.1089    96.160  220.98
  37   0.4367     88.740  0.1048    96.366  227.16
  38   0.7270     83.660  0.0953    96.612  233.27
  39   0.4384     88.320  0.1037    96.350  239.39
  40   0.4265     88.600  0.0972    96.532  245.48
  41   0.4739     87.290  0.0949    96.596  251.54
  42   0.4397     88.590  0.0857    97.006  257.67
  43   0.4230     89.010  0.0899    96.792  263.79
  44   0.4312     88.660  0.0817    97.144  269.88
  45   0.4513     88.090  0.0855    96.964  275.98
  46   0.4861     88.190  0.0785    97.330  282.07
  47   0.4551     88.270  0.0807    97.096  288.18
  48   0.4945     87.660  0.0735    97.398  294.32
  49   0.4389     89.200  0.0770    97.260  300.39
  50   0.4832     87.630  0.0686    97.584  306.45
  51   0.5456     87.670  0.0649    97.726  312.55
  52   0.5652     86.940  0.0679    97.654  318.63
  53   0.3984     89.770  0.0614    97.862  324.73
  54   0.4929     88.190  0.0669    97.706  330.88
  55   0.4577     89.100  0.0671    97.638  336.93
  56   0.4525     89.300  0.0623    97.872  343.01
  57   0.5218     88.370  0.0607    97.846  349.12
  58   0.4605     88.890  0.0564    98.056  355.20
  59   0.4405     89.520  0.0633    97.826  361.26
  60   0.4696     89.020  0.0587    97.968  367.38
  61   0.4282     89.960  0.0568    97.998  373.45
  62   0.4672     89.020  0.0558    98.116  379.54
  63   0.4659     89.340  0.0566    98.048  385.62
  64   0.4829     89.110  0.0562    97.976  391.72
  65   0.4196     90.080  0.0544    98.072  397.83
  66   0.4504     89.230  0.0523    98.222  403.97
  67   0.4930     88.780  0.0553    98.020  410.04
  68   0.4454     89.620  0.0495    98.248  416.15
  69   0.4227     89.780  0.0551    98.110  422.25
  70   0.4560     89.090  0.0449    98.486  428.32
  71   0.4930     88.170  0.0524    98.188  434.40
  72   0.5183     88.330  0.0465    98.422  440.53
  73   0.4503     89.760  0.0459    98.404  446.62
  74   0.4438     89.460  0.0447    98.468  452.71
  75   0.4881     89.430  0.0479    98.404  458.84
  76   0.4412     89.810  0.0436    98.502  464.92
  77   0.4313     90.080  0.0514    98.234  471.00
  78   0.4405     90.210  0.0435    98.484  477.12
  79   0.4312     89.880  0.0427    98.580  483.19
  80   0.4301     89.580  0.0480    98.378  489.30
  81   0.4292     90.420  0.0443    98.480  495.38
  82   0.4404     89.750  0.0389    98.666  501.47
  83   0.4422     90.020  0.0452    98.476  507.55
  84   0.4617     89.970  0.0382    98.738  513.67
  85   0.4202     90.360  0.0407    98.620  519.78
  86   0.4427     89.750  0.0448    98.358  525.88
  87   0.4477     90.070  0.0413    98.596  532.00
  88   0.4957     89.110  0.0400    98.622  538.12
  89   0.4461     89.890  0.0384    98.736  544.27
  90   0.4923     89.170  0.0412    98.580  550.36
