Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8344     28.870  1.8968    29.514  7.72
   2   1.4615     45.980  1.4894    44.480  13.82
   3   1.2763     55.130  1.2595    53.556  19.92
   4   1.1598     60.130  1.0913    60.826  26.14
   5   0.9365     66.290  0.9450    66.382  32.24
   6   0.9151     67.970  0.8504    69.780  38.32
   7   0.8546     71.020  0.7701    72.858  44.47
   8   0.7164     75.230  0.6894    75.918  50.57
   9   0.6836     76.780  0.6314    78.014  56.78
  10   0.6327     78.010  0.5805    79.834  62.88
  11   0.6340     78.890  0.5487    80.982  68.96
  12   0.5799     79.770  0.5270    81.940  75.04
  13   0.6017     80.380  0.5138    82.230  81.18
  14   0.5646     81.140  0.4879    83.116  87.29
  15   0.5943     80.500  0.4787    83.518  93.48
  16   0.5277     82.390  0.4688    83.918  99.57
  17   0.5696     80.530  0.4533    84.382  105.66
  18   0.5202     82.960  0.4443    84.688  111.81
  19   0.5676     81.300  0.4328    85.134  117.94
  20   0.5150     82.920  0.4271    85.366  124.02
  21   0.5425     81.900  0.4224    85.424  130.18
  22   0.4600     84.580  0.4128    85.752  136.31
  23   0.5172     82.730  0.4071    86.056  142.41
  24   0.4883     84.100  0.4044    85.980  148.50
  25   0.5350     82.310  0.4003    86.162  154.62
  26   0.5337     83.020  0.3943    86.592  160.72
  27   0.5150     83.590  0.3829    86.776  166.89
  28   0.4582     84.410  0.3814    86.956  172.97
  29   0.5118     83.050  0.3805    86.896  179.07
  30   0.4401     85.430  0.3777    86.976  185.17
  31   0.4729     84.060  0.3726    87.214  191.29
  32   0.4405     85.310  0.3673    87.352  197.41
  33   0.4509     84.650  0.3687    87.356  203.62
  34   0.4389     85.090  0.3706    87.312  209.71
  35   0.4232     85.990  0.3693    87.280  215.81
  36   0.4617     84.280  0.3647    87.386  221.89
  37   0.4612     84.810  0.3638    87.476  227.98
  38   0.4924     84.310  0.3577    87.656  234.12
  39   0.4354     85.650  0.3614    87.546  240.27
  40   0.4418     85.590  0.3577    87.676  246.37
  41   0.4583     85.190  0.3584    87.654  252.49
  42   0.4522     85.300  0.3575    87.748  258.59
  43   0.4629     85.120  0.3505    87.952  264.68
  44   0.4719     84.530  0.3425    88.174  270.86
  45   0.4625     85.010  0.3456    88.130  276.94
  46   0.4693     85.370  0.3373    88.412  283.05
  47   0.4410     85.740  0.3471    88.054  289.17
  48   0.4851     84.450  0.3435    88.178  295.26
  49   0.4255     86.290  0.3498    87.942  301.34
  50   0.4454     85.550  0.3428    88.250  307.53
  51   0.4527     85.220  0.3436    88.300  313.60
  52   0.4873     84.160  0.3396    88.378  319.70
  53   0.4472     85.750  0.3394    88.348  325.80
  54   0.4500     85.540  0.3379    88.366  331.89
  55   0.4864     84.380  0.3361    88.382  338.08
  56   0.5107     83.780  0.3346    88.452  344.17
  57   0.4232     86.010  0.3427    88.244  350.27
  58   0.4469     85.030  0.3357    88.518  356.37
  59   0.4479     85.930  0.3323    88.584  362.49
  60   0.4360     85.880  0.3312    88.644  368.66
  61   0.4482     85.590  0.3331    88.634  374.86
  62   0.4244     86.360  0.3285    88.766  380.97
  63   0.4261     86.240  0.3244    88.816  387.11
  64   0.4169     85.890  0.3330    88.398  393.23
  65   0.3895     86.720  0.3327    88.476  399.33
  66   0.4353     85.830  0.3260    88.730  405.49
  67   0.3945     86.720  0.3250    88.730  411.64
  68   0.4666     84.110  0.3263    88.752  417.77
  69   0.4843     85.250  0.3244    88.930  423.91
  70   0.4242     86.020  0.3267    88.706  430.01
  71   0.4356     85.540  0.3208    88.958  436.13
  72   0.3917     86.950  0.3253    88.866  442.26
  73   0.4412     85.660  0.3295    88.734  448.37
  74   0.4615     85.350  0.3250    88.780  454.45
  75   0.4274     85.980  0.3204    88.978  460.56
  76   0.4242     85.970  0.3168    89.082  466.68
  77   0.4174     86.370  0.3190    89.082  472.79
  78   0.4651     85.380  0.3245    88.842  478.92
  79   0.4174     86.110  0.3205    88.914  485.13
  80   0.4118     86.850  0.3169    89.002  491.22
  81   0.4244     86.230  0.3236    88.864  497.29
  82   0.4347     85.990  0.3198    88.948  503.38
  83   0.4604     85.140  0.3215    88.954  509.46
  84   0.4860     84.950  0.3197    88.934  515.55
  85   0.4813     84.420  0.3218    88.942  521.71
