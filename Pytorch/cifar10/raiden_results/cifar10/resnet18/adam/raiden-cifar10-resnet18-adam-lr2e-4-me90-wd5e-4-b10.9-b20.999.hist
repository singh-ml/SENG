Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2350     56.550  1.4653    45.960  7.64
   2   1.0516     62.600  1.0609    61.910  13.66
   3   0.8563     69.790  0.8729    69.112  19.69
   4   0.7275     75.000  0.7415    74.008  25.78
   5   0.7596     73.450  0.6519    77.230  31.80
   6   0.7753     74.360  0.5903    79.306  37.85
   7   0.6332     78.350  0.5409    81.136  43.88
   8   0.5900     79.790  0.4931    82.892  49.89
   9   0.5739     80.420  0.4586    83.978  55.96
  10   0.5915     80.940  0.4340    84.766  61.96
  11   0.5985     80.470  0.4073    85.944  67.98
  12   0.5446     82.610  0.3824    86.588  74.01
  13   0.5517     82.130  0.3607    87.562  80.04
  14   0.5308     83.060  0.3427    88.026  86.06
  15   0.5263     82.660  0.3264    88.622  92.18
  16   0.5340     83.500  0.3036    89.690  98.21
  17   0.4554     85.380  0.2912    89.766  104.21
  18   0.4826     84.490  0.2792    90.356  110.27
  19   0.5755     82.140  0.2648    90.860  116.31
  20   0.5308     83.480  0.2536    91.132  122.41
  21   0.4525     85.610  0.2372    91.766  128.43
  22   0.4259     85.980  0.2351    91.916  134.49
  23   0.4392     86.530  0.2228    92.480  140.53
  24   0.4990     84.360  0.2189    92.386  146.58
  25   0.5180     85.130  0.2078    92.882  152.66
  26   0.4377     86.440  0.1994    93.236  158.77
  27   0.4831     85.390  0.1942    93.316  164.78
  28   0.3871     88.190  0.1808    93.800  170.82
  29   0.4531     85.950  0.1711    94.220  176.84
  30   0.4496     87.040  0.1712    93.930  182.88
  31   0.4291     86.960  0.1684    94.288  188.89
  32   0.4472     86.300  0.1567    94.546  194.97
  33   0.4352     87.310  0.1558    94.602  200.99
  34   0.4968     86.190  0.1444    95.004  207.01
  35   0.4476     87.120  0.1408    95.140  213.03
  36   0.5056     85.690  0.1319    95.484  219.03
  37   0.4072     88.160  0.1349    95.320  225.04
  38   0.4693     86.660  0.1330    95.278  231.13
  39   0.4777     86.530  0.1286    95.588  237.17
  40   0.4431     87.580  0.1184    95.906  243.24
  41   0.4369     87.920  0.1173    95.988  249.28
  42   0.4428     87.730  0.1164    95.992  255.32
  43   0.4096     88.660  0.1146    96.146  261.37
  44   0.4103     88.590  0.1098    96.246  267.46
  45   0.4048     88.500  0.1058    96.342  273.48
  46   0.4059     88.730  0.1071    96.212  279.51
  47   0.4790     87.510  0.0983    96.624  285.52
  48   0.5051     86.480  0.0929    96.820  291.56
  49   0.3957     88.700  0.0960    96.758  297.66
  50   0.4594     87.640  0.0948    96.724  303.67
  51   0.3914     89.330  0.0920    96.914  309.68
  52   0.4464     87.990  0.0923    96.930  315.71
  53   0.4256     88.670  0.0898    96.914  321.74
  54   0.4661     88.010  0.0870    97.118  327.73
  55   0.4090     89.220  0.0869    97.036  333.85
  56   0.3959     89.000  0.0847    97.104  339.86
  57   0.4087     88.820  0.0848    97.100  345.87
  58   0.3928     89.340  0.0853    97.096  351.89
  59   0.4296     88.620  0.0833    97.146  357.93
  60   0.4549     88.090  0.0759    97.452  364.07
  61   0.3814     89.480  0.0804    97.298  370.10
  62   0.4248     88.730  0.0764    97.388  376.11
  63   0.5215     87.280  0.0780    97.432  382.16
  64   0.4905     88.250  0.0739    97.514  388.17
  65   0.3968     89.610  0.0748    97.410  394.24
  66   0.4692     87.630  0.0726    97.576  400.25
  67   0.4583     88.260  0.0752    97.428  406.28
  68   0.4131     89.400  0.0721    97.552  412.33
  69   0.3785     89.500  0.0721    97.578  418.36
  70   0.4376     89.190  0.0607    97.968  424.40
  71   0.4046     89.670  0.0666    97.790  430.52
  72   0.4215     89.020  0.0701    97.660  436.55
  73   0.4004     89.640  0.0676    97.676  442.56
  74   0.4374     88.600  0.0676    97.710  448.56
  75   0.4280     88.740  0.0658    97.728  454.57
  76   0.4392     88.780  0.0693    97.682  460.67
  77   0.4059     89.400  0.0611    98.000  466.71
  78   0.3601     89.840  0.0632    97.888  472.71
  79   0.4213     89.090  0.0650    97.800  478.72
  80   0.3861     89.830  0.0664    97.708  484.74
  81   0.4062     89.830  0.0581    97.996  490.76
  82   0.4121     89.890  0.0591    97.998  496.84
  83   0.4221     89.240  0.0603    97.992  502.89
  84   0.4062     89.390  0.0630    97.874  508.93
  85   0.4215     89.070  0.0591    98.042  514.97
  86   0.4091     89.840  0.0611    97.970  521.00
  87   0.4143     89.240  0.0614    97.972  527.02
  88   0.4095     89.790  0.0613    97.856  533.03
  89   0.4865     88.060  0.0557    98.096  539.06
  90   0.4798     87.920  0.0617    97.934  545.07
