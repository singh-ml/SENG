Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3976     48.760  1.6406    38.844  7.56
   2   1.1702     57.800  1.3077    52.210  13.65
   3   1.0662     61.790  1.1334    59.308  19.73
   4   0.9641     65.390  0.9963    64.320  25.82
   5   0.8742     69.420  0.8945    68.246  31.92
   6   0.7987     71.800  0.8010    71.620  38.06
   7   0.7562     73.450  0.7307    74.124  44.14
   8   0.7094     75.220  0.6685    76.554  50.21
   9   0.6653     76.910  0.6247    78.068  56.29
  10   0.6284     78.680  0.5823    79.700  62.37
  11   0.6533     77.950  0.5435    80.892  68.47
  12   0.5958     79.270  0.5101    82.206  74.60
  13   0.5692     80.410  0.4763    83.438  80.72
  14   0.5665     81.050  0.4478    84.274  86.81
  15   0.5454     81.900  0.4324    84.858  92.90
  16   0.5225     82.160  0.4075    85.590  98.98
  17   0.5150     82.800  0.3871    86.522  105.07
  18   0.5199     82.480  0.3636    87.278  111.25
  19   0.4859     84.090  0.3474    87.902  117.32
  20   0.5087     83.450  0.3283    88.410  123.41
  21   0.4808     84.140  0.3087    89.240  129.49
  22   0.4829     84.490  0.2930    89.740  135.61
  23   0.4898     84.310  0.2791    90.288  141.68
  24   0.4652     85.040  0.2680    90.624  147.81
  25   0.4746     84.880  0.2549    91.094  153.88
  26   0.4725     85.320  0.2451    91.312  159.99
  27   0.4700     85.080  0.2311    91.838  166.06
  28   0.4790     85.420  0.2219    92.232  172.16
  29   0.4987     85.160  0.2082    92.684  178.28
  30   0.4778     85.210  0.2007    93.064  184.36
  31   0.4736     86.020  0.1891    93.362  190.43
  32   0.4663     85.950  0.1863    93.496  196.52
  33   0.4885     85.720  0.1723    94.096  202.61
  34   0.4672     86.590  0.1674    94.056  208.70
  35   0.4787     85.850  0.1660    94.112  214.83
  36   0.4787     85.850  0.1570    94.432  220.96
  37   0.4623     86.660  0.1489    94.806  227.08
  38   0.4843     86.270  0.1411    95.040  233.15
  39   0.4963     86.340  0.1430    95.078  239.25
  40   0.4728     86.920  0.1307    95.352  245.35
  41   0.4725     86.650  0.1242    95.656  251.49
  42   0.4875     86.390  0.1197    95.800  257.58
  43   0.4996     86.280  0.1166    96.014  263.65
  44   0.4658     86.740  0.1128    96.134  269.73
  45   0.4956     87.110  0.1122    96.124  275.82
  46   0.5223     86.120  0.1034    96.350  281.91
  47   0.4841     86.670  0.1057    96.330  288.06
  48   0.4826     86.630  0.1061    96.224  294.16
  49   0.4664     87.480  0.0986    96.598  300.25
  50   0.5078     87.150  0.0863    97.046  306.33
  51   0.4996     86.890  0.0897    96.878  312.45
  52   0.4778     87.380  0.0868    97.046  318.60
  53   0.4857     86.980  0.0830    97.116  324.69
  54   0.5125     86.760  0.0800    97.238  330.77
  55   0.5035     87.700  0.0775    97.336  336.86
  56   0.4962     87.560  0.0759    97.336  342.96
  57   0.5178     87.300  0.0795    97.226  349.04
  58   0.5228     87.170  0.0748    97.382  355.21
  59   0.5212     87.360  0.0695    97.582  361.33
  60   0.5040     87.480  0.0670    97.690  367.40
  61   0.5063     87.740  0.0709    97.498  373.49
  62   0.5023     87.620  0.0715    97.572  379.57
  63   0.5333     87.320  0.0669    97.724  385.64
  64   0.4867     88.040  0.0618    97.886  391.79
  65   0.5254     87.270  0.0677    97.610  397.85
  66   0.4789     87.970  0.0639    97.758  403.95
  67   0.5080     87.910  0.0579    98.014  410.05
  68   0.5246     87.520  0.0558    98.066  416.13
  69   0.4881     88.030  0.0565    98.082  422.22
  70   0.4645     88.880  0.0569    98.006  428.31
  71   0.4962     88.160  0.0557    98.172  434.42
  72   0.4805     88.470  0.0534    98.216  440.49
  73   0.4970     88.110  0.0522    98.212  446.57
  74   0.5215     87.520  0.0534    98.178  452.67
  75   0.5192     88.050  0.0548    98.088  458.85
  76   0.5171     88.300  0.0513    98.248  464.97
  77   0.5030     88.040  0.0518    98.238  471.05
  78   0.5177     88.270  0.0461    98.448  477.16
  79   0.5269     87.840  0.0491    98.246  483.25
  80   0.5193     88.360  0.0514    98.272  489.37
  81   0.5201     88.340  0.0455    98.458  495.52
  82   0.5114     88.120  0.0443    98.490  501.67
  83   0.5019     88.390  0.0469    98.402  507.79
  84   0.5143     88.160  0.0458    98.468  513.89
  85   0.5081     88.590  0.0459    98.470  519.99
