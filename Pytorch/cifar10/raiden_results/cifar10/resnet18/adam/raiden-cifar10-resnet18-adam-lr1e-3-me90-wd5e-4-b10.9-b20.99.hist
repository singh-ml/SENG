Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3170     55.450  1.3985    48.880  9.00
   2   0.9766     66.600  0.9335    66.946  15.17
   3   0.8694     70.770  0.7409    74.110  21.34
   4   0.9280     69.730  0.6417    77.774  27.47
   5   0.7148     75.550  0.5817    80.008  33.55
   6   0.6173     79.400  0.5290    82.134  39.62
   7   0.6884     77.950  0.4908    83.240  45.74
   8   0.7623     77.250  0.4587    84.262  51.84
   9   0.5845     80.880  0.4267    85.620  57.94
  10   0.6997     77.770  0.4074    86.202  64.01
  11   0.5351     82.290  0.3834    86.884  70.08
  12   0.5170     83.000  0.3694    87.500  76.14
  13   0.5225     83.400  0.3506    88.168  82.31
  14   0.4983     84.020  0.3427    88.468  88.38
  15   0.5115     83.840  0.3296    88.816  94.53
  16   0.4427     85.360  0.3122    89.564  100.60
  17   0.4767     84.650  0.3035    89.610  106.69
  18   0.5317     83.570  0.2906    90.074  112.79
  19   0.4078     86.470  0.2827    90.402  118.88
  20   0.3930     87.360  0.2788    90.424  125.05
  21   0.4559     85.380  0.2686    90.814  131.17
  22   0.5889     83.230  0.2663    90.824  137.24
  23   0.4153     86.650  0.2538    91.242  143.31
  24   0.4292     86.140  0.2524    91.332  149.41
  25   0.3943     87.900  0.2421    91.660  155.61
  26   0.4464     85.940  0.2456    91.622  161.68
  27   0.3851     87.950  0.2358    91.850  167.78
  28   0.4148     86.850  0.2359    92.006  173.90
  29   0.4985     85.210  0.2277    92.272  179.99
  30   0.3954     88.520  0.2255    92.286  186.11
  31   0.3572     88.670  0.2220    92.444  192.23
  32   0.4810     85.510  0.2203    92.426  198.32
  33   0.3625     88.140  0.2177    92.564  204.44
  34   0.3770     87.810  0.2085    92.902  210.56
  35   0.4163     87.830  0.2099    92.830  216.63
  36   0.4085     87.040  0.2123    92.858  222.74
  37   0.3655     88.330  0.2087    92.888  228.88
  38   0.3732     88.580  0.1986    93.124  234.95
  39   0.4423     86.440  0.1958    93.388  241.01
  40   0.3736     87.760  0.1984    93.334  247.13
  41   0.4055     87.250  0.1905    93.478  253.21
  42   0.3849     88.070  0.1949    93.376  259.38
  43   0.4024     87.580  0.1952    93.316  265.49
  44   0.4333     87.380  0.1922    93.440  271.61
  45   0.3819     88.700  0.1824    93.756  277.69
  46   0.4263     87.260  0.1848    93.616  283.81
  47   0.3702     89.110  0.1804    93.770  289.89
  48   0.4065     88.210  0.1865    93.606  296.04
  49   0.4307     86.640  0.1842    93.684  302.12
  50   0.3842     88.040  0.1790    93.942  308.19
  51   0.3728     88.700  0.1775    93.964  314.24
  52   0.3834     88.580  0.1778    93.884  320.32
  53   0.3819     88.900  0.1740    94.008  326.40
  54   0.3477     89.470  0.1735    94.066  332.48
  55   0.3613     89.410  0.1760    93.942  338.61
  56   0.3853     88.150  0.1729    94.032  344.74
  57   0.4107     88.100  0.1692    94.256  350.81
  58   0.3868     88.270  0.1704    94.148  356.92
  59   0.3481     89.250  0.1671    94.314  363.00
  60   0.3586     89.200  0.1694    94.280  369.15
  61   0.4163     88.260  0.1657    94.210  375.23
  62   0.3557     89.700  0.1629    94.384  381.32
  63   0.3633     88.890  0.1647    94.364  387.39
  64   0.3965     88.210  0.1688    94.226  393.48
  65   0.3350     89.750  0.1614    94.510  399.58
  66   0.3438     89.920  0.1621    94.492  405.75
  67   0.3692     89.430  0.1618    94.536  411.85
  68   0.3957     88.380  0.1636    94.470  417.94
  69   0.3439     89.950  0.1614    94.414  424.01
  70   0.3657     89.070  0.1597    94.648  430.08
  71   0.3611     89.470  0.1608    94.460  436.21
  72   0.3765     88.660  0.1577    94.584  442.29
  73   0.3686     88.930  0.1559    94.618  448.37
  74   0.3892     88.880  0.1542    94.654  454.45
  75   0.3559     89.260  0.1530    94.878  460.53
  76   0.3646     89.240  0.1579    94.548  466.65
  77   0.3720     89.090  0.1559    94.590  472.77
  78   0.3639     89.310  0.1571    94.618  478.96
  79   0.3654     89.760  0.1507    94.806  485.03
  80   0.3811     89.510  0.1520    94.732  491.10
  81   0.3434     90.150  0.1561    94.694  497.16
  82   0.3447     89.790  0.1517    94.860  503.28
  83   0.3847     89.220  0.1564    94.640  509.44
  84   0.3896     89.280  0.1507    94.794  515.55
  85   0.3966     88.670  0.1495    94.912  521.69
  86   0.3943     88.180  0.1517    94.860  527.75
  87   0.3638     89.820  0.1523    94.816  533.83
  88   0.3514     89.840  0.1499    94.996  539.95
  89   0.3546     89.410  0.1472    94.980  546.12
  90   0.3819     89.110  0.1493    94.972  552.20
