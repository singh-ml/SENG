Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.0792     31.170  1.8187    33.006  7.69
   2   1.3024     53.060  1.3644    50.182  13.85
   3   1.4561     52.540  1.1064    60.388  20.06
   4   1.0459     63.800  0.9313    66.942  26.23
   5   1.2894     62.250  0.8058    71.618  32.39
   6   0.8359     70.910  0.7043    75.462  38.54
   7   0.7910     73.940  0.6389    77.936  44.69
   8   0.7884     74.470  0.5862    79.662  50.92
   9   0.7564     76.080  0.5423    81.438  57.06
  10   0.6961     77.170  0.5094    82.424  63.22
  11   0.7261     76.840  0.4954    82.856  69.36
  12   0.6634     78.460  0.4667    83.956  75.52
  13   0.7004     77.370  0.4601    84.258  81.69
  14   0.5779     81.310  0.4384    84.984  87.85
  15   0.5716     80.890  0.4320    85.164  94.11
  16   0.7666     75.900  0.4188    85.568  100.27
  17   0.5938     80.880  0.4094    85.976  106.45
  18   0.6797     77.730  0.4031    86.130  112.61
  19   0.5534     81.500  0.3937    86.412  118.77
  20   0.5095     83.440  0.3912    86.678  124.92
  21   0.4902     83.930  0.3797    86.902  131.08
  22   0.5181     83.370  0.3756    87.054  137.24
  23   0.5988     81.220  0.3688    87.310  143.44
  24   0.5659     81.780  0.3687    87.344  149.60
  25   0.5608     82.640  0.3580    87.730  155.78
  26   0.5823     81.920  0.3529    87.834  162.04
  27   0.5076     83.560  0.3496    88.062  168.19
  28   0.4820     84.090  0.3483    88.132  174.40
  29   0.5502     82.860  0.3411    88.156  180.60
  30   0.4055     86.470  0.3391    88.404  186.77
  31   0.5251     83.510  0.3333    88.550  192.93
  32   0.4791     84.340  0.3301    88.674  199.17
  33   0.5041     83.940  0.3301    88.676  205.33
  34   0.5645     82.340  0.3253    88.858  211.51
  35   0.5158     83.260  0.3272    88.568  217.67
  36   0.4718     84.530  0.3235    88.804  223.83
  37   0.6492     80.740  0.3178    88.942  230.04
  38   0.5024     84.250  0.3151    89.094  236.22
  39   0.4690     84.900  0.3141    89.140  242.40
  40   0.3912     86.950  0.3082    89.278  248.55
  41   0.4445     85.750  0.3173    88.920  254.70
  42   0.4030     86.420  0.3096    89.326  260.86
  43   0.5075     84.200  0.3035    89.488  267.09
  44   0.4864     85.260  0.3056    89.550  273.27
  45   0.5474     82.300  0.3065    89.424  279.45
  46   0.4618     85.540  0.3082    89.460  285.63
  47   0.4071     86.310  0.3038    89.524  291.80
  48   0.4187     86.560  0.3035    89.382  297.98
  49   0.7769     77.700  0.2992    89.736  304.18
  50   0.4440     85.800  0.3028    89.654  310.33
  51   0.4232     86.570  0.3006    89.714  316.51
  52   0.4110     87.130  0.2937    89.884  322.68
  53   0.4674     85.380  0.2900    90.066  328.85
  54   0.3637     87.970  0.2948    90.040  335.01
  55   0.4433     86.040  0.2938    89.890  341.24
  56   0.3662     87.790  0.2939    89.872  347.40
  57   0.6302     81.840  0.2917    89.978  353.54
  58   0.4227     86.300  0.2885    90.040  359.70
  59   0.4470     85.520  0.2906    90.030  365.87
  60   0.4566     85.880  0.2936    89.826  372.03
  61   0.4208     86.420  0.2906    90.094  378.26
  62   0.5132     84.270  0.2818    90.286  384.44
  63   0.3930     87.040  0.2865    89.954  390.59
  64   0.4937     84.130  0.2813    90.108  396.75
  65   0.4987     84.120  0.2859    90.248  402.93
  66   0.4580     85.310  0.2882    90.116  409.10
  67   0.4378     86.130  0.2836    90.146  415.34
  68   0.4170     86.910  0.2835    90.114  421.51
  69   0.7706     77.980  0.2808    90.272  427.66
  70   0.5198     83.680  0.2803    90.278  433.83
  71   0.3992     86.900  0.2813    90.292  440.01
  72   0.4199     87.390  0.2781    90.362  446.29
  73   0.4332     86.520  0.2791    90.420  452.48
  74   0.4543     85.910  0.2811    90.276  458.64
  75   0.5589     83.280  0.2786    90.504  464.83
  76   0.3823     87.570  0.2741    90.540  471.03
  77   0.3787     87.290  0.2769    90.274  477.22
  78   0.4052     86.810  0.2770    90.328  483.43
  79   0.4631     85.680  0.2763    90.542  489.59
  80   0.5570     83.340  0.2724    90.424  495.79
  81   0.5166     83.240  0.2688    90.726  501.98
  82   0.4314     86.620  0.2789    90.372  508.14
  83   0.4958     85.020  0.2718    90.624  514.33
  84   0.4879     84.390  0.2721    90.636  520.52
  85   0.4165     86.930  0.2713    90.596  526.68
