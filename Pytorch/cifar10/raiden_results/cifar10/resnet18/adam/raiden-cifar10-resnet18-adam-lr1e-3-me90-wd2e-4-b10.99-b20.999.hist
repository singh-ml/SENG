Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2987     52.730  1.5527    42.610  7.75
   2   1.2238     57.660  1.1796    57.206  13.95
   3   0.9998     64.180  0.9922    64.336  20.15
   4   0.8975     68.600  0.8645    69.234  26.33
   5   0.7140     75.170  0.7383    74.272  32.47
   6   0.6705     77.140  0.6573    77.058  38.67
   7   0.7026     76.680  0.5950    79.336  44.87
   8   0.5755     80.550  0.5384    81.320  51.04
   9   0.5596     81.560  0.4947    82.996  57.24
  10   0.6605     78.690  0.4620    83.980  63.42
  11   0.4954     83.530  0.4400    85.020  69.60
  12   0.4885     83.410  0.4038    86.064  75.75
  13   0.4553     84.330  0.3827    86.972  81.88
  14   0.5058     83.640  0.3548    87.732  88.10
  15   0.4497     85.180  0.3382    88.494  94.26
  16   0.4233     85.750  0.3346    88.432  100.43
  17   0.3814     87.460  0.3071    89.568  106.57
  18   0.4171     86.770  0.2859    90.190  112.74
  19   0.4177     86.720  0.2756    90.716  118.94
  20   0.4016     87.170  0.2716    90.818  125.05
  21   0.3708     87.890  0.2602    91.010  131.19
  22   0.4705     86.070  0.2488    91.414  137.38
  23   0.3711     87.800  0.2380    91.832  143.51
  24   0.3905     87.380  0.2256    92.282  149.67
  25   0.3698     88.360  0.2291    92.056  155.86
  26   0.3291     89.640  0.2180    92.440  162.02
  27   0.3238     89.390  0.2028    93.122  168.17
  28   0.4051     87.550  0.2069    92.884  174.34
  29   0.3334     90.100  0.1990    93.122  180.49
  30   0.3805     88.800  0.1868    93.460  186.64
  31   0.3555     89.540  0.1890    93.472  192.85
  32   0.3686     89.340  0.1766    94.008  198.97
  33   0.3781     88.930  0.1794    93.698  205.14
  34   0.3320     89.730  0.1759    93.890  211.27
  35   0.3406     90.380  0.1692    94.182  217.45
  36   0.3486     90.030  0.1683    94.186  223.62
  37   0.3652     89.640  0.1578    94.494  229.83
  38   0.3620     89.600  0.1530    94.654  235.95
  39   0.3579     89.530  0.1608    94.402  242.08
  40   0.3224     90.460  0.1576    94.652  248.22
  41   0.3578     89.760  0.1491    94.836  254.33
  42   0.3266     90.820  0.1398    95.176  260.53
  43   0.3326     90.370  0.1418    95.152  266.68
  44   0.3528     89.870  0.1402    95.258  272.83
  45   0.3358     90.240  0.1490    94.808  278.99
  46   0.3521     89.910  0.1393    95.188  285.16
  47   0.3401     90.270  0.1384    95.158  291.33
  48   0.3438     90.570  0.1342    95.372  297.55
  49   0.3395     90.370  0.1289    95.586  303.67
  50   0.3797     90.070  0.1284    95.638  309.84
  51   0.3281     90.490  0.1281    95.536  315.95
  52   0.3309     90.380  0.1325    95.442  322.10
  53   0.3310     90.550  0.1271    95.650  328.34
  54   0.3765     89.450  0.1235    95.686  334.50
  55   0.3278     90.620  0.1201    95.832  340.64
  56   0.3219     91.010  0.1187    95.900  346.78
  57   0.3059     91.290  0.1215    95.856  352.90
  58   0.3724     90.200  0.1081    96.240  359.05
  59   0.3424     90.690  0.1117    96.108  365.19
  60   0.3317     90.810  0.1151    96.018  371.35
  61   0.3552     90.050  0.1188    96.016  377.49
  62   0.3521     90.120  0.1151    96.124  383.61
  63   0.3604     90.260  0.1191    95.888  389.78
  64   0.3417     90.730  0.1160    95.938  396.00
  65   0.3204     90.820  0.1146    96.108  402.17
  66   0.3131     91.060  0.1096    96.224  408.29
  67   0.3471     90.780  0.1076    96.346  414.48
  68   0.3444     90.670  0.1145    96.048  420.60
  69   0.3577     90.620  0.1138    96.104  426.80
  70   0.3383     91.040  0.1077    96.256  432.95
  71   0.3340     90.780  0.1047    96.424  439.09
  72   0.3447     90.730  0.1061    96.342  445.25
  73   0.3453     90.780  0.1079    96.252  451.38
  74   0.3250     91.080  0.1105    96.206  457.55
  75   0.3433     90.920  0.1035    96.386  463.78
  76   0.3529     90.510  0.1017    96.552  469.94
  77   0.3377     90.780  0.1035    96.458  476.13
  78   0.3470     90.860  0.1065    96.356  482.31
  79   0.3637     90.490  0.1040    96.438  488.49
  80   0.3281     91.170  0.0991    96.734  494.70
  81   0.3216     91.130  0.0979    96.560  500.84
  82   0.3324     90.930  0.0979    96.740  506.99
  83   0.3493     90.480  0.1019    96.526  513.13
  84   0.3604     90.670  0.0971    96.742  519.30
  85   0.3620     90.310  0.1075    96.310  525.50
  86   0.3302     91.060  0.1000    96.588  531.67
  87   0.3636     90.520  0.0968    96.606  537.85
  88   0.3460     90.530  0.1013    96.518  543.98
  89   0.3293     91.130  0.0978    96.610  550.17
  90   0.3519     91.270  0.0950    96.714  556.36
