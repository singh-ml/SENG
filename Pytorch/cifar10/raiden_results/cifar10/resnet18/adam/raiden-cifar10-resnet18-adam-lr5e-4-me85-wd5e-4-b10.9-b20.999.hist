Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3512     53.650  1.3840    49.356  7.66
   2   0.9762     66.450  0.9195    67.276  13.72
   3   1.1801     63.570  0.7265    74.696  19.74
   4   0.7171     75.570  0.6261    78.272  25.83
   5   0.7651     74.390  0.5670    80.224  31.90
   6   0.5816     80.070  0.5123    82.410  37.90
   7   0.5415     81.730  0.4681    83.888  43.96
   8   0.5584     81.870  0.4397    85.012  49.99
   9   0.4992     82.890  0.4096    86.068  56.00
  10   0.5944     80.080  0.3890    86.668  62.12
  11   0.5407     82.750  0.3639    87.698  68.16
  12   0.5166     83.380  0.3486    88.012  74.21
  13   0.5633     81.580  0.3348    88.498  80.24
  14   0.4977     83.170  0.3180    89.118  86.31
  15   0.5773     82.120  0.3039    89.638  92.38
  16   0.7353     77.870  0.2934    90.168  98.49
  17   0.6670     80.320  0.2824    90.300  104.54
  18   0.4155     86.290  0.2694    90.920  110.60
  19   0.4454     85.980  0.2549    91.304  116.67
  20   0.3603     88.060  0.2565    91.372  122.71
  21   0.4786     85.370  0.2466    91.686  128.74
  22   0.5590     82.750  0.2354    91.998  134.83
  23   0.3682     87.870  0.2277    92.252  140.90
  24   0.4115     86.730  0.2209    92.506  146.91
  25   0.4402     86.200  0.2115    92.826  152.99
  26   0.5473     84.100  0.2069    92.904  159.07
  27   0.3459     88.960  0.1973    93.264  165.08
  28   0.5316     84.760  0.1938    93.456  171.18
  29   0.4607     86.190  0.1850    93.746  177.27
  30   0.3918     88.080  0.1865    93.694  183.34
  31   0.3776     88.630  0.1746    94.196  189.34
  32   0.4408     87.070  0.1741    94.096  195.37
  33   0.3829     88.440  0.1729    94.152  201.40
  34   0.3940     87.970  0.1631    94.366  207.47
  35   0.3486     89.200  0.1608    94.590  213.50
  36   0.3700     88.700  0.1670    94.410  219.55
  37   0.4190     87.620  0.1540    94.734  225.62
  38   0.3712     88.190  0.1591    94.508  231.67
  39   0.4091     88.120  0.1487    94.942  237.84
  40   0.3557     88.910  0.1474    94.976  243.86
  41   0.3417     89.700  0.1450    95.084  249.89
  42   0.4006     88.170  0.1385    95.442  255.93
  43   0.3168     90.430  0.1343    95.424  262.00
  44   0.3590     88.940  0.1431    95.164  268.12
  45   0.3378     89.670  0.1325    95.482  274.17
  46   0.3353     89.970  0.1317    95.560  280.21
  47   0.3640     89.130  0.1328    95.472  286.24
  48   0.3697     89.290  0.1284    95.682  292.27
  49   0.4141     88.070  0.1161    96.044  298.29
  50   0.3376     90.520  0.1278    95.722  304.37
  51   0.5029     86.550  0.1238    95.768  310.43
  52   0.3365     90.450  0.1231    95.862  316.46
  53   0.3412     89.830  0.1232    95.858  322.46
  54   0.3672     89.720  0.1159    96.102  328.50
  55   0.3516     90.050  0.1154    96.088  334.64
  56   0.5450     85.570  0.1207    95.942  340.69
  57   0.3744     89.920  0.1176    96.068  346.76
  58   0.5363     86.390  0.1195    95.970  352.81
  59   0.3570     89.940  0.1124    96.150  358.89
  60   0.3383     90.360  0.1152    96.000  364.96
  61   0.3633     89.760  0.1124    96.258  371.04
  62   0.3815     89.190  0.1122    96.228  377.08
  63   0.3273     90.280  0.1029    96.490  383.13
  64   0.3697     89.650  0.1048    96.392  389.17
  65   0.3893     88.820  0.1108    96.184  395.24
  66   0.3788     89.400  0.1065    96.370  401.38
  67   0.3840     89.940  0.0996    96.612  407.45
  68   0.3584     90.300  0.1039    96.478  413.49
  69   0.3342     90.560  0.1037    96.482  419.52
  70   0.3635     90.020  0.1031    96.514  425.61
  71   0.3938     89.010  0.1018    96.576  431.65
  72   0.3394     90.560  0.0976    96.708  437.71
  73   0.3684     90.200  0.0980    96.666  443.85
  74   0.3544     89.690  0.0995    96.678  449.89
  75   0.3706     89.770  0.0962    96.644  455.94
  76   0.3242     90.990  0.1010    96.566  461.99
  77   0.3661     90.210  0.0943    96.772  468.03
  78   0.4564     88.470  0.0992    96.646  474.18
  79   0.3710     90.190  0.0922    96.866  480.22
  80   0.3680     90.240  0.0961    96.746  486.32
  81   0.3445     90.920  0.0964    96.706  492.36
  82   0.3850     90.040  0.0950    96.852  498.45
  83   0.4077     89.390  0.0969    96.660  504.48
  84   0.3599     90.390  0.0985    96.634  510.63
  85   0.4009     89.530  0.0955    96.788  516.68
