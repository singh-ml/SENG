Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2995     53.150  1.4720    45.822  7.84
   2   1.0239     63.480  1.0642    61.806  14.05
   3   0.8873     69.330  0.8729    69.220  20.27
   4   0.8560     70.830  0.7385    73.944  26.46
   5   0.7537     74.260  0.6592    76.954  32.72
   6   0.6063     79.160  0.5935    79.192  38.91
   7   0.7293     75.770  0.5459    80.930  45.13
   8   0.5750     80.190  0.5019    82.452  51.33
   9   0.5743     80.120  0.4614    83.834  57.51
  10   0.5429     81.560  0.4362    84.822  63.72
  11   0.6194     79.960  0.4064    85.996  69.98
  12   0.5355     82.290  0.3897    86.398  76.19
  13   0.6137     79.860  0.3620    87.552  82.42
  14   0.5677     81.660  0.3501    87.680  88.62
  15   0.5440     82.130  0.3286    88.604  94.81
  16   0.5188     82.400  0.3142    88.894  101.02
  17   0.4904     84.420  0.2968    89.706  107.27
  18   0.4559     84.720  0.2847    90.264  113.49
  19   0.5421     83.420  0.2690    90.692  119.71
  20   0.4643     84.850  0.2559    91.188  125.94
  21   0.4237     86.400  0.2420    91.638  132.17
  22   0.4311     85.750  0.2346    91.910  138.45
  23   0.4262     86.560  0.2248    92.218  144.67
  24   0.4491     86.340  0.2134    92.626  150.87
  25   0.5488     83.780  0.2060    92.874  157.08
  26   0.4187     86.900  0.2001    92.988  163.32
  27   0.4483     86.130  0.1934    93.352  169.52
  28   0.3918     87.890  0.1869    93.432  175.82
  29   0.4541     86.390  0.1756    93.910  182.03
  30   0.4778     85.880  0.1672    94.142  188.24
  31   0.4752     86.030  0.1605    94.504  194.42
  32   0.4204     87.220  0.1619    94.340  200.65
  33   0.4374     87.330  0.1533    94.682  206.88
  34   0.4492     86.200  0.1514    94.722  213.06
  35   0.5008     85.940  0.1440    94.986  219.29
  36   0.4556     86.930  0.1385    95.106  225.47
  37   0.4374     87.520  0.1347    95.414  231.70
  38   0.4227     87.440  0.1337    95.358  237.98
  39   0.4192     87.580  0.1262    95.714  244.18
  40   0.4187     87.790  0.1250    95.620  250.37
  41   0.3736     89.050  0.1224    95.784  256.54
  42   0.5137     86.090  0.1131    96.166  262.76
  43   0.5414     85.470  0.1163    96.054  268.97
  44   0.5021     85.900  0.1128    96.164  275.23
  45   0.3927     88.950  0.1068    96.366  281.42
  46   0.4018     88.550  0.1061    96.334  287.62
  47   0.4123     88.790  0.1041    96.460  293.85
  48   0.4234     88.120  0.1050    96.358  300.08
  49   0.4729     87.550  0.0912    96.922  306.30
  50   0.4417     87.740  0.1018    96.530  312.51
  51   0.4371     88.520  0.0931    96.836  318.73
  52   0.4096     89.130  0.0913    96.902  324.92
  53   0.5258     86.470  0.0927    96.810  331.09
  54   0.4636     87.820  0.0848    97.176  337.38
  55   0.4361     88.240  0.0909    96.912  343.61
  56   0.4003     89.160  0.0883    96.938  349.81
  57   0.4606     87.790  0.0839    97.192  356.04
  58   0.3858     89.530  0.0850    97.142  362.23
  59   0.4216     88.480  0.0838    97.154  368.45
  60   0.4016     88.700  0.0776    97.422  374.70
  61   0.4428     88.180  0.0809    97.216  380.91
  62   0.4038     89.070  0.0757    97.408  387.12
  63   0.3714     90.040  0.0779    97.296  393.34
  64   0.4262     89.200  0.0702    97.646  399.57
  65   0.5045     87.290  0.0740    97.556  405.84
  66   0.4087     89.280  0.0767    97.384  412.07
  67   0.4233     89.050  0.0699    97.738  418.25
  68   0.3852     89.390  0.0783    97.352  424.44
  69   0.4743     88.650  0.0724    97.548  430.64
  70   0.4093     89.360  0.0699    97.632  436.84
  71   0.3982     89.650  0.0675    97.718  443.07
  72   0.4065     89.640  0.0710    97.642  449.30
  73   0.3893     89.890  0.0674    97.702  455.50
  74   0.3620     90.300  0.0678    97.682  461.73
  75   0.4056     89.100  0.0634    97.868  467.94
  76   0.3969     89.790  0.0710    97.590  474.14
  77   0.4463     88.480  0.0592    98.042  480.41
  78   0.4026     89.500  0.0667    97.788  486.63
  79   0.4347     89.000  0.0635    97.772  492.85
  80   0.4556     88.690  0.0584    98.082  499.04
  81   0.4455     88.290  0.0683    97.662  505.22
  82   0.4150     89.210  0.0586    98.006  511.50
  83   0.4282     89.120  0.0614    97.964  517.70
  84   0.4070     89.130  0.0632    97.906  523.97
  85   0.3722     90.110  0.0604    98.090  530.26
