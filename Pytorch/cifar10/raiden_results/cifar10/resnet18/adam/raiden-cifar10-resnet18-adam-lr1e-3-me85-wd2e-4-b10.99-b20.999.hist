Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3018     51.870  1.5058    44.318  7.72
   2   1.0554     62.190  1.1378    58.908  13.84
   3   0.9032     67.480  0.9606    65.638  20.06
   4   0.8138     72.200  0.7981    71.820  26.18
   5   0.7826     73.550  0.7004    75.422  32.32
   6   0.6528     77.910  0.6180    78.498  38.45
   7   0.6106     79.180  0.5613    80.708  44.55
   8   0.5825     81.010  0.5239    81.906  50.69
   9   0.5505     81.550  0.4819    83.384  56.89
  10   0.4760     83.830  0.4533    84.552  63.00
  11   0.5168     83.440  0.4276    85.314  69.11
  12   0.4899     83.710  0.3871    86.654  75.20
  13   0.4484     84.660  0.3703    87.258  81.35
  14   0.4795     83.940  0.3634    87.388  87.51
  15   0.4051     86.240  0.3437    88.172  93.70
  16   0.4371     86.210  0.3123    89.308  99.82
  17   0.4160     86.280  0.3055    89.600  105.94
  18   0.3829     87.220  0.2883    90.246  112.07
  19   0.4008     86.440  0.2728    90.624  118.23
  20   0.3879     87.040  0.2686    90.754  124.34
  21   0.4274     85.940  0.2525    91.318  130.53
  22   0.4367     86.280  0.2497    91.386  136.67
  23   0.3490     88.720  0.2382    91.990  142.83
  24   0.3736     87.900  0.2254    92.252  148.95
  25   0.4103     87.110  0.2156    92.716  155.05
  26   0.3767     88.350  0.2173    92.560  161.27
  27   0.3531     88.840  0.2100    92.710  167.42
  28   0.3882     87.730  0.2057    92.892  173.59
  29   0.3604     89.000  0.1958    93.238  179.72
  30   0.3283     89.800  0.1939    93.214  185.82
  31   0.3261     89.540  0.1896    93.438  191.96
  32   0.3497     89.250  0.1782    93.872  198.13
  33   0.3737     88.740  0.1748    94.052  204.23
  34   0.3212     90.100  0.1753    93.882  210.39
  35   0.3553     88.870  0.1696    94.240  216.53
  36   0.3532     89.550  0.1699    94.096  222.64
  37   0.3556     89.010  0.1683    94.148  228.83
  38   0.3532     89.490  0.1560    94.702  234.97
  39   0.3703     89.040  0.1526    94.708  241.07
  40   0.3292     90.130  0.1529    94.800  247.17
  41   0.3495     89.430  0.1482    94.860  253.31
  42   0.3431     89.630  0.1438    95.082  259.48
  43   0.3770     89.410  0.1434    95.110  265.63
  44   0.3647     89.500  0.1457    94.946  271.79
  45   0.3356     90.210  0.1487    94.926  277.89
  46   0.3299     90.190  0.1419    94.988  284.04
  47   0.3195     90.230  0.1330    95.404  290.18
  48   0.3493     90.140  0.1306    95.502  296.36
  49   0.3492     89.800  0.1278    95.562  302.44
  50   0.3379     90.700  0.1316    95.452  308.55
  51   0.3371     90.270  0.1267    95.620  314.67
  52   0.3439     90.160  0.1266    95.630  320.78
  53   0.3594     89.920  0.1213    95.806  326.96
  54   0.3616     89.740  0.1331    95.402  333.11
  55   0.3332     90.740  0.1230    95.804  339.24
  56   0.3653     90.330  0.1163    95.976  345.39
  57   0.3947     89.340  0.1241    95.762  351.48
  58   0.3488     90.510  0.1254    95.670  357.65
  59   0.3956     90.100  0.1144    96.030  363.86
  60   0.3669     90.190  0.1142    95.944  370.01
  61   0.3601     90.030  0.1191    95.880  376.10
  62   0.3377     90.440  0.1166    96.014  382.22
  63   0.3566     90.180  0.1158    96.056  388.32
  64   0.3452     90.470  0.1078    96.386  394.47
  65   0.3272     90.860  0.1101    96.218  400.62
  66   0.3436     90.670  0.1093    96.278  406.75
  67   0.3580     90.120  0.1094    96.162  412.86
  68   0.3843     90.320  0.1054    96.404  419.00
  69   0.3627     90.950  0.1078    96.376  425.15
  70   0.3375     90.340  0.1107    96.178  431.29
  71   0.3601     90.470  0.1090    96.184  437.45
  72   0.3533     90.370  0.1063    96.306  443.57
  73   0.3475     90.280  0.1047    96.392  449.73
  74   0.3558     90.510  0.1041    96.314  455.84
  75   0.3515     90.610  0.1059    96.382  461.96
  76   0.3560     90.440  0.1046    96.364  468.09
  77   0.3432     90.360  0.1025    96.446  474.26
  78   0.3282     91.220  0.0994    96.638  480.41
  79   0.3301     91.320  0.0960    96.656  486.58
  80   0.3685     90.730  0.1015    96.462  492.74
  81   0.3755     90.010  0.1016    96.470  498.87
  82   0.3436     90.880  0.1073    96.212  505.15
  83   0.3599     90.360  0.0993    96.572  511.26
  84   0.3374     90.990  0.0984    96.660  517.38
  85   0.3303     91.190  0.0993    96.636  523.53
