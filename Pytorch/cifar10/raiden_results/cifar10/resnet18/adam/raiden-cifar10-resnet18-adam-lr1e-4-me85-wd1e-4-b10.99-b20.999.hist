Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3687     50.220  1.6342    39.512  7.72
   2   1.1595     57.770  1.2825    53.228  13.85
   3   1.0114     64.060  1.0938    60.708  19.94
   4   0.9762     65.510  0.9678    65.456  26.02
   5   0.8466     70.190  0.8738    69.088  32.16
   6   0.8065     72.120  0.7906    72.092  38.27
   7   0.7391     74.100  0.7270    74.234  44.48
   8   0.7014     76.170  0.6737    76.200  50.63
   9   0.6841     76.730  0.6255    78.048  56.72
  10   0.6398     77.920  0.5766    79.886  62.84
  11   0.6130     79.100  0.5345    81.440  68.98
  12   0.6007     80.100  0.5112    82.104  75.11
  13   0.5836     80.270  0.4803    83.178  81.28
  14   0.5807     80.610  0.4557    84.218  87.42
  15   0.5534     81.860  0.4357    84.836  93.56
  16   0.5417     82.120  0.4099    85.678  99.64
  17   0.5316     82.650  0.3834    86.688  105.73
  18   0.5112     83.050  0.3659    87.300  111.85
  19   0.5220     83.320  0.3468    88.044  118.01
  20   0.4999     83.600  0.3352    88.366  124.10
  21   0.4862     83.940  0.3096    89.200  130.19
  22   0.4812     84.670  0.2943    89.690  136.29
  23   0.5151     83.610  0.2846    90.050  142.37
  24   0.4813     85.050  0.2759    90.380  148.59
  25   0.5065     84.500  0.2572    91.022  154.69
  26   0.4787     84.900  0.2524    91.220  160.82
  27   0.5217     84.220  0.2328    91.834  166.97
  28   0.4876     85.340  0.2238    92.138  173.06
  29   0.4927     84.710  0.2109    92.558  179.23
  30   0.4966     84.900  0.1980    92.990  185.36
  31   0.4872     85.450  0.1946    93.136  191.46
  32   0.5468     84.530  0.1784    93.614  197.56
  33   0.4952     85.410  0.1819    93.550  203.66
  34   0.4830     85.960  0.1675    94.112  209.79
  35   0.5073     85.460  0.1597    94.408  215.96
  36   0.4861     86.360  0.1520    94.626  222.09
  37   0.4872     86.550  0.1499    94.790  228.24
  38   0.5064     85.510  0.1379    95.174  234.31
  39   0.5499     85.400  0.1346    95.260  240.41
  40   0.5095     86.080  0.1266    95.614  246.55
  41   0.5143     86.170  0.1253    95.554  252.68
  42   0.4998     86.530  0.1195    95.738  258.78
  43   0.5332     85.800  0.1103    96.200  264.93
  44   0.5295     86.510  0.1108    96.096  271.07
  45   0.5432     86.300  0.1082    96.246  277.18
  46   0.5030     86.690  0.0995    96.450  283.34
  47   0.5169     87.000  0.1000    96.494  289.44
  48   0.5308     86.760  0.0937    96.812  295.57
  49   0.5099     87.100  0.0908    96.762  301.66
  50   0.5065     87.340  0.0864    96.942  307.79
  51   0.5232     86.860  0.0835    96.988  313.93
  52   0.5168     86.930  0.0825    97.128  320.13
  53   0.5315     86.860  0.0818    97.192  326.24
  54   0.5127     87.320  0.0777    97.354  332.39
  55   0.5292     87.060  0.0700    97.598  338.50
  56   0.5180     87.440  0.0691    97.580  344.62
  57   0.5284     87.320  0.0696    97.564  350.72
  58   0.5406     87.080  0.0740    97.438  356.84
  59   0.5492     87.510  0.0662    97.636  363.04
  60   0.5392     87.260  0.0670    97.624  369.17
  61   0.5474     87.300  0.0669    97.676  375.33
  62   0.5491     87.110  0.0670    97.626  381.47
  63   0.5509     86.970  0.0627    97.724  387.57
  64   0.5336     87.190  0.0624    97.816  393.78
  65   0.5442     87.770  0.0568    98.088  399.91
  66   0.5548     87.370  0.0568    98.046  406.02
  67   0.5416     87.820  0.0515    98.134  412.14
  68   0.5289     87.730  0.0536    98.184  418.21
  69   0.5516     87.610  0.0498    98.304  424.35
  70   0.5469     88.100  0.0482    98.378  430.52
  71   0.5422     87.960  0.0468    98.438  436.61
  72   0.5690     87.670  0.0502    98.222  442.70
  73   0.5546     87.830  0.0500    98.262  448.81
  74   0.5437     87.510  0.0523    98.184  454.93
  75   0.5253     87.920  0.0513    98.202  461.02
  76   0.5404     87.730  0.0459    98.442  467.22
  77   0.5586     87.690  0.0476    98.338  473.33
  78   0.5608     87.880  0.0488    98.312  479.47
  79   0.5498     87.810  0.0499    98.338  485.61
  80   0.5952     87.740  0.0419    98.560  491.72
  81   0.5709     88.220  0.0414    98.626  497.91
  82   0.5411     88.250  0.0416    98.606  504.01
  83   0.5889     87.390  0.0450    98.420  510.12
  84   0.5623     87.880  0.0486    98.322  516.23
  85   0.5900     87.770  0.0466    98.364  522.31
