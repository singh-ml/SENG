Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-4', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3812     48.610  1.6451    39.140  7.75
   2   1.1879     56.530  1.2881    52.990  13.91
   3   1.0380     62.620  1.1144    59.952  20.02
   4   0.9721     65.810  0.9773    65.266  26.15
   5   0.8618     69.920  0.8668    69.252  32.28
   6   0.7838     72.500  0.7917    72.062  38.45
   7   0.7259     75.030  0.7177    74.712  44.57
   8   0.6827     76.080  0.6552    76.960  50.70
   9   0.6414     77.530  0.6068    78.720  56.85
  10   0.6205     78.130  0.5660    80.316  62.96
  11   0.5762     80.030  0.5276    81.558  69.13
  12   0.5773     80.530  0.4967    82.734  75.24
  13   0.5443     81.140  0.4650    83.814  81.34
  14   0.5613     80.600  0.4439    84.558  87.47
  15   0.5337     82.220  0.4221    85.360  93.57
  16   0.5254     82.270  0.4012    86.250  99.68
  17   0.5218     82.810  0.3751    86.972  105.91
  18   0.5036     83.490  0.3585    87.672  112.05
  19   0.5159     83.010  0.3413    88.066  118.16
  20   0.4787     84.380  0.3304    88.400  124.30
  21   0.4828     84.200  0.3123    89.200  130.40
  22   0.4842     84.410  0.2935    89.804  136.52
  23   0.4702     85.010  0.2755    90.332  142.71
  24   0.4660     84.970  0.2659    90.702  148.86
  25   0.4899     84.430  0.2532    91.190  154.96
  26   0.4594     85.470  0.2433    91.528  161.08
  27   0.4907     84.570  0.2341    91.914  167.20
  28   0.4698     85.380  0.2201    92.394  173.33
  29   0.4720     85.420  0.2142    92.498  179.51
  30   0.4599     86.190  0.1974    93.106  185.61
  31   0.4613     86.250  0.1824    93.674  191.74
  32   0.4735     85.670  0.1795    93.748  197.88
  33   0.4487     86.370  0.1708    94.010  204.00
  34   0.4844     85.960  0.1643    94.270  210.14
  35   0.4520     86.670  0.1554    94.652  216.36
  36   0.5027     86.170  0.1455    95.076  222.52
  37   0.4800     86.510  0.1368    95.166  228.64
  38   0.4908     86.060  0.1356    95.270  234.75
  39   0.4987     86.420  0.1368    95.344  240.89
  40   0.5121     85.980  0.1298    95.394  247.01
  41   0.4748     86.470  0.1290    95.464  253.23
  42   0.4725     86.950  0.1235    95.664  259.33
  43   0.4912     86.360  0.1168    95.882  265.45
  44   0.4941     87.010  0.1074    96.230  271.56
  45   0.5008     86.850  0.1076    96.236  277.70
  46   0.4890     87.060  0.0976    96.610  283.90
  47   0.4872     87.030  0.0998    96.564  290.01
  48   0.4752     87.580  0.0926    96.776  296.14
  49   0.4917     86.770  0.0897    96.854  302.28
  50   0.5177     86.370  0.0866    97.008  308.41
  51   0.5051     86.870  0.0856    97.026  314.56
  52   0.5002     87.130  0.0853    97.026  320.72
  53   0.4890     87.320  0.0790    97.228  326.82
  54   0.4899     87.560  0.0790    97.304  332.92
  55   0.5058     87.350  0.0761    97.356  339.04
  56   0.5063     87.250  0.0736    97.466  345.17
  57   0.5247     87.150  0.0741    97.450  351.33
  58   0.5254     87.190  0.0740    97.464  357.47
  59   0.5019     87.360  0.0733    97.476  363.61
  60   0.4929     87.440  0.0654    97.730  369.73
  61   0.5077     87.350  0.0635    97.756  375.87
  62   0.5064     87.910  0.0648    97.736  382.01
  63   0.4872     87.730  0.0748    97.402  388.19
  64   0.5121     87.520  0.0648    97.754  394.32
  65   0.5457     87.160  0.0573    97.988  400.43
  66   0.5340     87.650  0.0573    97.946  406.54
  67   0.5156     87.650  0.0602    97.964  412.63
  68   0.5067     87.600  0.0585    97.966  418.83
  69   0.5137     87.830  0.0616    97.884  424.95
  70   0.5101     87.710  0.0576    98.008  431.06
  71   0.4873     88.120  0.0543    98.164  437.18
  72   0.4856     88.270  0.0480    98.380  443.29
  73   0.5357     87.730  0.0474    98.458  449.41
  74   0.4986     87.670  0.0557    98.164  455.61
  75   0.4940     88.140  0.0519    98.222  461.72
  76   0.5012     88.420  0.0482    98.362  467.85
  77   0.5222     87.800  0.0479    98.392  473.97
  78   0.4953     88.030  0.0499    98.284  480.06
  79   0.5020     88.090  0.0459    98.472  486.19
  80   0.5293     87.710  0.0462    98.444  492.32
  81   0.5226     87.680  0.0512    98.326  498.51
  82   0.5045     87.810  0.0507    98.248  504.66
  83   0.4986     88.050  0.0463    98.418  510.83
  84   0.5385     87.720  0.0465    98.380  516.98
  85   0.5203     88.030  0.0464    98.464  523.22
