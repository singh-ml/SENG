Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.6878     35.730  1.9170    28.754  7.91
   2   1.4772     45.760  1.5534    41.990  14.17
   3   1.2729     53.900  1.3521    49.984  20.40
   4   1.1619     57.930  1.1961    56.446  26.60
   5   1.0557     62.290  1.0266    63.266  32.83
   6   0.9213     67.620  0.9273    67.028  39.03
   7   0.9490     67.570  0.8468    69.966  45.35
   8   0.8411     71.060  0.7755    72.716  51.56
   9   0.7045     75.320  0.7176    74.868  57.79
  10   0.6933     75.920  0.6639    76.728  64.01
  11   0.6597     77.310  0.6226    78.410  70.25
  12   0.6347     78.470  0.5872    79.614  76.58
  13   0.5890     79.800  0.5622    80.486  82.78
  14   0.6275     78.310  0.5383    81.444  88.99
  15   0.5721     80.520  0.5308    81.696  95.22
  16   0.5700     80.600  0.5098    82.412  101.43
  17   0.6167     79.150  0.4929    83.132  107.65
  18   0.6098     80.070  0.4813    83.414  113.95
  19   0.5157     82.260  0.4743    83.740  120.15
  20   0.5686     80.820  0.4572    84.152  126.42
  21   0.5371     82.430  0.4459    84.664  132.64
  22   0.5495     81.720  0.4398    84.806  138.86
  23   0.4959     83.130  0.4347    84.940  145.08
  24   0.5152     83.290  0.4336    85.164  151.41
  25   0.4987     83.060  0.4248    85.308  157.65
  26   0.4836     83.670  0.4154    85.756  163.84
  27   0.4973     83.600  0.4101    85.864  170.10
  28   0.4743     84.090  0.4082    85.834  176.34
  29   0.5206     82.880  0.3961    86.208  182.60
  30   0.4969     83.290  0.3990    86.246  188.93
  31   0.5005     83.340  0.3968    86.258  195.11
  32   0.4674     84.320  0.3860    86.752  201.31
  33   0.4631     84.390  0.3974    86.312  207.55
  34   0.5475     81.770  0.3812    86.732  213.82
  35   0.4743     83.450  0.3854    86.636  220.13
  36   0.4777     83.570  0.3750    87.048  226.38
  37   0.4732     84.170  0.3800    86.928  232.65
  38   0.4611     84.400  0.3778    86.958  238.87
  39   0.4588     84.600  0.3703    87.120  245.08
  40   0.4564     84.360  0.3666    87.418  251.36
  41   0.4587     84.470  0.3724    87.164  257.54
  42   0.4700     84.430  0.3652    87.432  263.75
  43   0.4508     85.020  0.3653    87.418  269.96
  44   0.4730     84.620  0.3715    87.234  276.18
  45   0.4950     83.730  0.3596    87.550  282.40
  46   0.4460     84.990  0.3577    87.674  288.66
  47   0.5373     82.450  0.3630    87.614  294.86
  48   0.4254     85.870  0.3597    87.758  301.07
  49   0.4409     85.210  0.3588    87.598  307.27
  50   0.4686     84.950  0.3458    88.084  313.53
  51   0.4519     84.690  0.3524    87.882  319.83
  52   0.4416     85.090  0.3533    87.902  326.04
  53   0.4496     85.070  0.3478    87.992  332.28
  54   0.4353     85.520  0.3488    87.908  338.57
  55   0.4647     84.820  0.3439    88.150  344.78
  56   0.5067     83.300  0.3499    88.056  351.01
  57   0.4521     84.610  0.3456    88.150  357.24
  58   0.4532     84.410  0.3493    88.070  363.56
  59   0.4591     84.720  0.3426    88.286  369.77
  60   0.4885     84.090  0.3412    88.270  375.99
  61   0.4345     85.330  0.3440    88.042  382.23
  62   0.4328     85.430  0.3443    88.234  388.49
  63   0.4359     85.480  0.3363    88.330  394.81
  64   0.4044     86.460  0.3402    88.248  401.03
  65   0.4856     83.850  0.3393    88.112  407.24
  66   0.4432     85.350  0.3317    88.470  413.47
  67   0.4175     85.640  0.3324    88.496  419.65
  68   0.4422     85.660  0.3358    88.410  425.90
  69   0.4243     85.810  0.3346    88.284  432.14
  70   0.4704     84.080  0.3343    88.338  438.39
  71   0.4434     85.150  0.3453    88.108  444.62
  72   0.4270     85.620  0.3376    88.422  450.81
  73   0.4747     84.210  0.3319    88.632  457.01
  74   0.4523     85.420  0.3309    88.360  463.33
  75   0.4420     85.340  0.3380    88.430  469.59
  76   0.4181     86.480  0.3300    88.514  475.82
  77   0.3834     87.090  0.3305    88.632  482.07
  78   0.4122     85.590  0.3306    88.634  488.28
  79   0.4435     85.650  0.3301    88.582  494.57
  80   0.4107     86.050  0.3260    88.796  500.79
  81   0.4176     85.770  0.3323    88.674  507.04
  82   0.3964     86.760  0.3297    88.592  513.29
  83   0.4673     85.150  0.3314    88.530  519.53
  84   0.4188     86.060  0.3397    88.324  525.74
  85   0.3975     86.390  0.3325    88.532  531.97
