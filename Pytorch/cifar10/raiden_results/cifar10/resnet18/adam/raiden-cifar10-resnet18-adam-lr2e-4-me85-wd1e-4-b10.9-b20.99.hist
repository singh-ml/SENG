Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2489     55.940  1.4803    45.356  7.73
   2   1.0515     63.360  1.0768    61.214  13.89
   3   0.9634     67.340  0.8729    68.864  20.08
   4   0.7403     74.140  0.7399    74.054  26.27
   5   0.7346     74.910  0.6524    77.464  32.49
   6   0.7907     73.140  0.5813    79.608  38.68
   7   0.7196     76.860  0.5303    81.402  44.86
   8   0.6317     79.010  0.4836    83.066  51.04
   9   0.5692     81.080  0.4496    84.226  57.20
  10   0.5367     82.220  0.4148    85.532  63.41
  11   0.5891     80.420  0.3842    86.718  69.65
  12   0.4886     83.850  0.3653    87.292  75.83
  13   0.5936     80.480  0.3406    88.224  81.99
  14   0.5141     84.020  0.3177    89.032  88.16
  15   0.5544     82.520  0.2994    89.516  94.35
  16   0.5254     82.840  0.2806    90.236  100.59
  17   0.4741     84.970  0.2661    90.758  106.75
  18   0.4409     85.840  0.2483    91.324  112.95
  19   0.4968     84.460  0.2366    91.782  119.11
  20   0.4873     84.980  0.2277    92.156  125.30
  21   0.5521     83.580  0.2151    92.374  131.48
  22   0.6209     81.470  0.2018    92.956  137.76
  23   0.4716     85.870  0.1948    93.164  143.97
  24   0.4471     86.700  0.1798    93.584  150.16
  25   0.4412     86.590  0.1792    93.692  156.36
  26   0.4840     86.180  0.1661    94.180  162.53
  27   0.5012     85.480  0.1586    94.488  168.76
  28   0.4304     86.950  0.1508    94.790  174.98
  29   0.4181     87.820  0.1443    94.914  181.14
  30   0.4078     88.550  0.1380    95.158  187.33
  31   0.4770     86.480  0.1301    95.522  193.50
  32   0.4232     88.650  0.1284    95.446  199.69
  33   0.5165     86.020  0.1209    95.634  205.92
  34   0.5543     85.290  0.1185    95.786  212.11
  35   0.4538     87.460  0.1106    96.180  218.30
  36   0.4161     88.370  0.1103    96.014  224.47
  37   0.4505     88.060  0.1034    96.292  230.65
  38   0.4563     87.930  0.0972    96.646  236.84
  39   0.4273     88.360  0.0973    96.578  243.08
  40   0.3973     88.740  0.0941    96.758  249.25
  41   0.4665     87.900  0.0898    96.804  255.44
  42   0.4672     87.940  0.0890    96.916  261.63
  43   0.4480     88.720  0.0856    96.994  267.80
  44   0.4111     89.150  0.0824    97.184  274.01
  45   0.4915     87.310  0.0806    97.146  280.20
  46   0.4398     88.920  0.0784    97.204  286.36
  47   0.4482     88.270  0.0764    97.316  292.54
  48   0.5142     87.470  0.0748    97.400  298.74
  49   0.4132     89.550  0.0718    97.490  304.94
  50   0.4180     88.960  0.0729    97.472  311.23
  51   0.5009     88.290  0.0683    97.636  317.42
  52   0.4335     89.120  0.0687    97.626  323.62
  53   0.4712     88.450  0.0650    97.644  329.83
  54   0.4178     89.340  0.0664    97.700  336.05
  55   0.5176     87.520  0.0616    97.868  342.31
  56   0.4512     88.970  0.0622    97.882  348.50
  57   0.4523     89.430  0.0590    97.908  354.67
  58   0.4408     89.040  0.0579    98.032  360.88
  59   0.4169     89.850  0.0596    97.920  367.09
  60   0.5329     88.370  0.0568    98.056  373.28
  61   0.6941     85.920  0.0569    98.042  379.51
  62   0.4860     89.050  0.0549    98.078  385.70
  63   0.4385     89.700  0.0559    98.002  391.89
  64   0.4606     89.390  0.0557    98.038  398.07
  65   0.4302     89.470  0.0532    98.094  404.26
  66   0.4401     89.830  0.0528    98.206  410.45
  67   0.4442     90.060  0.0519    98.186  416.76
  68   0.4375     89.860  0.0494    98.308  422.95
  69   0.4615     89.510  0.0480    98.330  429.14
  70   0.5364     88.160  0.0516    98.230  435.30
  71   0.4375     89.880  0.0486    98.318  441.45
  72   0.4734     88.860  0.0478    98.330  447.65
  73   0.4335     90.030  0.0498    98.276  453.93
  74   0.4271     90.060  0.0475    98.346  460.10
  75   0.4688     90.130  0.0443    98.418  466.28
  76   0.4235     90.470  0.0473    98.338  472.46
  77   0.4666     89.490  0.0440    98.468  478.63
  78   0.4659     89.700  0.0443    98.460  484.81
  79   0.4417     89.760  0.0453    98.440  491.07
  80   0.4535     89.880  0.0440    98.484  497.26
  81   0.4598     89.480  0.0447    98.466  503.46
  82   0.4536     89.630  0.0413    98.538  509.65
  83   0.4862     89.060  0.0417    98.596  515.86
  84   0.4879     88.990  0.0412    98.598  522.04
  85   0.4255     89.950  0.0409    98.606  528.30
