Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3254     56.880  1.3861    49.418  7.71
   2   1.1101     63.750  0.9187    67.354  13.77
   3   0.7219     74.520  0.7340    74.562  19.87
   4   0.9182     70.770  0.6229    78.342  25.93
   5   0.7617     74.800  0.5438    81.212  31.97
   6   0.9335     72.770  0.4872    83.228  38.04
   7   0.5831     81.140  0.4516    84.436  44.17
   8   0.6856     79.490  0.4168    85.568  50.20
   9   0.5901     81.270  0.3848    86.902  56.23
  10   0.4848     84.310  0.3564    87.640  62.30
  11   0.4624     84.880  0.3355    88.356  68.34
  12   0.5087     83.760  0.3164    89.032  74.41
  13   0.4428     85.310  0.2941    89.924  80.48
  14   0.4271     86.300  0.2775    90.386  86.55
  15   0.5081     83.600  0.2685    90.606  92.56
  16   0.4864     85.360  0.2489    91.438  98.59
  17   0.4506     85.960  0.2395    91.802  104.66
  18   0.4270     86.580  0.2307    91.960  110.73
  19   0.4231     86.710  0.2204    92.558  116.86
  20   0.4687     85.800  0.2101    92.666  122.89
  21   0.4606     85.940  0.2037    92.986  128.95
  22   0.4178     87.320  0.1950    93.264  135.02
  23   0.3961     87.920  0.1874    93.562  141.04
  24   0.3824     88.490  0.1813    93.764  147.13
  25   0.4426     87.330  0.1719    93.998  153.19
  26   0.3875     89.000  0.1677    94.148  159.22
  27   0.3341     89.740  0.1601    94.426  165.25
  28   0.3750     89.000  0.1575    94.572  171.32
  29   0.3444     89.620  0.1506    94.828  177.39
  30   0.4292     88.420  0.1495    94.716  183.43
  31   0.3075     90.990  0.1435    95.066  189.50
  32   0.4297     88.390  0.1402    95.126  195.57
  33   0.5316     85.920  0.1329    95.386  201.60
  34   0.3838     89.770  0.1337    95.282  207.67
  35   0.3595     89.910  0.1297    95.560  213.81
  36   0.4125     88.590  0.1264    95.686  219.82
  37   0.4024     88.810  0.1234    95.680  225.87
  38   0.3542     89.740  0.1205    95.926  231.91
  39   0.4232     88.440  0.1224    95.746  237.97
  40   0.3722     89.710  0.1189    95.802  244.09
  41   0.3587     90.240  0.1162    95.974  250.15
  42   0.3867     89.480  0.1093    96.264  256.23
  43   0.3622     89.950  0.1153    96.076  262.27
  44   0.3621     90.160  0.1154    95.932  268.32
  45   0.3950     88.670  0.1061    96.318  274.36
  46   0.3659     90.050  0.1090    96.144  280.43
  47   0.4524     88.390  0.1014    96.492  286.50
  48   0.3882     89.890  0.1061    96.290  292.54
  49   0.3750     90.130  0.1017    96.462  298.58
  50   0.3378     91.070  0.0994    96.544  304.61
  51   0.4526     88.410  0.0988    96.606  310.62
  52   0.3297     90.770  0.1016    96.480  316.76
  53   0.3830     90.070  0.0955    96.672  322.81
  54   0.3685     90.860  0.0955    96.748  328.86
  55   0.3769     90.420  0.0934    96.796  334.89
  56   0.3837     90.050  0.0960    96.658  340.93
  57   0.4095     89.060  0.0922    96.750  346.97
  58   0.3584     90.580  0.0931    96.786  353.08
  59   0.3706     90.140  0.0939    96.698  359.15
  60   0.4126     89.830  0.0885    96.912  365.17
  61   0.3702     90.450  0.0883    96.948  371.26
  62   0.4316     88.670  0.0908    96.830  377.34
  63   0.4341     88.990  0.0895    96.856  383.45
  64   0.3780     90.320  0.0907    96.894  389.59
  65   0.3530     90.390  0.0863    96.894  395.64
  66   0.3746     90.610  0.0856    97.056  401.73
  67   0.3543     90.820  0.0850    97.024  407.83
  68   0.3480     90.860  0.0862    96.966  413.92
  69   0.3853     90.330  0.0814    97.190  419.97
  70   0.4221     89.450  0.0845    97.098  426.10
  71   0.3411     91.340  0.0830    97.162  432.16
  72   0.3307     91.320  0.0826    97.200  438.23
  73   0.4157     89.640  0.0828    97.126  444.29
  74   0.3478     90.860  0.0808    97.188  450.33
  75   0.3810     90.230  0.0825    97.112  456.39
  76   0.3527     91.250  0.0824    97.224  462.44
  77   0.4041     90.020  0.0797    97.336  468.61
  78   0.4167     89.540  0.0804    97.224  474.68
  79   0.4036     90.480  0.0778    97.270  480.77
  80   0.3932     90.240  0.0802    97.162  486.83
  81   0.4111     89.530  0.0783    97.280  492.90
  82   0.4106     90.320  0.0771    97.410  498.98
  83   0.3560     90.870  0.0842    97.112  505.13
  84   0.3540     91.020  0.0816    97.202  511.18
  85   0.3775     90.800  0.0758    97.352  517.24
