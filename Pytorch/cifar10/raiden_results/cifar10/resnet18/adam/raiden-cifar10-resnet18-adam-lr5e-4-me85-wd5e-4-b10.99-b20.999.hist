Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2771     53.990  1.5066    44.476  7.71
   2   1.0719     62.540  1.1124    59.754  13.80
   3   0.9500     66.630  0.9190    67.070  19.94
   4   0.8178     71.840  0.7836    72.294  25.99
   5   0.6722     76.700  0.6853    76.046  32.05
   6   0.6317     78.690  0.6003    79.070  38.09
   7   0.7781     73.830  0.5491    80.988  44.14
   8   0.6252     79.050  0.5104    82.360  50.25
   9   0.5044     82.630  0.4725    83.670  56.27
  10   0.5377     81.810  0.4332    85.180  62.32
  11   0.5273     82.540  0.4051    86.044  68.34
  12   0.5158     82.510  0.3927    86.458  74.38
  13   0.4674     84.500  0.3750    87.090  80.41
  14   0.4555     85.070  0.3476    88.210  86.52
  15   0.4156     85.870  0.3294    88.622  92.57
  16   0.4193     86.290  0.3154    89.158  98.62
  17   0.4098     86.970  0.3035    89.552  104.66
  18   0.4152     86.030  0.2910    90.056  110.69
  19   0.4854     85.220  0.2798    90.334  116.72
  20   0.4688     85.060  0.2674    90.794  122.80
  21   0.3885     87.130  0.2560    91.366  128.85
  22   0.3884     87.720  0.2436    91.624  134.87
  23   0.3904     86.960  0.2312    92.236  140.91
  24   0.3431     89.050  0.2267    92.122  146.96
  25   0.4298     86.800  0.2211    92.542  153.09
  26   0.4052     87.580  0.2132    92.590  159.11
  27   0.3776     88.580  0.2014    93.140  165.15
  28   0.3907     87.440  0.1985    93.266  171.19
  29   0.3688     88.610  0.1939    93.416  177.25
  30   0.3739     89.210  0.1946    93.300  183.27
  31   0.3793     88.490  0.1808    93.832  189.39
  32   0.3375     89.380  0.1746    94.066  195.42
  33   0.3570     89.020  0.1709    94.106  201.44
  34   0.3932     88.330  0.1668    94.186  207.45
  35   0.3408     89.360  0.1717    94.144  213.50
  36   0.3861     88.660  0.1644    94.404  219.58
  37   0.3442     89.870  0.1564    94.628  225.63
  38   0.3705     88.970  0.1559    94.774  231.66
  39   0.3219     90.360  0.1448    95.078  237.67
  40   0.3533     89.630  0.1431    95.094  243.71
  41   0.3620     89.730  0.1454    95.110  249.73
  42   0.3372     90.220  0.1391    95.218  255.85
  43   0.3828     88.770  0.1458    94.952  261.90
  44   0.3468     89.860  0.1389    95.260  267.95
  45   0.3164     90.670  0.1425    95.008  273.98
  46   0.3669     89.320  0.1356    95.354  280.03
  47   0.3482     89.910  0.1328    95.542  286.11
  48   0.3402     89.890  0.1199    95.930  292.15
  49   0.3514     89.950  0.1278    95.606  298.22
  50   0.3211     90.170  0.1223    95.888  304.29
  51   0.3437     90.280  0.1253    95.776  310.32
  52   0.3071     90.570  0.1315    95.494  316.40
  53   0.3459     90.480  0.1190    96.062  322.45
  54   0.3072     90.720  0.1202    95.896  328.47
  55   0.3302     90.590  0.1151    96.068  334.52
  56   0.3804     89.480  0.1168    95.990  340.54
  57   0.3544     89.950  0.1160    96.000  346.57
  58   0.3317     90.260  0.1124    96.224  352.68
  59   0.3606     89.730  0.1083    96.384  358.70
  60   0.3320     90.440  0.1090    96.344  364.71
  61   0.3313     90.690  0.1064    96.362  370.76
  62   0.3493     90.360  0.1070    96.372  376.83
  63   0.3372     90.370  0.1092    96.378  382.95
  64   0.3216     90.510  0.1135    96.198  388.97
  65   0.3425     90.650  0.0982    96.740  395.05
  66   0.3378     90.560  0.1000    96.656  401.10
  67   0.3247     90.760  0.1026    96.496  407.13
  68   0.3410     90.530  0.1034    96.446  413.19
  69   0.3318     90.790  0.0989    96.690  419.28
  70   0.3130     90.940  0.0992    96.698  425.32
  71   0.3329     91.090  0.1003    96.644  431.35
  72   0.3468     90.200  0.1004    96.646  437.40
  73   0.3487     90.490  0.0974    96.668  443.44
  74   0.3343     90.880  0.0993    96.610  449.45
  75   0.3300     91.220  0.0934    96.770  455.56
  76   0.3334     90.560  0.0913    96.874  461.59
  77   0.3394     90.840  0.0938    96.778  467.64
  78   0.3446     90.720  0.0941    96.790  473.67
  79   0.3558     90.400  0.1059    96.408  479.75
  80   0.3118     90.930  0.0954    96.762  485.80
  81   0.3368     90.790  0.0933    96.914  491.92
  82   0.3378     90.870  0.0955    96.822  497.95
  83   0.3363     90.680  0.0979    96.664  504.01
  84   0.3520     90.760  0.0913    96.904  510.02
  85   0.3324     91.150  0.0911    96.902  516.06
