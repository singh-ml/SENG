Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-4', '--weight-decay', '2e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2689     56.530  1.3940    48.846  7.71
   2   1.1132     63.740  0.9330    66.770  13.88
   3   0.9040     70.070  0.7309    74.334  19.97
   4   0.8440     72.850  0.6256    78.064  26.11
   5   0.6108     79.420  0.5437    81.202  32.20
   6   0.6189     79.580  0.5001    82.846  38.26
   7   0.5499     82.370  0.4604    84.052  44.35
   8   0.6838     78.210  0.4187    85.664  50.50
   9   0.5437     81.950  0.3857    86.458  56.56
  10   0.5277     83.110  0.3604    87.412  62.61
  11   0.4585     84.930  0.3396    88.238  68.66
  12   0.5216     83.800  0.3187    88.860  74.73
  13   0.4707     85.030  0.2956    89.742  80.77
  14   0.6149     81.490  0.2846    90.096  86.89
  15   0.4094     86.810  0.2706    90.592  92.99
  16   0.5454     83.440  0.2587    91.218  99.05
  17   0.4758     84.990  0.2464    91.390  105.10
  18   0.4712     85.560  0.2355    91.816  111.19
  19   0.4411     86.400  0.2253    92.196  117.26
  20   0.4131     86.590  0.2148    92.540  123.43
  21   0.5532     83.830  0.2050    92.832  129.53
  22   0.4505     87.010  0.1959    93.004  135.58
  23   0.3935     88.260  0.1924    93.322  141.64
  24   0.4501     86.990  0.1856    93.590  147.72
  25   0.4062     87.550  0.1728    93.936  153.79
  26   0.4340     87.420  0.1686    94.176  159.94
  27   0.4589     87.410  0.1603    94.474  166.03
  28   0.3700     89.130  0.1537    94.572  172.09
  29   0.4298     87.820  0.1514    94.828  178.16
  30   0.4292     87.530  0.1505    94.768  184.22
  31   0.4720     87.160  0.1433    95.094  190.30
  32   0.3893     88.770  0.1414    95.092  196.43
  33   0.3883     89.050  0.1359    95.338  202.50
  34   0.4357     87.650  0.1285    95.402  208.59
  35   0.4692     87.120  0.1260    95.760  214.68
  36   0.3685     89.650  0.1264    95.580  220.73
  37   0.4277     87.850  0.1218    95.764  226.87
  38   0.3838     89.310  0.1156    95.980  232.95
  39   0.5423     85.430  0.1150    96.044  239.01
  40   0.3411     90.570  0.1117    96.074  245.06
  41   0.3889     89.450  0.1096    96.200  251.12
  42   0.4699     88.050  0.1064    96.336  257.16
  43   0.3772     89.580  0.1090    96.264  263.31
  44   0.4321     87.850  0.1048    96.390  269.35
  45   0.3570     90.240  0.1000    96.516  275.40
  46   0.3732     90.020  0.0976    96.616  281.47
  47   0.4388     88.660  0.0983    96.548  287.54
  48   0.4843     88.050  0.0988    96.568  293.66
  49   0.4362     88.630  0.0956    96.746  299.73
  50   0.4684     88.270  0.0935    96.758  305.79
  51   0.3836     89.620  0.0923    96.772  311.88
  52   0.3362     90.510  0.0907    96.872  317.93
  53   0.4059     88.980  0.0896    96.882  323.98
  54   0.3741     90.210  0.0898    96.880  330.10
  55   0.4098     89.140  0.0840    97.108  336.18
  56   0.3616     90.570  0.0865    97.072  342.24
  57   0.3704     90.280  0.0857    96.954  348.33
  58   0.4334     88.890  0.0843    97.070  354.37
  59   0.3505     90.700  0.0845    97.148  360.50
  60   0.3799     90.010  0.0795    97.296  366.56
  61   0.4074     89.610  0.0803    97.234  372.61
  62   0.3816     90.130  0.0854    97.104  378.68
  63   0.3901     90.210  0.0765    97.424  384.75
  64   0.3325     91.060  0.0848    97.090  390.80
  65   0.3957     90.030  0.0767    97.318  396.95
  66   0.3538     90.530  0.0772    97.396  403.03
  67   0.3707     90.290  0.0774    97.398  409.09
  68   0.3815     90.370  0.0744    97.470  415.17
  69   0.4194     89.560  0.0765    97.414  421.25
  70   0.3941     89.570  0.0716    97.630  427.33
  71   0.3980     89.870  0.0775    97.326  433.48
  72   0.3798     90.140  0.0736    97.498  439.53
  73   0.3711     90.160  0.0726    97.508  445.62
  74   0.3653     90.400  0.0704    97.562  451.68
  75   0.3541     90.610  0.0742    97.412  457.75
  76   0.3745     90.310  0.0687    97.676  463.82
  77   0.4505     89.000  0.0688    97.636  469.97
  78   0.4194     89.980  0.0723    97.580  476.03
  79   0.3459     90.430  0.0756    97.424  482.08
  80   0.3917     90.050  0.0694    97.658  488.15
  81   0.3738     90.580  0.0649    97.782  494.20
  82   0.3676     90.660  0.0668    97.658  500.26
  83   0.4798     89.010  0.0659    97.742  506.39
  84   0.3653     90.710  0.0705    97.590  512.45
  85   0.3813     90.580  0.0673    97.722  518.54
