Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4192478208 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2367     54.270  1.4754    45.460  7.78
   2   1.0533     62.980  1.0772    61.138  13.89
   3   0.8420     70.870  0.8736    68.932  19.95
   4   0.7757     73.470  0.7337    74.260  26.07
   5   0.7347     75.450  0.6462    77.296  32.17
   6   0.6976     76.300  0.5834    79.500  38.24
   7   0.6649     77.530  0.5350    81.236  44.33
   8   0.5968     79.980  0.4983    82.574  50.42
   9   0.5564     81.090  0.4545    84.070  56.53
  10   0.6251     78.850  0.4272    85.156  62.64
  11   0.5146     82.560  0.4020    86.096  68.71
  12   0.5392     81.680  0.3761    86.990  74.80
  13   0.4881     83.520  0.3538    87.858  80.88
  14   0.6239     79.460  0.3313    88.598  87.00
  15   0.4598     85.330  0.3171    88.998  93.06
  16   0.5147     83.470  0.3028    89.630  99.24
  17   0.5263     83.430  0.2868    90.000  105.33
  18   0.4951     84.360  0.2721    90.598  111.40
  19   0.4074     86.460  0.2605    90.918  117.46
  20   0.4264     86.560  0.2456    91.692  123.57
  21   0.4679     85.120  0.2319    91.916  129.69
  22   0.4154     86.970  0.2264    92.054  135.80
  23   0.4543     85.770  0.2156    92.352  141.92
  24   0.5066     83.880  0.2085    92.712  148.02
  25   0.4634     86.480  0.1967    93.182  154.08
  26   0.4124     86.790  0.1898    93.428  160.14
  27   0.4476     86.120  0.1824    93.660  166.19
  28   0.5270     84.760  0.1767    93.910  172.31
  29   0.4418     86.370  0.1683    94.256  178.38
  30   0.4251     87.280  0.1631    94.386  184.45
  31   0.4287     87.010  0.1560    94.496  190.56
  32   0.4343     87.130  0.1522    94.766  196.61
  33   0.4870     85.900  0.1437    94.936  202.67
  34   0.3951     88.090  0.1454    94.896  208.82
  35   0.4510     86.800  0.1368    95.312  214.91
  36   0.4858     85.750  0.1294    95.468  220.99
  37   0.4303     87.680  0.1299    95.528  227.05
  38   0.3995     88.290  0.1278    95.554  233.12
  39   0.4375     87.650  0.1215    95.844  239.26
  40   0.4320     88.210  0.1192    95.846  245.36
  41   0.4169     88.440  0.1158    95.974  251.46
  42   0.4664     87.240  0.1138    96.094  257.55
  43   0.4541     87.730  0.1116    96.210  263.66
  44   0.4078     88.110  0.1075    96.280  269.75
  45   0.4291     88.080  0.1031    96.454  275.82
  46   0.4766     87.140  0.1013    96.466  281.96
  47   0.4891     86.970  0.1033    96.456  288.01
  48   0.4588     87.470  0.1000    96.606  294.09
  49   0.4290     88.910  0.0940    96.790  300.16
  50   0.4034     88.640  0.0933    96.822  306.24
  51   0.4099     89.100  0.0913    96.904  312.35
  52   0.4728     87.680  0.0851    97.066  318.40
  53   0.4495     87.880  0.0894    96.946  324.46
  54   0.4058     89.230  0.0865    97.150  330.54
  55   0.4596     88.120  0.0831    97.180  336.64
  56   0.4192     88.560  0.0869    97.000  342.68
  57   0.4507     87.750  0.0856    97.082  348.85
  58   0.4659     87.860  0.0742    97.422  354.93
  59   0.3849     89.640  0.0795    97.274  361.05
  60   0.4012     89.430  0.0808    97.220  367.13
  61   0.4405     88.540  0.0774    97.336  373.22
  62   0.4300     88.320  0.0736    97.512  379.34
  63   0.4358     88.790  0.0772    97.398  385.42
  64   0.4198     89.120  0.0754    97.432  391.48
  65   0.4468     88.730  0.0720    97.554  397.54
  66   0.4127     89.440  0.0738    97.484  403.65
  67   0.3818     89.740  0.0749    97.478  409.72
  68   0.4452     88.480  0.0697    97.618  415.87
  69   0.4227     88.720  0.0685    97.718  421.99
  70   0.4133     89.120  0.0709    97.688  428.07
  71   0.5079     87.250  0.0711    97.594  434.14
  72   0.4272     88.550  0.0664    97.794  440.22
  73   0.5006     87.780  0.0687    97.660  446.39
  74   0.4093     89.690  0.0658    97.756  452.45
  75   0.5322     86.420  0.0640    97.802  458.54
  76   0.3842     89.860  0.0642    97.906  464.60
  77   0.3982     89.310  0.0655    97.740  470.65
  78   0.3633     90.300  0.0652    97.780  476.75
  79   0.4417     89.200  0.0627    97.852  482.84
  80   0.4152     89.550  0.0600    97.960  488.97
  81   0.4058     89.440  0.0598    97.932  495.07
  82   0.3884     90.010  0.0632    97.862  501.14
  83   0.3818     89.810  0.0626    97.920  507.24
  84   0.3831     89.910  0.0604    97.990  513.30
  85   0.3774     90.040  0.0599    97.942  519.41
