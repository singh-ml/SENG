Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '10', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6341272576 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5087     45.640  1.6672    37.250  19.49
   2   1.3164     54.320  1.2506    54.644  36.90
   3   0.9755     65.900  1.0178    63.666  54.32
   4   1.1755     61.520  0.8663    69.264  71.91
   5   0.8672     70.740  0.7597    73.314  89.32
   6   0.8598     72.200  0.6811    76.056  106.95
   7   0.6376     78.240  0.6044    78.846  124.35
   8   0.6470     78.450  0.5643    80.276  141.82
   9   0.5731     80.760  0.5226    81.706  159.22
  10   0.5868     80.100  0.4827    83.234  176.64
  11   0.5571     81.540  0.4470    84.400  194.22
  12   0.5240     82.810  0.4178    85.436  211.61
  13   0.4994     83.990  0.3967    85.984  229.26
  14   0.5199     83.770  0.3734    87.058  246.67
  15   0.4908     84.150  0.3553    87.640  264.17
  16   0.6007     82.480  0.3335    88.348  281.58
  17   0.5242     83.460  0.3146    88.950  299.05
  18   0.5603     82.560  0.2986    89.596  316.66
  19   0.4834     84.870  0.2826    90.168  334.04
  20   0.5115     84.470  0.2740    90.472  351.64
  21   0.4574     85.710  0.2533    91.216  369.05
  22   0.5341     84.680  0.2418    91.434  386.54
  23   0.4570     86.650  0.2296    91.900  403.93
  24   0.5379     84.540  0.2165    92.358  421.36
  25   0.5502     84.100  0.2096    92.570  438.95
  26   0.4364     87.290  0.1954    93.122  456.41
  27   0.6000     84.020  0.1891    93.338  474.03
  28   0.5162     85.510  0.1784    93.650  491.44
  29   0.4307     87.730  0.1655    94.196  508.94
  30   0.5261     86.010  0.1631    94.282  526.34
  31   0.4705     87.200  0.1622    94.102  543.88
  32   0.4584     87.290  0.1423    94.974  561.32
  33   0.4794     86.790  0.1371    95.154  578.72
  34   0.4509     87.920  0.1342    95.252  596.32
  35   0.5498     85.920  0.1255    95.532  613.75
  36   0.4866     87.480  0.1262    95.460  631.26
  37   0.5154     87.290  0.1175    95.806  648.67
  38   0.4681     87.930  0.1134    96.018  666.03
  39   0.5351     86.790  0.1045    96.318  683.63
  40   0.4726     88.170  0.1013    96.378  701.04
  41   0.4855     87.810  0.0895    96.762  718.65
  42   0.5216     87.600  0.0929    96.698  736.07
  43   0.4787     88.570  0.0870    96.898  753.58
  44   0.5328     87.890  0.0840    96.990  770.99
  45   0.5131     88.290  0.0858    96.904  788.43
  46   0.5209     87.930  0.0758    97.350  805.85
  47   0.5727     88.000  0.0737    97.350  823.25
  48   0.5271     88.280  0.0646    97.758  840.88
  49   0.4772     88.800  0.0674    97.646  858.27
  50   0.5181     88.610  0.0662    97.564  875.66
  51   0.5526     88.280  0.0623    97.754  893.26
  52   0.5227     88.330  0.0647    97.714  910.65
  53   0.6225     86.820  0.0569    97.988  928.24
  54   0.5339     89.050  0.0562    97.994  945.67
  55   0.4928     88.970  0.0492    98.256  963.34
  56   0.5019     89.660  0.0513    98.162  980.78
  57   0.6026     88.270  0.0470    98.406  998.31
  58   0.5545     88.670  0.0506    98.296  1015.72
  59   0.5644     88.440  0.0462    98.384  1033.12
  60   0.5389     89.310  0.0450    98.446  1050.73
  61   0.5195     89.230  0.0407    98.550  1068.17
  62   0.5312     89.090  0.0394    98.638  1085.70
  63   0.4907     89.580  0.0411    98.508  1103.11
  64   0.5569     88.590  0.0385    98.650  1120.54
  65   0.5325     89.200  0.0359    98.786  1137.95
  66   0.5286     89.320  0.0345    98.814  1155.33
  67   0.5604     89.200  0.0368    98.710  1172.92
  68   0.5483     89.300  0.0312    98.902  1190.29
  69   0.5308     89.580  0.0353    98.810  1207.88
  70   0.5560     89.200  0.0274    99.036  1225.29
  71   0.5226     89.830  0.0303    98.946  1242.66
  72   0.5670     89.580  0.0314    98.902  1260.30
  73   0.5573     89.440  0.0289    99.026  1277.69
  74   0.5520     89.520  0.0278    99.002  1295.31
  75   0.5134     89.940  0.0290    98.958  1312.73
  76   0.5559     89.800  0.0286    99.044  1330.26
  77   0.5316     90.280  0.0236    99.194  1347.67
  78   0.6342     88.630  0.0268    99.078  1365.20
  79   0.5529     89.570  0.0237    99.194  1382.63
  80   0.5558     89.840  0.0221    99.258  1400.05
  81   0.5430     90.020  0.0211    99.296  1417.67
  82   0.5595     90.120  0.0199    99.322  1435.09
  83   0.5403     89.980  0.0190    99.342  1452.73
  84   0.5921     89.280  0.0207    99.290  1470.16
  85   0.5788     89.420  0.0198    99.296  1487.73
