Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4476480512 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.3205     12.210  3.5772    11.026  9.14
   2   2.3210     12.300  3.5772    11.032  16.65
   3   2.3207     12.240  3.5773    11.046  24.21
   4   2.3209     12.290  3.5759    11.088  31.78
   5   2.3208     12.340  3.5764    11.300  39.22
   6   2.3208     12.220  3.5762    11.070  46.66
   7   2.3213     12.330  3.5763    11.184  54.19
   8   2.3208     12.250  3.5769    11.328  61.60
   9   2.3213     12.310  3.5776    11.016  69.14
  10   2.3214     12.360  3.5753    11.302  76.84
  11   2.3213     12.190  3.5769    11.206  84.28
  12   2.3214     12.250  3.5774    11.194  91.78
  13   2.3210     12.300  3.5764    11.260  99.24
  14   2.3210     12.350  3.5760    11.182  106.63
  15   2.3209     12.350  3.5759    11.256  114.14
  16   2.3208     12.360  3.5758    11.358  121.63
  17   2.3210     12.230  3.5760    11.152  129.11
  18   2.3215     12.220  3.5761    11.124  136.67
  19   2.3208     12.290  3.5760    11.182  144.27
  20   2.3209     12.350  3.5772    11.220  151.72
  21   2.3212     12.270  3.5766    11.308  159.09
  22   2.3211     12.440  3.5768    11.150  166.50
  23   2.3208     12.270  3.5754    11.420  174.03
  24   2.3215     12.180  3.5759    11.302  181.47
  25   2.3214     12.340  3.5759    11.386  188.90
  26   2.3214     12.150  3.5759    11.296  196.33
  27   2.3212     12.260  3.5755    11.152  203.89
  28   2.3210     12.230  3.5765    11.340  211.51
  29   2.3212     12.260  3.5761    10.924  219.03
  30   2.3212     12.350  3.5763    11.064  226.48
  31   2.3210     12.380  3.5760    11.414  233.88
  32   2.3213     12.240  3.5764    11.050  241.51
  33   2.3207     12.370  3.5773    11.352  248.98
  34   2.3215     12.240  3.5759    11.182  256.42
  35   2.3208     12.360  3.5759    11.296  263.84
  36   2.3211     12.270  3.5759    11.222  271.29
  37   2.3214     12.230  3.5756    11.106  278.80
  38   2.3211     12.220  3.5776    11.224  286.40
  39   2.3211     12.320  3.5758    11.288  293.77
  40   2.3211     12.330  3.5754    11.278  301.22
  41   2.3210     12.350  3.5762    11.196  308.89
  42   2.3211     12.170  3.5763    11.036  316.68
  43   2.3211     12.270  3.5763    11.288  324.16
  44   2.3210     12.370  3.5762    11.196  331.55
  45   2.3212     12.360  3.5768    11.358  339.05
  46   2.3207     12.320  3.5761    11.006  346.59
  47   2.3211     12.320  3.5763    11.242  354.21
  48   2.3211     12.390  3.5769    11.178  361.74
  49   2.3212     12.270  3.5771    10.972  369.13
  50   2.3210     12.280  3.5754    11.114  376.47
  51   2.3210     12.300  3.5770    11.320  383.81
  52   2.3211     12.320  3.5776    11.178  391.25
  53   2.3211     12.220  3.5766    11.102  398.62
  54   2.3213     12.280  3.5750    11.324  406.09
  55   2.3209     12.260  3.5769    11.094  413.45
  56   2.3213     12.240  3.5755    11.210  420.76
  57   2.3211     12.300  3.5772    10.998  428.41
  58   2.3210     12.280  3.5758    11.136  435.78
  59   2.3215     12.100  3.5775    11.002  443.29
  60   2.3215     12.140  3.5769    10.980  450.76
  61   2.3211     12.340  3.5766    11.118  458.25
  62   2.3208     12.290  3.5770    11.254  465.84
  63   2.3206     12.240  3.5772    11.154  473.28
  64   2.3209     12.270  3.5767    11.298  480.67
  65   2.3211     12.310  3.5759    11.120  488.07
  66   2.3213     12.140  3.5765    11.126  495.66
  67   2.3209     12.310  3.5766    11.392  503.01
  68   2.3211     12.330  3.5762    11.348  510.56
  69   2.3212     12.280  3.5763    11.208  517.92
  70   2.3212     12.430  3.5754    11.146  525.42
  71   2.3204     12.250  3.5772    11.144  532.88
  72   2.3212     12.220  3.5762    11.034  540.30
  73   2.3211     12.500  3.5774    11.040  547.66
  74   2.3211     12.280  3.5767    11.308  555.00
  75   2.3209     12.240  3.5762    11.156  562.47
  76   2.3209     12.230  3.5768    11.180  570.11
  77   2.3207     12.300  3.5768    11.372  577.61
  78   2.3210     12.300  3.5767    11.206  585.19
  79   2.3214     12.250  3.5768    11.192  592.64
  80   2.3213     12.170  3.5763    11.184  600.11
  81   2.3216     12.180  3.5763    11.150  607.76
  82   2.3213     12.210  3.5752    11.164  615.12
  83   2.3212     12.240  3.5762    11.288  622.46
  84   2.3213     12.300  3.5767    11.158  630.00
  85   2.3215     12.180  3.5769    11.376  637.32
