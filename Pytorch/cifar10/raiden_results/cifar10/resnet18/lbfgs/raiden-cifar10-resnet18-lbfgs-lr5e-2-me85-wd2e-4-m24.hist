Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '24', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 5729680896 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.3292     10.470  4.8547    10.340  10.14
   2   2.3289     10.810  4.8349    11.040  19.10
   3   2.3288     10.810  4.8339    10.864  27.89
   4   2.3286     10.720  4.8327    11.054  36.69
   5   2.3285     10.570  4.8343    10.936  45.53
   6   2.3285     10.620  4.8331    10.754  54.39
   7   2.3287     10.810  4.8340    10.962  63.33
   8   2.3287     10.550  4.8338    11.058  72.06
   9   2.3286     10.580  4.8342    11.028  80.85
  10   2.3284     10.770  4.8342    11.004  89.65
  11   2.3283     10.610  4.8335    11.132  98.48
  12   2.3283     10.820  4.8339    10.780  107.25
  13   2.3291     10.640  4.8332    11.036  116.11
  14   2.3283     10.860  4.8331    10.956  124.88
  15   2.3285     10.690  4.8347    10.898  133.73
  16   2.3289     10.710  4.8344    10.868  142.58
  17   2.3286     10.770  4.8339    10.956  151.44
  18   2.3286     10.710  4.8336    10.932  160.30
  19   2.3284     10.570  4.8332    11.010  169.08
  20   2.3288     10.730  4.8337    10.902  178.02
  21   2.3286     10.750  4.8335    11.000  186.83
  22   2.3286     10.680  4.8330    10.932  195.54
  23   2.3284     10.800  4.8335    10.922  204.29
  24   2.3288     10.760  4.8339    10.874  213.19
  25   2.3286     10.520  4.8330    10.906  221.93
  26   2.3284     10.820  4.8325    11.182  230.77
  27   2.3283     10.560  4.8340    10.832  239.47
  28   2.3286     10.690  4.8345    10.794  248.42
  29   2.3286     10.810  4.8335    11.068  257.10
  30   2.3287     10.760  4.8339    10.972  265.85
  31   2.3287     10.790  4.8342    10.802  274.70
  32   2.3290     10.890  4.8328    10.924  283.49
  33   2.3289     10.790  4.8341    11.072  292.30
  34   2.3286     10.590  4.8342    10.802  301.15
  35   2.3286     10.820  4.8331    10.964  309.90
  36   2.3283     10.760  4.8334    10.696  318.89
  37   2.3289     10.810  4.8353    10.816  327.74
  38   2.3280     10.640  4.8343    10.958  336.53
  39   2.3284     10.620  4.8340    10.860  345.38
  40   2.3288     10.690  4.8339    10.908  354.35
  41   2.3286     10.670  4.8329    10.968  363.21
  42   2.3286     10.680  4.8335    11.084  372.01
  43   2.3286     10.850  4.8338    10.946  380.78
  44   2.3287     10.500  4.8339    10.870  389.59
  45   2.3286     10.600  4.8342    10.994  398.45
  46   2.3287     10.750  4.8342    10.980  407.27
  47   2.3285     10.650  4.8343    10.758  416.12
  48   2.3288     10.730  4.8346    10.924  425.10
  49   2.3290     10.660  4.8338    10.928  433.90
  50   2.3285     10.620  4.8344    10.854  442.60
  51   2.3283     10.800  4.8346    10.870  451.40
  52   2.3284     10.810  4.8331    10.892  460.32
  53   2.3281     10.720  4.8337    10.940  469.23
  54   2.3284     10.730  4.8341    10.836  477.96
  55   2.3287     10.890  4.8342    10.836  486.71
  56   2.3288     10.680  4.8328    11.002  495.64
  57   2.3291     10.700  4.8345    10.880  504.47
  58   2.3287     10.810  4.8339    10.990  513.16
  59   2.3285     10.650  4.8336    11.032  521.89
  60   2.3289     10.680  4.8341    10.978  530.73
  61   2.3285     10.640  4.8343    10.926  539.60
  62   2.3286     10.790  4.8343    10.842  548.37
  63   2.3289     10.610  4.8340    11.032  557.17
  64   2.3284     10.650  4.8334    11.104  565.83
  65   2.3285     10.580  4.8322    11.112  574.76
  66   2.3283     10.870  4.8337    10.770  583.61
  67   2.3286     10.710  4.8331    11.032  592.49
  68   2.3284     10.910  4.8340    10.852  601.32
  69   2.3283     10.710  4.8332    10.852  610.19
  70   2.3289     10.750  4.8335    10.912  618.99
  71   2.3286     10.680  4.8332    10.908  627.88
  72   2.3285     10.630  4.8334    10.774  636.63
  73   2.3285     10.740  4.8327    10.976  645.56
  74   2.3287     10.790  4.8340    11.004  654.38
  75   2.3286     10.660  4.8328    11.148  663.14
  76   2.3287     10.710  4.8343    10.868  671.98
  77   2.3288     10.590  4.8337    10.988  680.99
  78   2.3285     10.640  4.8334    10.958  689.90
  79   2.3284     10.800  4.8346    10.954  698.67
  80   2.3284     10.620  4.8334    10.890  707.52
  81   2.3285     10.870  4.8341    10.896  716.39
  82   2.3288     10.600  4.8336    10.840  725.16
  83   2.3284     10.740  4.8346    10.806  733.89
  84   2.3286     10.720  4.8342    10.944  742.57
  85   2.3287     10.610  4.8339    10.950  751.39
