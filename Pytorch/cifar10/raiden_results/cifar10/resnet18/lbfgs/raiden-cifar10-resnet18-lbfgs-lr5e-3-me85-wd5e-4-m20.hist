Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4238856704 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.4294     10.030  8.6954    10.084  8.78
   2   2.4298     10.070  8.6955    10.080  16.30
   3   2.4287     10.090  8.6957    10.056  23.63
   4   2.4275     10.050  8.6953    10.046  30.90
   5   2.4296     10.090  8.6959    10.046  38.30
   6   2.4294     10.100  8.6947    10.100  45.63
   7   2.4300     10.100  8.6948    10.128  53.18
   8   2.4293     10.060  8.6943    10.084  60.45
   9   2.4309     10.080  8.6945    10.060  67.68
  10   2.4299     10.040  8.6956    10.120  75.08
  11   2.4308     10.070  8.6951    10.072  82.44
  12   2.4293     10.050  8.6949    10.116  89.77
  13   2.4292     10.020  8.6948    10.128  97.10
  14   2.4298     10.030  8.6949    10.152  104.30
  15   2.4299     10.060  8.6960    10.056  111.63
  16   2.4287     10.100  8.6960    10.092  119.25
  17   2.4289     10.020  8.6959    10.094  126.69
  18   2.4303     10.080  8.6949    10.102  134.06
  19   2.4311     10.040  8.6963    10.068  141.45
  20   2.4302     10.040  8.6947    10.124  148.79
  21   2.4300     10.030  8.6957    10.080  156.28
  22   2.4288     10.010  8.6956    10.048  163.51
  23   2.4278     10.040  8.6953    10.096  171.07
  24   2.4290     10.090  8.6952    10.060  178.37
  25   2.4298     10.110  8.6950    10.050  185.59
  26   2.4289     10.080  8.6943    10.088  193.14
  27   2.4305     10.080  8.6942    10.086  200.54
  28   2.4287     10.090  8.6953    10.092  207.76
  29   2.4297     10.060  8.6952    10.016  215.22
  30   2.4313     10.110  8.6960    10.084  222.62
  31   2.4302     10.100  8.6952    10.074  229.95
  32   2.4294     10.100  8.6938    10.168  237.22
  33   2.4289     10.050  8.6942    10.124  244.56
  34   2.4287     10.020  8.6954    10.098  251.90
  35   2.4293     10.050  8.6955    10.056  259.34
  36   2.4285     10.080  8.6959    10.034  266.67
  37   2.4274     10.080  8.6951    10.078  273.92
  38   2.4297     10.080  8.6953    10.136  281.38
  39   2.4281     10.050  8.6950    10.106  288.61
  40   2.4274     10.020  8.6961    10.072  296.16
  41   2.4288     10.040  8.6952    10.054  303.51
  42   2.4274     10.020  8.6957    10.078  310.77
  43   2.4284     10.030  8.6957    10.070  318.00
  44   2.4281     10.050  8.6942    10.110  325.52
  45   2.4284     10.030  8.6951    10.138  332.86
  46   2.4292     10.110  8.6952    10.126  340.16
  47   2.4285     10.080  8.6950    10.152  347.56
  48   2.4281     10.080  8.6949    10.096  354.86
  49   2.4294     10.040  8.6953    10.160  362.43
  50   2.4291     10.050  8.6961    10.054  370.02
  51   2.4294     10.010  8.6957    10.056  377.40
  52   2.4302     10.070  8.6957    10.052  384.67
  53   2.4292     10.070  8.6954    10.122  392.03
  54   2.4296     10.020  8.6949    10.060  399.42
  55   2.4284     10.070  8.6951    10.084  406.83
  56   2.4303     10.020  8.6947    10.118  414.26
  57   2.4294     10.040  8.6959    10.104  421.49
  58   2.4302     10.040  8.6958    10.146  428.80
  59   2.4290     10.100  8.6963    10.136  436.18
  60   2.4289     10.080  8.6953    10.132  443.49
  61   2.4298     10.070  8.6956    10.076  450.81
  62   2.4275     10.050  8.6949    10.108  458.21
  63   2.4300     10.090  8.6949    10.106  465.52
  64   2.4312     10.100  8.6941    10.110  472.91
  65   2.4290     10.110  8.6951     9.992  480.34
  66   2.4293     10.050  8.6958    10.104  487.68
  67   2.4280     10.090  8.6949    10.084  494.95
  68   2.4294     10.090  8.6945    10.066  502.41
  69   2.4290     10.110  8.6954    10.068  509.75
  70   2.4307     10.080  8.6948    10.096  517.07
  71   2.4308     10.070  8.6952    10.092  524.53
  72   2.4298     10.080  8.6947    10.158  531.79
  73   2.4286     10.060  8.6959    10.090  539.22
  74   2.4301     10.080  8.6946    10.104  546.65
  75   2.4301     10.040  8.6959    10.090  553.99
  76   2.4306     10.090  8.6956    10.064  561.27
  77   2.4294     10.040  8.6937    10.064  568.71
  78   2.4300     10.090  8.6948    10.094  576.02
  79   2.4291     10.110  8.6959    10.086  583.66
  80   2.4303     10.080  8.6957    10.172  590.93
  81   2.4295     10.000  8.6958    10.146  598.33
  82   2.4305     10.070  8.6945    10.094  605.78
  83   2.4297     10.070  8.6947    10.082  613.19
  84   2.4291     10.100  8.6946    10.142  620.52
  85   2.4289     10.100  8.6954    10.082  627.83
