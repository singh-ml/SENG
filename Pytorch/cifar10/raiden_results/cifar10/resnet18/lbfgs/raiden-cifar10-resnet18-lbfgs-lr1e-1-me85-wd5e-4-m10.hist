Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4477399552 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.0887     23.940  8.5214    15.722  9.39
   2   2.1162     26.760  18.0325    23.224  17.14
   3   1.8086     33.520  8.0751    30.848  24.88
   4   1.7509     35.810  7.9299    33.616  32.72
   5   1.7052     36.430  7.8844    35.336  40.60
   6   3317814.3438     10.000  867296120.8029    21.948  48.39
   7   43577.7177      9.980  2199165.7749    10.010  56.13
   8   585099.2250     10.710  45227863.5077    10.252  63.93
   9   679316.5299     10.690  1702606.0893    10.480  71.70
  10   667893.5932     10.730  1716118.7194    10.546  79.35
  11   536679.0738     10.680  1683935.3890    10.522  86.77
  12   523341.0082     10.680  1687095.2462    10.546  94.56
  13   675232.3404     10.680  1701322.8495    10.492  102.18
  14   815209.6517     10.750  1690972.4847    10.618  109.74
  15   551388.1151     10.750  1617770.7462    10.500  117.25
  16   786742.6635     10.750  1680623.5989    10.472  124.82
  17   845688.9057     10.720  1669292.7136    10.574  132.25
  18   709164.2739     10.720  1695400.5166    10.524  139.92
  19   547540.4533     10.680  1678180.1843    10.590  147.50
  20   598006.4254     10.730  1669890.9222    10.486  155.02
  21   618015.7427     10.680  1678078.2117    10.598  162.43
  22   854181.0980     10.790  1677116.7430    10.508  170.03
  23   779548.3782     10.780  1687694.0804    10.534  177.44
  24   673014.1420     10.720  1714413.3814    10.580  184.84
  25   600387.6386     10.670  1684220.6837    10.544  192.36
  26   739072.5381     10.720  1683715.7972    10.562  199.92
  27   719750.5548     10.700  1642915.1454    10.436  207.51
  28   799519.6989     10.710  1675290.6320    10.490  214.99
  29   929705.8955     10.790  1704993.8616    10.434  222.39
  30   582064.0748     10.650  1694169.4024    10.532  229.97
  31   887425.9378     10.780  1661455.4579    10.512  237.58
  32   915238.6084     10.730  1691209.3814    10.488  245.11
  33   478713.7301     10.680  1669481.6499    10.556  252.59
  34   573935.9320     10.640  1698598.7321    10.562  260.05
  35   602926.2119     10.700  1678119.6295    10.540  267.67
  36   936028.3448     10.770  1678722.6754    10.596  275.25
  37   698290.8338     10.740  1677206.0128    10.606  282.81
  38   653556.1490     10.690  1683713.0013    10.534  290.36
  39   627466.8204     10.690  1678472.2258    10.538  297.86
  40   812404.7932     10.780  1692831.8463    10.586  305.28
  41   633430.7708     10.680  1669259.3304    10.560  312.86
  42   377631.4681     10.610  1663605.2168    10.542  320.33
  43   833036.1770     10.750  1698343.4796    10.552  327.78
  44   636799.0168     10.690  1706207.3144    10.486  335.27
  45   638261.5950     10.690  1672717.1926    10.606  342.79
  46   767531.9816     10.720  1684193.2506    10.636  350.33
  47   587045.3811     10.690  1674416.8182    10.616  357.78
  48   887642.7571     10.690  1675093.2073    10.580  365.26
  49   821918.0587     10.750  1682498.4439    10.518  372.81
  50   847645.4998     10.720  1672462.0644    10.512  380.33
  51   672870.4883     10.710  1674703.2876    10.498  387.78
  52   553930.1207     10.710  1708198.3967    10.528  395.31
  53   814367.1516     10.760  1711080.6097    10.612  402.84
  54   802676.7343     10.780  1708494.7538    10.530  410.35
  55   555220.1893     10.640  1660594.2258    10.588  418.12
  56   624890.5771     10.730  1684582.0249    10.530  425.61
  57   693138.4490     10.700  1659165.7736    10.446  433.11
  58   752501.7573     10.740  1658956.2672    10.616  440.70
  59   997117.5025     10.720  1711057.2966    10.478  448.13
  60   496618.8089     10.640  1681396.6224    10.550  455.70
  61   726390.5406     10.750  1689390.7245    10.516  463.21
  62   718143.5234     10.700  1650249.2258    10.588  470.81
  63   872079.0810     10.770  1664879.4777    10.586  478.48
  64   727677.2137     10.680  1687802.3144    10.594  486.06
  65   690378.8621     10.710  1674683.3909    10.520  493.82
  66   642073.9553     10.680  1691281.3520    10.608  501.36
  67   901318.5019     10.740  1689032.8788    10.562  508.95
  68   495047.1341     10.670  1709066.2200    10.548  516.44
  69   575880.6527     10.670  1708260.0918    10.496  523.93
  70   764858.8097     10.770  1674622.6186    10.564  531.51
  71   734969.2636     10.750  1680933.8756    10.452  538.91
  72   899274.0627     10.730  1664516.8361    10.492  546.51
  73   925754.4237     10.810  1669994.5102    10.546  554.04
  74   1035475.7336     10.760  1684633.4917    10.514  561.51
  75   667400.6124     10.710  1668074.3195    10.442  568.93
  76   835220.3211     10.750  1675016.4031    10.502  576.40
  77   774611.9799     10.700  1696660.3055    10.556  584.03
  78   587406.7619     10.730  1652595.8546    10.590  591.55
  79   734530.5202     10.750  1657236.4082    10.542  599.11
  80   847231.2855     10.720  1685392.9024    10.546  606.56
  81   475982.4726     10.720  1725873.1390    10.594  614.29
  82   640572.0563     10.700  1668858.2066    10.530  621.79
  83   625713.3905     10.710  1671025.8042    10.534  629.40
  84   897324.1382     10.730  1699510.6607    10.604  636.81
  85   839305.8995     10.740  1682966.2073    10.472  644.37
