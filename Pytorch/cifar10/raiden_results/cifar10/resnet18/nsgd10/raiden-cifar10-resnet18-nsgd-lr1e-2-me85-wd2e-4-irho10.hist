Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '10', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6340878848 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5985     41.350  2.0218    28.524  274.11
   2   1.2235     54.720  1.3396    50.918  546.53
   3   1.0837     61.920  1.0462    62.280  819.59
   4   1.1095     64.210  0.8434    70.276  1092.46
   5   0.7724     73.750  0.6978    75.612  1365.01
   6   0.6980     75.650  0.5974    79.148  1637.80
   7   0.7326     75.620  0.5230    81.948  1910.61
   8   0.6160     80.250  0.4748    83.664  2183.73
   9   0.5313     81.760  0.4357    84.898  2457.20
  10   0.5379     82.600  0.3901    86.460  2730.81
  11   0.5056     83.570  0.3571    87.644  3003.44
  12   0.5465     82.830  0.3296    88.450  3276.58
  13   0.4257     86.740  0.3075    89.330  3549.47
  14   0.4556     85.560  0.2822    90.140  3822.45
  15   0.4022     86.750  0.2632    90.880  4096.16
  16   0.5364     83.290  0.2465    91.466  4368.86
  17   0.4989     84.370  0.2274    92.078  4642.39
  18   0.3656     88.860  0.2149    92.482  4915.44
  19   0.4065     87.860  0.1971    93.064  5188.12
  20   0.4549     87.660  0.1901    93.374  5461.42
  21   0.3636     88.800  0.1688    94.042  5734.42
  22   0.4477     88.200  0.1581    94.400  6007.33
  23   0.4468     87.490  0.1535    94.568  6280.39
  24   0.4166     88.300  0.1450    94.870  6553.28
  25   0.3926     89.990  0.1334    95.360  6826.35
  26   0.4092     89.300  0.1182    95.710  7099.52
  27   0.4448     88.200  0.1175    95.830  7372.19
  28   0.3952     90.540  0.1051    96.364  7645.25
  29   0.3999     90.120  0.0984    96.524  7918.25
  30   0.4708     89.220  0.0873    96.840  8191.82
  31   0.4245     89.860  0.0857    96.940  8464.90
  32   0.4182     90.540  0.0778    97.198  8737.27
  33   0.4220     90.110  0.0707    97.484  9010.10
  34   0.4695     89.630  0.0658    97.694  9283.30
  35   0.4077     90.700  0.0625    97.814  9556.32
  36   0.4244     90.820  0.0516    98.236  9828.98
  37   0.4101     91.020  0.0586    97.930  10101.90
  38   0.5229     89.390  0.0480    98.336  10375.03
  39   0.4138     91.480  0.0413    98.522  10648.56
  40   0.4461     91.100  0.0404    98.522  10920.78
  41   0.4336     91.160  0.0356    98.812  11194.16
  42   0.4205     91.280  0.0326    98.890  11466.96
