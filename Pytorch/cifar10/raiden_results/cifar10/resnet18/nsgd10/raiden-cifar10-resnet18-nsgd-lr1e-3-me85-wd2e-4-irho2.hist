Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6339961856 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.6539     37.760  1.8959    29.154  275.72
   2   1.4339     46.780  1.5470    42.558  548.64
   3   1.3317     51.310  1.3948    48.740  821.53
   4   1.2304     55.590  1.2764    53.440  1094.31
   5   1.1013     60.280  1.1761    57.384  1367.23
   6   1.0541     61.580  1.0871    60.810  1641.03
   7   1.0039     64.210  1.0182    63.394  1914.40
   8   0.9356     66.920  0.9553    65.610  2187.51
   9   0.9183     67.660  0.9086    67.488  2460.25
  10   0.9216     67.560  0.8639    69.090  2733.94
  11   0.8640     69.480  0.8285    70.426  3007.26
  12   0.8964     68.440  0.7856    72.116  3280.48
  13   0.7807     72.410  0.7498    73.182  3553.69
  14   0.7564     73.010  0.7168    74.502  3827.12
  15   0.6915     75.730  0.6895    75.452  4100.04
  16   0.7376     74.230  0.6623    76.584  4373.14
  17   0.7229     74.720  0.6359    77.636  4647.49
  18   0.7305     75.600  0.6160    78.240  4920.89
  19   0.6516     77.110  0.5898    79.072  5194.04
  20   0.7038     76.100  0.5636    80.192  5467.51
  21   0.6201     78.500  0.5532    80.620  5740.92
  22   0.6227     78.730  0.5310    81.352  6014.06
  23   0.6110     78.810  0.5173    81.818  6286.77
  24   0.6117     79.250  0.5034    82.460  6560.28
  25   0.5924     79.760  0.4884    82.774  6833.56
  26   0.5940     79.750  0.4755    83.198  7106.77
  27   0.6427     78.590  0.4585    83.768  7380.31
  28   0.5720     80.850  0.4510    84.226  7653.46
  29   0.5728     80.890  0.4336    84.698  7926.44
  30   0.5388     82.160  0.4237    85.116  8199.29
  31   0.5623     81.340  0.4107    85.608  8472.93
  32   0.5383     82.100  0.4014    85.868  8746.22
  33   0.5135     82.500  0.3890    86.288  9018.96
  34   0.5234     82.780  0.3793    86.684  9291.68
  35   0.5207     82.990  0.3698    86.952  9564.59
  36   0.5496     82.640  0.3605    87.378  9837.50
  37   0.5317     82.890  0.3528    87.552  10110.45
  38   0.5457     82.630  0.3418    88.056  10383.45
  39   0.4945     84.140  0.3305    88.438  10656.05
  40   0.5223     83.560  0.3256    88.638  10928.79
  41   0.5141     83.110  0.3164    88.804  11202.38
  42   0.5040     83.890  0.3060    89.238  11475.87
  43   0.4911     83.990  0.3049    89.256  11748.57
