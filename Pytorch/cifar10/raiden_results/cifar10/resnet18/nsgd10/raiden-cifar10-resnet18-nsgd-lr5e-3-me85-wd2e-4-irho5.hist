Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '5', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6339961856 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.2647     54.270  1.5994    40.846  276.08
   2   1.0083     65.480  1.0834    61.326  550.80
   3   0.8460     70.940  0.8389    70.420  825.61
   4   0.7772     74.310  0.7054    75.482  1099.98
   5   0.8273     74.110  0.6041    79.116  1374.28
   6   0.7149     76.750  0.5402    81.086  1648.77
   7   0.5420     81.770  0.4922    82.780  1922.93
   8   0.5508     81.950  0.4424    84.678  2197.15
   9   0.6722     79.470  0.4014    85.860  2471.48
  10   0.5304     82.900  0.3722    87.148  2745.94
  11   0.4764     84.130  0.3381    88.166  3020.65
  12   0.5255     83.590  0.3224    88.762  3295.02
  13   0.5116     84.690  0.3038    89.294  3569.81
  14   0.4979     84.640  0.2814    90.226  3844.24
  15   0.5727     83.340  0.2576    90.902  4118.82
  16   0.4309     86.460  0.2447    91.514  4393.13
  17   0.4548     86.250  0.2308    91.908  4668.87
  18   0.3848     88.010  0.2169    92.346  4944.28
  19   0.4359     86.680  0.1987    93.038  5219.48
  20   0.5329     85.310  0.1910    93.226  5494.85
  21   0.4428     87.380  0.1795    93.634  5769.38
  22   0.4323     87.740  0.1694    94.062  6044.16
  23   0.4971     86.410  0.1548    94.500  6318.94
  24   0.4242     88.010  0.1459    94.850  6593.67
  25   0.4657     87.470  0.1367    95.222  6868.28
  26   0.4445     88.560  0.1302    95.416  7143.14
  27   0.4815     87.730  0.1223    95.544  7417.76
  28   0.4517     88.420  0.1136    95.970  7692.54
  29   0.4459     88.650  0.0985    96.504  7967.08
  30   0.4815     87.960  0.0990    96.420  8241.96
  31   0.4526     88.490  0.0898    96.770  8516.83
  32   0.4288     89.080  0.0859    96.994  8791.46
  33   0.5477     87.220  0.0798    97.170  9066.20
  34   0.4554     89.300  0.0758    97.266  9340.79
  35   0.4480     89.350  0.0708    97.478  9615.89
  36   0.4537     89.700  0.0651    97.678  9890.48
  37   0.4451     89.920  0.0610    97.786  10164.98
  38   0.4324     90.250  0.0555    98.004  10440.10
  39   0.4619     90.060  0.0514    98.234  10714.86
  40   0.4713     89.870  0.0469    98.360  10989.36
  41   0.4571     90.200  0.0450    98.442  11264.28
  42   0.4676     89.750  0.0416    98.518  11539.23
  43   0.4412     90.580  0.0398    98.628  11814.05
