Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6342845440 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5563     46.850  1.6454    39.134  276.65
   2   1.1293     60.080  1.1647    57.886  551.56
   3   0.9610     66.570  0.9240    67.316  826.75
   4   0.7820     73.340  0.7650    73.194  1101.98
   5   0.6928     76.850  0.6472    77.452  1377.13
   6   0.6711     77.530  0.5701    80.286  1652.09
   7   0.6378     79.110  0.5173    82.072  1927.09
   8   0.6258     79.470  0.4706    83.508  2202.16
   9   0.5731     81.380  0.4357    84.752  2477.33
  10   0.4921     84.190  0.3997    86.040  2752.23
  11   0.4826     84.200  0.3647    87.276  3026.98
  12   0.5861     81.570  0.3381    88.294  3302.24
  13   0.4526     85.270  0.3191    88.822  3577.11
  14   0.5051     84.800  0.3028    89.416  3852.32
  15   0.5509     82.910  0.2801    90.246  4128.40
  16   0.4011     86.820  0.2602    90.760  4403.83
  17   0.4544     86.220  0.2456    91.278  4679.10
  18   0.4668     85.580  0.2323    91.872  4954.08
  19   0.4716     85.880  0.2195    92.176  5229.49
  20   0.4215     87.050  0.2042    92.986  5504.24
  21   0.4307     87.370  0.1913    93.174  5779.41
  22   0.4342     86.640  0.1839    93.582  6054.61
  23   0.4378     87.700  0.1640    94.256  6329.70
  24   0.5296     86.190  0.1620    94.170  6604.66
  25   0.5693     85.600  0.1454    94.816  6880.06
  26   0.4431     87.430  0.1433    94.914  7155.20
  27   0.4177     88.210  0.1307    95.356  7430.28
  28   0.4460     88.150  0.1259    95.484  7705.31
  29   0.4235     89.010  0.1165    95.872  7981.48
  30   0.4506     88.060  0.1101    96.090  8256.28
  31   0.4599     88.060  0.1026    96.392  8531.71
  32   0.4960     87.720  0.0933    96.696  8806.86
  33   0.4310     89.300  0.0870    96.874  9081.79
  34   0.4921     88.430  0.0789    97.216  9357.15
  35   0.5098     88.150  0.0760    97.306  9632.12
  36   0.4387     88.920  0.0681    97.644  9907.23
  37   0.4455     89.330  0.0683    97.556  10182.77
  38   0.4322     89.580  0.0624    97.862  10458.20
  39   0.4200     90.220  0.0610    97.886  10733.35
  40   0.4008     90.540  0.0510    98.248  11008.65
  41   0.4453     89.690  0.0477    98.322  11284.12
