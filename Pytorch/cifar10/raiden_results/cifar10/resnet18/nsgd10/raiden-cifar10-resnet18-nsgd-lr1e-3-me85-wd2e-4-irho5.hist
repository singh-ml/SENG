Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '5', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6339961856 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5161     44.150  1.7576    33.978  288.59
   2   1.2482     55.130  1.3640    50.048  560.08
   3   1.1411     58.720  1.1645    57.908  831.68
   4   0.9866     64.810  1.0277    63.364  1103.23
   5   0.9925     66.080  0.9258    67.138  1374.84
   6   0.8216     71.740  0.8350    70.508  1646.49
   7   0.8331     71.530  0.7585    73.166  1917.81
   8   0.7331     74.500  0.6985    75.364  2189.35
   9   0.7605     75.080  0.6437    77.400  2460.57
  10   0.7223     76.180  0.6145    78.496  2731.95
  11   0.6558     78.780  0.5699    79.944  3003.75
  12   0.6989     76.840  0.5417    81.230  3275.41
  13   0.6500     78.610  0.5072    82.288  3546.80
  14   0.5591     81.390  0.4847    83.044  3818.62
  15   0.5626     81.910  0.4602    84.042  4090.16
  16   0.5777     81.220  0.4436    84.542  4361.79
  17   0.5573     82.060  0.4165    85.374  4635.50
  18   0.5689     81.280  0.4010    85.992  4907.46
  19   0.5766     81.490  0.3873    86.484  5179.54
  20   0.5428     83.140  0.3688    87.206  5451.64
  21   0.4757     84.300  0.3476    87.910  5723.16
  22   0.5499     82.870  0.3382    88.186  5995.13
  23   0.5070     84.030  0.3257    88.678  6266.78
  24   0.5157     83.630  0.3106    89.102  6538.22
  25   0.5170     83.720  0.2931    89.798  6809.93
  26   0.5017     84.560  0.2919    89.760  7081.90
  27   0.4922     84.730  0.2749    90.352  7353.54
  28   0.4884     85.560  0.2623    90.824  7625.33
  29   0.5152     84.360  0.2587    90.972  7897.16
  30   0.5646     83.430  0.2422    91.496  8169.13
  31   0.5220     83.940  0.2401    91.602  8440.96
  32   0.5046     85.030  0.2281    91.974  8713.09
  33   0.5184     84.960  0.2169    92.274  8984.60
  34   0.5490     84.290  0.2096    92.644  9256.51
  35   0.5029     85.550  0.2018    92.930  9528.21
  36   0.4566     86.610  0.1941    93.066  9800.12
  37   0.4696     86.590  0.1886    93.238  10071.71
  38   0.5067     86.100  0.1778    93.762  10343.27
  39   0.4821     86.250  0.1755    93.796  10615.22
  40   0.4693     86.880  0.1669    94.022  10887.11
  41   0.5045     85.970  0.1586    94.380  11158.92
  42   0.4996     86.380  0.1549    94.514  11430.31
  43   0.4808     86.630  0.1512    94.610  11702.11
  44   0.5102     86.290  0.1410    94.906  11973.91
