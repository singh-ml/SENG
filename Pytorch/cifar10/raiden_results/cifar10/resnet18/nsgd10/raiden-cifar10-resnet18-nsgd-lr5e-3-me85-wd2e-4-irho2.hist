Use GPU: 0 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6340878848 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.3638     49.790  1.6669    37.392  277.57
   2   1.2195     57.520  1.2573    54.408  552.58
   3   1.1507     59.920  1.0228    63.466  828.11
   4   0.8793     69.670  0.8855    68.524  1103.95
   5   0.7855     72.750  0.7607    72.970  1380.09
   6   0.7453     74.610  0.6777    76.008  1655.46
   7   0.7668     74.320  0.6133    78.462  1930.64
   8   0.6390     78.220  0.5676    80.074  2206.20
   9   0.6076     79.770  0.5185    81.900  2482.11
  10   0.5924     80.430  0.4813    83.200  2758.07
  11   0.5797     80.840  0.4496    84.336  3033.45
  12   0.5914     80.470  0.4160    85.528  3308.98
  13   0.5533     82.200  0.3948    86.316  3584.88
  14   0.4820     83.780  0.3680    87.164  3860.66
  15   0.5460     82.380  0.3510    87.648  4136.94
  16   0.4980     84.100  0.3348    88.266  4412.18
  17   0.5156     83.810  0.3103    89.216  4687.79
  18   0.4886     84.590  0.2956    89.806  4963.97
  19   0.5497     83.220  0.2788    90.164  5239.61
  20   0.4564     85.660  0.2668    90.660  5515.23
  21   0.5526     83.130  0.2535    91.166  5790.81
  22   0.5637     83.370  0.2410    91.534  6066.68
  23   0.4563     85.320  0.2285    91.922  6342.88
  24   0.5000     85.300  0.2182    92.372  6618.62
  25   0.5114     84.840  0.2040    92.844  6894.18
  26   0.4396     86.890  0.1904    93.368  7170.02
  27   0.4516     86.880  0.1851    93.428  7444.99
  28   0.4535     87.140  0.1721    93.806  7721.16
  29   0.4486     87.240  0.1668    93.962  7997.33
  30   0.4425     87.360  0.1522    94.620  8273.23
  31   0.4647     86.880  0.1517    94.592  8548.74
  32   0.4414     87.490  0.1361    95.140  8823.88
  33   0.4853     87.130  0.1345    95.170  9099.39
  34   0.4345     87.920  0.1252    95.638  9374.45
  35   0.4507     88.250  0.1197    95.698  9650.18
  36   0.4510     88.150  0.1085    96.190  9926.19
  37   0.4642     87.630  0.1031    96.328  10201.86
  38   0.4671     88.080  0.0986    96.528  10477.56
  39   0.4568     88.290  0.0937    96.724  10752.93
  40   0.4458     88.360  0.0854    96.954  11027.97
  41   0.4413     88.500  0.0800    97.212  11304.36
