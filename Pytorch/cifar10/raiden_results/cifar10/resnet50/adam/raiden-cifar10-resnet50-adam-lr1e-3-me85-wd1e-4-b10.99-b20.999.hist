Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17946863104 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.4746     46.690  1.7528    36.316  25.84
   2   1.2500     55.840  1.3646    50.130  49.31
   3   1.0638     62.600  1.1361    58.960  72.81
   4   0.9273     68.020  0.9596    65.706  96.28
   5   0.8026     72.170  0.8254    70.846  119.75
   6   0.7298     74.510  0.7270    74.336  143.22
   7   0.6174     78.510  0.6511    77.408  166.69
   8   0.5894     79.590  0.5716    80.136  190.18
   9   0.6054     80.150  0.5350    81.372  213.65
  10   0.5681     81.420  0.4843    83.248  237.12
  11   0.5144     83.200  0.4420    84.844  260.59
  12   0.4951     83.090  0.4346    85.050  284.10
  13   0.4565     84.780  0.4179    85.548  307.58
  14   0.4669     84.460  0.3854    86.802  331.07
  15   0.4667     84.970  0.3612    87.448  354.57
  16   0.4758     84.850  0.3531    87.800  378.05
  17   0.4327     85.290  0.3476    88.182  401.52
  18   0.3973     86.960  0.3239    88.858  425.02
  19   0.4376     86.100  0.3157    89.058  448.49
  20   0.3910     87.170  0.2988    89.798  471.97
  21   0.4846     84.560  0.2839    90.374  495.46
  22   0.4399     86.160  0.2871    90.156  518.95
  23   0.4053     86.860  0.2726    90.660  542.43
  24   0.3973     86.930  0.2615    90.940  565.93
  25   0.3784     87.450  0.2534    91.112  589.42
  26   0.4095     87.220  0.2510    91.298  612.92
  27   0.4051     87.340  0.2343    91.918  636.42
  28   0.4163     86.770  0.2324    92.148  659.89
  29   0.4187     86.940  0.2288    92.114  683.41
  30   0.3778     88.060  0.2231    92.260  706.90
  31   0.3754     88.360  0.2091    92.800  730.37
  32   0.3676     88.510  0.2110    92.802  753.83
  33   0.3691     88.840  0.2006    93.030  777.34
  34   0.3339     89.810  0.1936    93.264  800.81
  35   0.3759     88.970  0.1938    93.206  824.28
  36   0.3374     89.380  0.1892    93.350  847.76
  37   0.3678     88.490  0.1808    93.728  871.23
  38   0.3561     89.090  0.1803    93.790  894.72
  39   0.3446     89.920  0.1725    93.904  918.23
  40   0.3546     89.630  0.1663    94.248  941.71
  41   0.3447     89.080  0.1744    93.758  965.20
  42   0.3230     89.900  0.1677    94.138  988.72
  43   0.3390     89.840  0.1617    94.406  1012.20
  44   0.3293     90.100  0.1524    94.752  1035.70
  45   0.3854     88.300  0.1534    94.690  1059.19
  46   0.3494     89.190  0.1516    94.768  1082.69
  47   0.3804     89.280  0.1498    94.776  1106.20
  48   0.3271     90.160  0.1513    94.796  1129.70
  49   0.3455     90.030  0.1433    95.024  1153.17
  50   0.3828     89.000  0.1395    95.128  1176.66
  51   0.3785     89.350  0.1328    95.438  1200.16
  52   0.3484     89.800  0.1343    95.408  1223.65
  53   0.3437     90.000  0.1332    95.336  1247.15
  54   0.3394     90.110  0.1346    95.352  1270.63
  55   0.3518     89.780  0.1273    95.532  1294.11
  56   0.3205     90.430  0.1251    95.628  1317.63
  57   0.3981     88.830  0.1268    95.520  1341.11
  58   0.3497     90.110  0.1226    95.566  1364.57
  59   0.4007     89.300  0.1290    95.458  1388.08
