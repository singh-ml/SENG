Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'resnet50', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '1e-4', '--beta1', '0.9', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17946863104 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5555     44.230  1.7884    33.536  25.90
   2   1.2915     52.990  1.4255    47.710  49.30
   3   1.0949     60.590  1.2249    55.578  72.73
   4   1.0133     64.890  1.0544    62.136  96.12
   5   0.8906     69.110  0.9143    67.362  119.51
   6   0.8095     72.660  0.7930    72.056  142.93
   7   0.7976     73.230  0.6943    75.662  166.37
   8   0.6125     78.650  0.6151    78.508  189.77
   9   0.6160     79.430  0.5523    80.718  213.19
  10   0.6249     79.530  0.4968    82.600  236.63
  11   0.5138     82.910  0.4517    84.144  260.04
  12   0.5691     81.470  0.4110    85.644  283.49
  13   0.5325     82.720  0.3799    86.644  306.87
  14   0.4947     83.880  0.3481    87.726  330.29
  15   0.5562     82.570  0.3265    88.606  353.69
  16   0.4858     84.040  0.3009    89.218  377.13
  17   0.4826     85.070  0.2781    90.276  400.52
  18   0.4573     85.370  0.2671    90.528  423.95
  19   0.5194     84.730  0.2465    91.446  447.35
  20   0.4426     86.740  0.2326    91.830  470.75
  21   0.4770     85.930  0.2193    92.256  494.13
  22   0.4592     85.930  0.2027    92.902  517.53
  23   0.4260     87.330  0.2007    92.770  540.94
  24   0.4377     86.950  0.1868    93.522  564.36
  25   0.4380     87.170  0.1763    93.670  587.74
  26   0.5195     85.960  0.1640    94.162  611.16
  27   0.4560     87.220  0.1609    94.386  634.58
  28   0.4282     88.320  0.1507    94.598  657.99
  29   0.5293     85.680  0.1450    94.814  681.43
  30   0.4544     87.480  0.1421    94.952  704.86
  31   0.5482     86.030  0.1282    95.552  728.26
  32   0.4318     88.100  0.1289    95.502  751.61
  33   0.4483     88.210  0.1209    95.718  775.02
  34   0.4882     87.750  0.1209    95.726  798.39
  35   0.4665     88.310  0.1149    95.866  821.79
  36   0.4507     88.530  0.1134    95.926  845.17
  37   0.4780     88.250  0.1082    96.270  868.58
  38   0.5103     87.070  0.1079    96.162  892.01
  39   0.4474     88.740  0.1018    96.334  915.43
  40   0.4553     88.180  0.0966    96.646  938.86
  41   0.4726     88.520  0.0986    96.598  962.24
  42   0.4795     88.960  0.0895    96.946  985.69
  43   0.4626     88.240  0.0916    96.856  1009.09
  44   0.5143     87.300  0.0926    96.740  1032.57
  45   0.5045     88.620  0.0842    97.034  1055.99
  46   0.4576     88.770  0.0826    97.136  1079.38
  47   0.4713     88.040  0.0824    97.236  1102.81
  48   0.4432     88.770  0.0790    97.312  1126.23
