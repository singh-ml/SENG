Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17946863104 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.0673     21.220  2.4447    16.602  25.88
   2   1.8290     29.670  1.9297    24.616  49.33
   3   1.5706     41.700  1.6980    35.508  72.81
   4   1.4310     47.310  1.5055    43.986  96.28
   5   1.2923     52.420  1.3551    50.270  119.75
   6   1.2100     57.310  1.2230    55.472  143.20
   7   1.1424     61.230  1.1163    59.396  166.66
   8   0.9753     64.950  1.0089    63.736  190.11
   9   0.9316     67.100  0.9441    65.988  213.59
  10   0.9069     68.900  0.8620    69.234  237.04
  11   0.8562     69.500  0.8311    70.264  260.49
  12   0.8364     71.420  0.7881    71.980  283.94
  13   0.7952     72.680  0.7467    73.372  307.38
  14   0.7519     73.750  0.7175    74.574  330.80
  15   0.7040     74.990  0.6872    75.704  354.29
  16   0.7538     74.000  0.6684    76.386  377.76
  17   0.6521     77.590  0.6472    77.180  401.17
  18   0.6455     77.880  0.6151    78.372  424.61
  19   0.6305     77.990  0.6027    78.798  448.05
  20   0.6435     77.420  0.5945    79.134  471.48
  21   0.6292     78.220  0.5757    79.736  494.95
  22   0.5892     79.490  0.5654    80.034  518.38
  23   0.5908     79.410  0.5426    80.856  541.81
  24   0.6440     78.520  0.5303    81.538  565.33
  25   0.5456     81.270  0.5258    81.598  588.76
  26   0.5830     80.440  0.5155    81.940  612.23
  27   0.5736     80.560  0.5007    82.444  635.69
  28   0.5644     80.780  0.4894    82.786  659.14
  29   0.5887     80.270  0.4859    82.932  682.58
  30   0.5809     80.360  0.4802    83.242  705.99
  31   0.5336     81.970  0.4694    83.574  729.45
  32   0.5573     81.130  0.4614    83.758  752.90
  33   0.5239     82.050  0.4521    84.204  776.34
  34   0.5344     81.830  0.4522    84.264  799.79
  35   0.5081     82.330  0.4426    84.640  823.21
  36   0.5076     82.480  0.4394    84.638  846.65
  37   0.5592     81.660  0.4319    84.932  870.12
  38   0.5112     82.460  0.4260    85.234  893.58
  39   0.5059     83.280  0.4232    85.246  917.03
  40   0.5187     82.730  0.4219    85.322  940.46
  41   0.4964     82.880  0.4150    85.502  963.92
  42   0.4968     82.910  0.4164    85.784  987.37
  43   0.4734     83.820  0.4088    85.800  1010.80
  44   0.4990     82.670  0.4084    85.612  1034.23
  45   0.5013     83.040  0.4052    85.790  1057.68
  46   0.5138     82.860  0.4021    86.030  1081.13
  47   0.4698     84.460  0.3876    86.486  1104.59
  48   0.4773     83.730  0.3931    86.298  1128.06
  49   0.4758     84.080  0.3880    86.504  1151.49
