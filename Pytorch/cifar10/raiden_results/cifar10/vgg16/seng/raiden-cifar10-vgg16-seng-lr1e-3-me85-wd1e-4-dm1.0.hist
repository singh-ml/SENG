Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7444392960 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2916     10.220  2.2998    10.366  12.65
   2   2.1947     19.430  2.2703    12.400  18.56
   3   1.9088     29.190  2.0836    22.370  24.37
   4   1.7181     34.640  1.8846    28.298  30.20
   5   1.6548     37.920  1.7459    33.262  35.99
   6   1.5143     42.580  1.6438    36.950  41.78
   7   1.4358     45.970  1.5488    41.340  47.58
   8   1.4475     46.450  1.4529    45.386  53.47
   9   1.2727     53.100  1.3866    48.304  59.29
  10   1.2155     55.270  1.3148    51.594  65.11
  11   1.1692     57.160  1.2600    53.642  70.91
  12   1.1079     59.880  1.1950    56.202  76.73
  13   1.0626     61.630  1.1483    58.222  82.54
  14   1.0528     61.720  1.1013    59.788  88.47
  15   0.9888     64.600  1.0671    61.120  94.31
  16   0.9808     64.640  1.0233    62.916  100.12
  17   0.9498     66.150  0.9997    63.836  105.96
  18   0.9019     67.940  0.9598    65.222  111.78
  19   0.8846     68.620  0.9293    66.538  117.67
  20   0.8754     68.790  0.9007    67.516  123.47
  21   0.8672     68.970  0.8731    68.520  129.31
  22   0.8018     71.480  0.8617    69.164  135.10
  23   0.7968     71.830  0.8294    70.248  140.90
  24   0.7703     72.850  0.7994    71.452  146.69
  25   0.7772     72.690  0.7791    72.172  152.58
  26   0.7695     73.410  0.7659    72.714  158.39
  27   0.7326     74.550  0.7454    73.424  164.20
  28   0.7524     74.050  0.7285    74.144  169.99
  29   0.6843     76.030  0.7082    74.802  175.80
  30   0.7128     75.500  0.6856    75.880  181.73
  31   0.6857     75.990  0.6791    75.840  187.52
  32   0.6882     76.030  0.6603    76.532  193.33
  33   0.6753     76.580  0.6438    77.288  199.15
  34   0.6841     76.860  0.6331    77.636  204.96
  35   0.6505     77.960  0.6154    78.368  210.78
  36   0.6323     78.420  0.6112    78.428  216.67
  37   0.6319     78.170  0.6000    78.896  222.47
  38   0.6557     78.130  0.5914    79.346  228.28
  39   0.6265     78.770  0.5808    79.650  234.11
  40   0.6058     78.940  0.5625    80.316  239.93
  41   0.6094     79.000  0.5553    80.610  245.73
  42   0.5954     79.570  0.5476    80.720  251.54
  43   0.5947     79.500  0.5375    81.104  257.45
  44   0.5889     79.760  0.5307    81.420  263.27
  45   0.5725     80.170  0.5152    81.890  269.07
  46   0.5670     80.650  0.5097    82.130  274.90
  47   0.5747     80.700  0.4996    82.560  280.71
  48   0.5668     81.120  0.4955    82.606  286.60
  49   0.5716     80.800  0.4829    82.886  292.39
  50   0.5840     80.450  0.4778    83.412  298.13
  51   0.5618     80.910  0.4699    83.558  303.94
  52   0.5627     81.160  0.4619    83.836  309.77
  53   0.5553     81.850  0.4618    83.658  315.57
  54   0.5508     81.350  0.4477    84.218  321.47
  55   0.5512     81.560  0.4441    84.486  327.26
  56   0.5574     81.560  0.4315    84.804  333.09
  57   0.5512     81.690  0.4324    84.768  338.91
  58   0.5319     82.300  0.4239    85.084  344.75
  59   0.5270     82.330  0.4195    85.354  350.58
  60   0.5329     82.610  0.4086    85.644  356.45
  61   0.5621     81.950  0.4022    85.854  362.28
  62   0.5347     82.500  0.3958    86.100  368.11
  63   0.5387     82.360  0.3929    86.314  373.91
  64   0.5219     83.240  0.3841    86.624  379.72
  65   0.5218     83.160  0.3798    86.728  385.60
  66   0.5252     83.130  0.3705    87.010  391.45
  67   0.5309     82.720  0.3705    87.140  397.25
  68   0.5314     82.750  0.3701    86.958  403.07
  69   0.5300     82.930  0.3607    87.558  408.89
  70   0.5149     83.690  0.3492    87.738  414.76
  71   0.5419     82.850  0.3489    87.870  420.57
  72   0.5161     83.470  0.3494    87.772  426.38
  73   0.5181     83.920  0.3315    88.492  432.20
  74   0.5242     83.480  0.3326    88.424  438.02
  75   0.5207     83.950  0.3276    88.590  443.82
  76   0.5449     82.930  0.3269    88.644  449.68
  77   0.5129     83.490  0.3150    88.876  455.49
  78   0.5015     83.880  0.3183    88.940  461.31
  79   0.5115     83.720  0.3076    89.232  467.12
  80   0.5492     83.430  0.3008    89.452  472.94
  81   0.5092     84.260  0.2994    89.590  478.74
  82   0.5197     84.030  0.2978    89.752  484.56
  83   0.5185     84.030  0.2923    89.748  490.48
  84   0.5282     84.170  0.2847    90.072  496.32
  85   0.5096     83.960  0.2882    89.950  502.11
