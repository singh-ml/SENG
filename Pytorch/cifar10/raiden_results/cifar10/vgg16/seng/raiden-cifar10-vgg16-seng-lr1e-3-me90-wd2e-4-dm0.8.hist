Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7444392960 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2666     12.760  2.2948    10.874  7.74
   2   2.0136     24.120  2.1263    19.652  13.52
   3   1.7772     32.730  1.9212    26.064  19.43
   4   1.5917     39.620  1.7494    32.778  25.22
   5   1.4516     45.500  1.6168    38.802  31.01
   6   1.3912     48.100  1.5003    43.550  36.84
   7   1.3021     51.220  1.4035    47.518  42.66
   8   1.2026     55.630  1.3260    50.652  48.48
   9   1.1843     56.890  1.2388    54.296  54.35
  10   1.0914     60.170  1.1758    56.884  60.15
  11   1.0366     62.450  1.1222    58.950  65.97
  12   0.9894     64.470  1.0648    61.158  71.79
  13   0.9469     65.900  1.0130    63.354  77.59
  14   0.9102     66.930  0.9789    64.656  83.38
  15   0.8882     67.930  0.9422    65.926  89.29
  16   0.8561     69.310  0.8959    67.790  95.12
  17   0.8173     70.490  0.8712    68.928  100.92
  18   0.7872     72.200  0.8353    70.144  106.73
  19   0.7609     73.070  0.8096    70.954  112.52
  20   0.7627     72.950  0.7782    72.148  118.43
  21   0.7279     74.330  0.7564    73.102  124.23
  22   0.7222     74.210  0.7318    73.920  130.04
  23   0.7238     74.500  0.7146    74.646  135.83
  24   0.6938     75.580  0.6925    75.456  141.64
  25   0.6927     75.960  0.6780    75.940  147.45
  26   0.6670     76.740  0.6591    76.434  153.32
  27   0.6454     77.610  0.6476    76.890  159.11
  28   0.6561     77.400  0.6313    77.540  164.90
  29   0.6700     77.430  0.6189    78.152  170.69
  30   0.6102     78.910  0.6000    79.014  176.48
  31   0.6003     78.930  0.5891    79.326  182.38
  32   0.5839     79.520  0.5742    79.660  188.17
  33   0.6018     79.260  0.5669    80.286  193.98
  34   0.5838     80.100  0.5521    80.632  199.77
  35   0.6269     79.350  0.5376    81.124  205.57
  36   0.5875     80.080  0.5339    81.280  211.37
  37   0.5635     80.590  0.5222    81.782  217.24
  38   0.5793     80.020  0.5153    82.084  223.06
  39   0.5468     81.620  0.5025    82.268  228.88
  40   0.5924     80.150  0.4941    82.842  234.68
  41   0.5594     81.360  0.4859    82.942  240.48
  42   0.5475     81.230  0.4719    83.644  246.37
  43   0.5426     81.450  0.4623    83.994  252.16
  44   0.5477     81.640  0.4558    84.044  257.98
  45   0.5589     82.030  0.4444    84.714  263.80
  46   0.5303     82.020  0.4396    84.802  269.60
  47   0.5544     81.500  0.4328    84.998  275.40
  48   0.5379     82.020  0.4250    85.316  281.27
  49   0.5353     82.430  0.4177    85.470  287.12
  50   0.5085     83.190  0.4116    85.790  292.88
  51   0.5187     83.030  0.4000    85.954  298.70
  52   0.5493     82.620  0.3980    86.068  304.51
  53   0.5116     83.260  0.3883    86.616  310.32
  54   0.5013     83.690  0.3794    86.830  316.17
  55   0.4964     83.500  0.3767    86.962  321.99
  56   0.4977     83.240  0.3701    87.254  327.82
  57   0.5052     83.550  0.3618    87.508  333.62
  58   0.5050     83.910  0.3606    87.474  339.43
  59   0.5064     84.310  0.3488    87.854  345.24
  60   0.4953     84.510  0.3493    87.922  351.04
  61   0.5079     84.030  0.3397    88.318  356.90
  62   0.5084     84.460  0.3319    88.410  362.72
  63   0.4972     84.130  0.3260    88.758  368.53
  64   0.4754     84.740  0.3220    88.860  374.32
  65   0.5167     83.750  0.3151    89.044  380.13
  66   0.4949     84.050  0.3140    89.134  386.02
  67   0.5150     84.100  0.3039    89.532  391.81
  68   0.4903     84.620  0.2997    89.434  397.60
  69   0.5465     83.380  0.2922    89.672  403.40
  70   0.5029     84.560  0.2932    89.754  409.20
  71   0.4924     84.070  0.2856    89.982  415.05
  72   0.4938     85.150  0.2833    90.212  420.84
  73   0.4928     84.720  0.2776    90.194  426.66
  74   0.5396     84.570  0.2784    90.344  432.47
  75   0.4889     84.680  0.2723    90.628  438.27
  76   0.5046     84.630  0.2682    90.678  444.10
  77   0.5096     84.890  0.2638    90.870  449.98
  78   0.5291     84.740  0.2546    91.090  455.80
  79   0.5005     85.080  0.2512    91.282  461.61
  80   0.4912     84.850  0.2519    91.210  467.41
  81   0.4849     85.100  0.2374    91.820  473.23
  82   0.4918     85.240  0.2338    91.884  479.02
  83   0.4974     85.330  0.2311    91.888  484.87
  84   0.4932     85.190  0.2340    91.950  490.69
  85   0.5321     84.900  0.2283    92.054  496.51
  86   0.5141     84.830  0.2247    92.162  502.34
  87   0.4850     85.520  0.2185    92.460  508.16
  88   0.5228     84.770  0.2142    92.626  513.96
  89   0.5274     85.090  0.2054    92.802  519.81
  90   0.4971     85.850  0.2065    92.764  525.61
