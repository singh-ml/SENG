Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7444392960 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.9045     26.370  2.1514    17.424  7.72
   2   1.5923     38.970  1.7749    31.532  13.60
   3   1.3716     48.780  1.4924    42.950  19.40
   4   1.1435     58.130  1.2888    52.174  25.25
   5   1.0357     63.210  1.0988    59.796  31.05
   6   0.9008     68.060  0.9657    65.046  36.84
   7   0.8117     71.010  0.8666    68.840  42.73
   8   0.7642     72.900  0.7960    72.042  48.53
   9   0.6929     76.070  0.7293    74.340  54.33
  10   0.6426     77.960  0.6788    76.204  60.13
  11   0.6472     77.490  0.6411    77.636  65.95
  12   0.6343     78.790  0.6035    79.064  71.76
  13   0.5831     80.180  0.5707    80.142  77.64
  14   0.5505     81.030  0.5374    81.244  83.47
  15   0.5392     82.080  0.5170    82.140  89.30
  16   0.5616     80.790  0.4971    82.870  95.11
  17   0.4965     82.880  0.4691    83.596  100.93
  18   0.5108     82.810  0.4472    84.644  106.72
  19   0.4891     83.570  0.4289    85.192  112.54
  20   0.5114     83.430  0.4166    85.772  118.36
  21   0.5123     83.180  0.3986    86.382  124.18
  22   0.4795     83.760  0.3807    86.886  129.98
  23   0.4748     84.460  0.3723    87.118  135.79
  24   0.4595     84.760  0.3595    87.646  141.58
  25   0.4422     85.670  0.3458    88.184  147.39
  26   0.4577     84.990  0.3221    88.858  153.25
  27   0.4583     85.010  0.3155    89.304  159.06
  28   0.4249     85.980  0.3010    89.552  164.86
  29   0.4481     85.660  0.2861    90.124  170.68
  30   0.4486     86.020  0.2878    90.038  176.50
  31   0.4266     86.040  0.2760    90.396  182.39
  32   0.4385     86.050  0.2612    91.050  188.19
  33   0.4522     86.320  0.2496    91.436  193.99
  34   0.4278     86.360  0.2417    91.640  199.82
  35   0.4475     85.730  0.2341    92.008  205.62
  36   0.4448     86.560  0.2267    92.200  211.45
  37   0.4506     86.250  0.2167    92.532  217.33
  38   0.4397     86.430  0.2044    93.080  223.14
  39   0.4562     86.420  0.2007    93.006  228.96
  40   0.4659     86.370  0.1904    93.494  234.76
  41   0.4278     86.620  0.1905    93.412  240.55
  42   0.4427     86.350  0.1783    93.788  246.45
  43   0.4397     87.250  0.1749    93.998  252.28
  44   0.4249     87.510  0.1666    94.194  258.09
  45   0.4487     87.230  0.1583    94.598  263.88
  46   0.4469     86.920  0.1517    94.720  269.71
  47   0.4691     87.490  0.1428    95.128  275.53
  48   0.4610     87.320  0.1440    95.048  281.40
  49   0.4493     87.300  0.1375    95.298  287.19
  50   0.4496     87.460  0.1365    95.332  292.92
  51   0.4863     87.180  0.1249    95.634  298.74
  52   0.4747     87.070  0.1176    95.970  304.54
  53   0.4539     88.170  0.1140    96.146  310.37
  54   0.4991     86.880  0.1076    96.316  316.28
  55   0.4787     87.820  0.1028    96.410  322.10
  56   0.4933     87.600  0.1029    96.452  327.90
  57   0.4862     87.910  0.1014    96.448  333.70
  58   0.4775     88.000  0.0920    96.844  339.50
  59   0.4881     87.930  0.0864    97.006  345.40
  60   0.4787     88.050  0.0846    97.044  351.22
  61   0.5232     87.610  0.0782    97.272  357.02
  62   0.5060     88.200  0.0738    97.488  362.82
  63   0.4872     88.190  0.0738    97.444  368.62
  64   0.5001     88.110  0.0687    97.618  374.44
  65   0.5104     88.260  0.0648    97.714  380.31
  66   0.5190     88.270  0.0629    97.858  386.13
  67   0.5402     88.270  0.0644    97.770  391.93
  68   0.5285     88.210  0.0580    98.054  397.73
  69   0.5147     88.360  0.0575    98.032  403.53
  70   0.5557     88.010  0.0527    98.208  409.31
  71   0.5293     88.640  0.0489    98.282  415.18
  72   0.5806     88.070  0.0479    98.332  420.98
  73   0.5440     88.420  0.0509    98.248  426.78
  74   0.5650     88.430  0.0448    98.424  432.58
  75   0.5911     87.930  0.0411    98.500  438.41
  76   0.5809     88.320  0.0426    98.536  444.27
  77   0.5866     88.140  0.0410    98.612  450.08
  78   0.5807     88.350  0.0411    98.526  455.93
  79   0.5716     88.520  0.0447    98.476  461.76
  80   0.5844     88.330  0.0387    98.624  467.55
  81   0.5757     88.470  0.0328    98.894  473.36
  82   0.5946     88.490  0.0341    98.838  479.24
  83   0.5774     88.240  0.0379    98.694  485.06
  84   0.5977     88.740  0.0326    98.886  490.87
  85   0.5868     88.910  0.0334    98.882  496.67
