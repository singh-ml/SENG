Use GPU: 0 for training
==> Running with ['main_seng.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 7446998016 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.0713     22.340  2.2119    15.434  7.72
   2   1.6120     38.570  1.8415    30.344  13.54
   3   1.4730     45.160  1.6128    38.510  19.37
   4   1.2436     54.120  1.4025    47.340  25.17
   5   1.0935     60.940  1.2310    54.774  30.97
   6   1.0085     63.550  1.0987    60.206  36.78
   7   0.9555     66.660  1.0000    63.836  42.65
   8   0.8553     69.450  0.9203    66.850  48.48
   9   0.8167     70.520  0.8537    69.436  54.31
  10   0.7956     72.360  0.7890    72.162  60.12
  11   0.7025     75.270  0.7427    73.834  65.94
  12   0.6573     77.330  0.7003    75.202  71.84
  13   0.6328     77.870  0.6660    76.432  77.65
  14   0.6251     78.050  0.6324    78.062  83.47
  15   0.6164     78.790  0.6042    78.850  89.30
  16   0.5941     79.480  0.5876    79.670  95.10
  17   0.5769     80.150  0.5538    80.862  100.93
  18   0.5752     80.360  0.5446    80.838  106.81
  19   0.5507     81.350  0.5121    82.346  112.62
  20   0.5287     81.830  0.4991    82.770  118.43
  21   0.5346     82.190  0.4798    83.278  124.25
  22   0.5457     81.790  0.4623    83.986  130.08
  23   0.5168     82.800  0.4377    84.858  135.90
  24   0.4996     83.410  0.4304    85.140  141.72
  25   0.5108     83.470  0.4153    85.680  147.52
  26   0.4978     83.260  0.4018    86.158  153.34
  27   0.5119     83.190  0.3905    86.514  159.15
  28   0.4839     83.960  0.3743    87.008  164.97
  29   0.4735     84.200  0.3650    87.420  170.84
  30   0.4959     84.430  0.3522    87.858  176.64
  31   0.4936     84.030  0.3440    88.052  182.46
  32   0.4866     84.480  0.3326    88.462  188.27
  33   0.4825     84.470  0.3165    89.174  194.08
  34   0.4700     84.710  0.3154    89.172  199.88
  35   0.4544     84.920  0.3044    89.564  205.76
  36   0.4675     85.070  0.2965    89.780  211.57
  37   0.4442     85.470  0.2833    90.280  217.39
  38   0.4794     84.950  0.2745    90.714  223.20
  39   0.4324     86.210  0.2654    90.866  228.99
  40   0.4463     85.990  0.2531    91.304  234.81
  41   0.4534     85.680  0.2491    91.364  240.70
  42   0.4633     85.660  0.2436    91.566  246.52
  43   0.4675     85.930  0.2351    91.798  252.33
  44   0.4876     85.310  0.2269    92.234  258.14
  45   0.4455     86.430  0.2174    92.556  263.99
  46   0.4460     86.590  0.2068    92.942  269.84
  47   0.4450     86.830  0.2010    93.106  275.64
  48   0.4598     85.950  0.1985    93.122  281.46
  49   0.5056     85.800  0.1907    93.288  287.29
  50   0.4498     86.990  0.1881    93.450  293.02
  51   0.4575     86.680  0.1782    93.728  298.84
  52   0.4634     86.260  0.1771    93.814  304.72
  53   0.4881     86.790  0.1603    94.432  310.53
  54   0.4675     87.160  0.1610    94.494  316.33
  55   0.4683     87.060  0.1552    94.596  322.14
  56   0.4647     86.690  0.1474    94.910  327.96
  57   0.4924     86.920  0.1459    95.014  333.87
  58   0.5101     86.320  0.1393    95.128  339.70
  59   0.4923     86.850  0.1358    95.284  345.51
  60   0.4941     86.570  0.1335    95.310  351.31
  61   0.5209     86.470  0.1246    95.760  357.13
  62   0.4670     87.650  0.1263    95.716  362.97
  63   0.4854     87.020  0.1185    95.864  368.83
  64   0.4963     87.410  0.1110    96.164  374.66
  65   0.4962     87.430  0.1102    96.206  380.46
  66   0.5014     87.760  0.1026    96.494  386.26
  67   0.5138     87.180  0.1004    96.570  392.07
  68   0.5396     86.960  0.0980    96.562  397.86
  69   0.5336     86.920  0.0953    96.678  403.75
  70   0.5148     87.470  0.0924    96.810  409.58
  71   0.5310     86.900  0.0915    96.860  415.38
  72   0.5532     87.370  0.0871    96.976  421.18
  73   0.5138     87.300  0.0824    97.072  426.98
  74   0.5150     87.610  0.0787    97.222  432.81
  75   0.5557     87.490  0.0789    97.198  438.70
  76   0.5412     87.420  0.0728    97.502  444.53
  77   0.6032     86.880  0.0705    97.528  450.34
  78   0.5691     87.520  0.0702    97.636  456.17
  79   0.5657     87.710  0.0670    97.622  462.02
  80   0.5579     87.660  0.0689    97.624  467.88
  81   0.5293     87.910  0.0712    97.498  473.78
  82   0.5748     87.710  0.0617    97.830  479.58
  83   0.5547     87.850  0.0623    97.906  485.41
  84   0.5810     87.820  0.0565    98.086  491.25
  85   0.5677     87.360  0.0570    98.048  497.06
  86   0.5626     87.910  0.0581    98.040  502.88
  87   0.5882     87.500  0.0546    98.146  508.71
  88   0.5982     87.780  0.0512    98.266  514.53
  89   0.6012     87.520  0.0521    98.186  520.35
  90   0.6000     87.200  0.0557    98.072  526.17
