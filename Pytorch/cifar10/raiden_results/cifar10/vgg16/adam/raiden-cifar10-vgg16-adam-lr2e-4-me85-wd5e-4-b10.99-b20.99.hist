Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.99', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4042061312 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.8878     21.280  2.0613    17.888  7.00
   2   1.7901     27.410  1.8623    23.614  12.41
   3   1.6267     35.430  1.7114    31.742  17.76
   4   1.4453     44.060  1.5585    38.710  23.10
   5   1.3549     48.050  1.4450    44.784  28.45
   6   1.2682     52.590  1.3310    49.980  33.79
   7   1.1552     57.580  1.2096    54.770  39.14
   8   1.0047     63.340  1.0988    59.922  44.58
   9   0.9263     66.410  0.9845    64.260  49.90
  10   0.8750     69.860  0.9109    67.518  55.24
  11   0.8331     70.810  0.8590    69.520  60.56
  12   0.7630     73.600  0.8129    71.628  65.88
  13   0.7944     72.510  0.7798    72.924  71.20
  14   0.7975     72.990  0.7396    74.424  76.53
  15   0.6824     76.700  0.6822    76.614  81.90
  16   0.6597     77.940  0.6310    78.538  87.21
  17   0.6446     78.670  0.6061    79.554  92.53
  18   0.6615     77.870  0.5891    80.128  97.88
  19   0.6221     79.770  0.5472    81.556  103.21
  20   0.5774     81.080  0.5195    82.556  108.52
  21   0.5916     81.050  0.5062    83.342  113.92
  22   0.5796     81.560  0.4929    83.652  119.26
  23   0.5762     81.600  0.4739    84.278  124.57
  24   0.5232     83.150  0.4437    85.212  129.89
  25   0.5213     82.990  0.4122    86.220  135.19
  26   0.5310     82.740  0.3967    86.940  140.53
  27   0.5295     83.300  0.3933    87.004  145.88
  28   0.5111     83.630  0.3661    87.878  151.31
  29   0.5267     83.430  0.3531    88.556  156.66
  30   0.5335     83.320  0.3476    88.668  161.99
  31   0.5162     84.130  0.3377    89.018  167.31
  32   0.5034     84.270  0.3253    89.394  172.66
  33   0.5069     84.290  0.3116    89.848  177.99
  34   0.4971     84.950  0.3030    90.000  183.32
  35   0.4673     85.410  0.2939    90.418  188.73
  36   0.5106     85.510  0.2811    90.762  194.05
  37   0.5106     85.140  0.2595    91.550  199.37
  38   0.5395     84.630  0.2812    90.852  204.70
  39   0.4975     85.270  0.2667    91.230  210.08
  40   0.4798     86.170  0.2478    91.896  215.41
  41   0.5177     85.900  0.2423    92.018  220.72
  42   0.4896     85.720  0.2384    92.236  226.14
  43   0.4637     86.680  0.2271    92.662  231.48
  44   0.4987     86.090  0.2140    93.082  236.79
  45   0.5282     86.010  0.2146    93.024  242.11
  46   0.5071     86.380  0.2121    93.016  247.44
  47   0.5035     85.800  0.1996    93.556  252.81
  48   0.4934     86.260  0.1885    93.954  258.21
  49   0.5082     86.790  0.1894    93.976  263.56
  50   0.4805     86.710  0.1778    94.244  268.88
  51   0.5102     86.210  0.1684    94.576  274.20
  52   0.5641     86.050  0.1723    94.428  279.53
  53   0.5229     86.210  0.1684    94.360  284.87
  54   0.4914     87.050  0.1747    94.266  290.25
  55   0.4992     86.950  0.1533    95.100  295.57
  56   0.4844     86.820  0.1622    94.820  300.91
  57   0.5238     86.800  0.1526    95.048  306.24
  58   0.5118     86.120  0.1454    95.212  311.57
  59   0.5243     86.690  0.1363    95.652  316.90
  60   0.5140     87.030  0.1456    95.222  322.25
  61   0.4824     87.100  0.1547    95.042  327.64
  62   0.5090     87.020  0.1492    95.284  332.97
  63   0.4813     87.090  0.1304    95.820  338.31
  64   0.4961     87.140  0.1365    95.466  343.67
  65   0.5081     87.710  0.1323    95.756  349.02
  66   0.5348     87.080  0.1291    95.852  354.35
  67   0.5021     87.140  0.1349    95.702  359.77
  68   0.4783     87.520  0.1485    95.266  365.08
  69   0.5231     87.200  0.1191    96.158  370.40
  70   0.5470     87.110  0.1192    96.190  375.76
  71   0.4873     87.660  0.1220    96.206  381.11
  72   0.5058     87.500  0.1120    96.518  386.43
  73   0.5171     87.120  0.1143    96.376  391.77
  74   0.5549     86.810  0.1170    96.272  397.18
  75   0.4611     88.270  0.1202    96.246  402.52
  76   0.5028     87.530  0.1022    96.798  407.85
  77   0.5242     87.600  0.1090    96.616  413.19
  78   0.5254     87.660  0.1052    96.640  418.54
  79   0.5165     87.580  0.1028    96.694  423.86
  80   0.5536     87.880  0.0948    96.968  429.30
  81   0.5204     87.290  0.1042    96.750  434.62
  82   0.5285     87.280  0.1043    96.662  439.94
  83   0.5241     87.790  0.1003    96.808  445.28
  84   0.5247     87.970  0.0967    96.944  450.62
  85   0.5059     86.870  0.0923    97.030  455.95
