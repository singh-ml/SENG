Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4041537024 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.1000     16.870  2.2313    14.518  6.91
   2   1.8533     23.000  1.9503    19.842  12.21
   3   1.7890     27.730  1.8511    23.332  17.54
   4   1.6788     30.580  1.7358    28.834  22.84
   5   1.5342     36.840  1.6144    33.800  28.18
   6   1.4199     43.730  1.5086    39.136  33.57
   7   1.3128     50.850  1.3967    45.826  38.93
   8   1.2098     55.130  1.2840    51.738  44.25
   9   1.1843     57.850  1.1844    56.472  49.58
  10   1.0701     62.420  1.1298    58.944  54.92
  11   1.0053     63.830  1.0247    62.878  60.24
  12   0.9575     66.380  0.9458    66.020  65.64
  13   0.9271     67.170  0.8918    68.304  70.96
  14   0.8641     70.980  0.8252    70.844  76.30
  15   0.7611     73.760  0.7569    73.630  81.62
  16   0.7580     73.700  0.7111    75.478  86.91
  17   0.6882     76.650  0.6812    76.498  92.22
  18   0.6952     77.350  0.6283    78.496  97.55
  19   0.6489     78.750  0.6060    79.410  102.97
  20   0.5971     80.210  0.5829    80.084  108.27
  21   0.6154     79.860  0.5351    81.948  113.58
  22   0.6387     78.550  0.5275    82.240  118.90
  23   0.5903     81.010  0.4993    83.242  124.21
  24   0.5427     82.190  0.4828    83.902  129.54
  25   0.5515     81.990  0.4702    84.304  134.95
  26   0.5797     81.790  0.4555    84.878  140.29
  27   0.5394     82.330  0.4219    85.852  145.61
  28   0.5345     83.610  0.4039    86.702  150.91
  29   0.5224     83.390  0.3837    87.190  156.23
  30   0.5568     82.940  0.3950    87.022  161.60
  31   0.5154     83.670  0.3724    87.456  166.91
  32   0.5318     83.970  0.3466    88.582  172.26
  33   0.5472     83.380  0.3600    88.166  177.56
  34   0.5310     83.360  0.3442    88.722  182.90
  35   0.5078     84.160  0.3303    89.020  188.23
  36   0.4988     84.740  0.3181    89.630  193.57
  37   0.5100     84.560  0.3037    90.166  198.87
  38   0.4909     84.940  0.3141    89.638  204.24
  39   0.5219     84.490  0.2791    90.644  209.57
  40   0.4758     85.460  0.2722    91.046  214.91
  41   0.5220     85.050  0.2573    91.564  220.25
  42   0.5559     84.140  0.2580    91.606  225.57
  43   0.5217     85.570  0.2616    91.458  230.90
  44   0.5067     85.390  0.2484    91.984  236.31
  45   0.5391     84.960  0.2361    92.342  241.62
  46   0.5168     85.780  0.2356    92.280  246.94
  47   0.4863     86.260  0.2266    92.536  252.30
  48   0.5633     85.660  0.2097    93.106  257.60
  49   0.5542     85.200  0.2050    93.254  262.91
  50   0.4806     85.930  0.2169    92.926  268.22
  51   0.4883     86.470  0.2122    93.008  273.63
  52   0.5542     86.000  0.2016    93.448  278.96
  53   0.5340     86.250  0.2039    93.370  284.68
  54   0.5013     86.430  0.1939    93.672  290.00
  55   0.4971     86.390  0.1865    94.012  295.33
  56   0.4948     86.540  0.1859    93.922  300.65
  57   0.5640     86.200  0.1785    94.252  306.05
  58   0.5083     86.680  0.1818    94.076  311.37
  59   0.4974     87.070  0.1772    94.240  316.73
  60   0.5113     86.900  0.1555    94.988  322.05
  61   0.5083     87.230  0.1675    94.588  327.36
  62   0.5767     86.470  0.1584    94.880  332.66
  63   0.5006     86.910  0.1559    95.048  338.09
  64   0.5221     86.610  0.1612    94.734  343.45
  65   0.5678     86.860  0.1487    95.120  348.79
  66   0.5448     85.750  0.1548    94.982  354.11
  67   0.5425     86.720  0.1500    95.232  359.43
  68   0.5540     86.230  0.1598    94.798  364.75
  69   0.5169     87.290  0.1498    95.244  370.15
  70   0.5338     86.930  0.1437    95.396  375.48
  71   0.5335     86.530  0.1275    95.970  380.79
  72   0.5424     87.130  0.1271    95.950  386.09
  73   0.5381     87.050  0.1211    96.098  391.40
  74   0.5265     87.310  0.1190    96.162  396.72
  75   0.5736     87.000  0.1286    95.966  402.09
  76   0.5388     87.130  0.1191    96.212  407.40
  77   0.5789     86.780  0.1270    96.030  412.72
  78   0.5543     86.590  0.1201    96.104  418.08
  79   0.5325     87.150  0.1130    96.328  423.43
  80   0.5410     86.670  0.1215    96.132  428.78
  81   0.5772     87.020  0.1171    96.210  434.10
  82   0.5622     87.320  0.1092    96.502  439.49
  83   0.5625     87.050  0.1103    96.472  444.80
  84   0.5243     87.560  0.1121    96.458  450.14
  85   0.5267     87.820  0.1066    96.608  455.45
