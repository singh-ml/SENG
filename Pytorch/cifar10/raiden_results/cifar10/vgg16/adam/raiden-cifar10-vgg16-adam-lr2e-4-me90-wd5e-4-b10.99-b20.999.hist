Use GPU: 0 for training
==> Running with ['main_adam.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '2e-4', '--weight-decay', '5e-4', '--beta1', '0.99', '--beta2', '0.999', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4042061312 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.9434     19.640  2.1486    15.946  6.95
   2   1.8304     22.920  1.8971    20.632  12.31
   3   1.7614     28.120  1.8214    24.998  17.75
   4   1.5887     35.810  1.6937    31.444  23.11
   5   1.4834     41.300  1.5582    37.442  28.42
   6   1.3723     46.700  1.4510    42.264  33.76
   7   1.2317     52.790  1.3315    48.692  39.11
   8   1.2125     54.100  1.2354    53.432  44.42
   9   1.0548     61.200  1.1216    58.344  49.83
  10   0.9351     66.200  0.9996    63.506  55.16
  11   0.9165     68.220  0.9192    67.104  60.49
  12   0.8828     70.070  0.8518    70.074  65.85
  13   0.8141     72.140  0.7882    72.524  71.16
  14   0.7356     75.180  0.7368    74.392  76.49
  15   0.7180     75.710  0.7029    75.838  81.93
  16   0.7336     76.680  0.6695    77.056  87.25
  17   0.6829     77.760  0.6246    78.896  92.59
  18   0.6224     80.010  0.5824    80.288  97.91
  19   0.6277     79.380  0.5618    81.268  103.23
  20   0.6787     79.040  0.5464    81.670  108.58
  21   0.5961     80.550  0.5151    82.866  114.00
  22   0.6241     79.970  0.4968    83.650  119.34
  23   0.5747     81.750  0.4684    84.508  124.69
  24   0.5745     82.300  0.4527    85.062  130.01
  25   0.5913     81.160  0.4395    85.376  135.35
  26   0.5410     83.090  0.4177    86.242  140.70
  27   0.5296     82.960  0.3917    87.106  146.03
  28   0.5655     82.890  0.3885    87.122  151.42
  29   0.5120     84.350  0.3666    87.786  156.75
  30   0.5129     83.950  0.3613    88.138  162.08
  31   0.5236     84.120  0.3482    88.822  167.40
  32   0.4996     84.890  0.3412    88.908  172.72
  33   0.5047     84.970  0.3150    89.578  178.08
  34   0.5146     84.460  0.3170    89.540  183.45
  35   0.5592     83.580  0.3139    89.706  188.79
  36   0.5017     84.870  0.3132    89.662  194.12
  37   0.5204     84.670  0.2928    90.480  199.48
  38   0.5129     85.100  0.3003    90.098  204.83
  39   0.5224     85.080  0.2809    90.930  210.19
  40   0.4894     86.070  0.2633    91.410  215.53
  41   0.5141     85.330  0.2555    91.760  220.91
  42   0.5134     85.780  0.2379    92.242  226.24
  43   0.4899     85.960  0.2371    92.216  231.57
  44   0.4894     86.330  0.2292    92.472  236.89
  45   0.4819     85.950  0.2285    92.622  242.20
  46   0.5027     85.970  0.2285    92.496  247.53
  47   0.4999     85.940  0.2353    92.236  252.91
  48   0.5178     86.140  0.2071    93.188  258.24
  49   0.5266     85.670  0.2117    93.140  263.60
  50   0.5190     85.980  0.1967    93.590  268.92
  51   0.4891     86.710  0.2011    93.510  274.30
  52   0.5355     85.560  0.2048    93.470  279.62
  53   0.4944     86.860  0.1885    93.842  284.95
  54   0.5136     86.310  0.1888    93.840  290.28
  55   0.4973     86.640  0.1747    94.458  295.62
  56   0.4898     86.590  0.1839    93.986  300.93
  57   0.4792     87.150  0.1743    94.322  306.29
  58   0.5225     86.650  0.1716    94.558  311.65
  59   0.4786     87.240  0.1718    94.558  316.99
  60   0.4873     87.310  0.1553    94.988  322.46
  61   0.5060     86.400  0.1655    94.616  327.81
  62   0.5208     86.930  0.1634    94.734  333.16
  63   0.4803     87.110  0.1511    95.168  338.48
  64   0.5241     86.300  0.1582    94.928  343.85
  65   0.5219     86.570  0.1523    95.094  349.19
  66   0.5285     86.600  0.1464    95.198  354.56
  67   0.5042     87.120  0.1475    95.200  359.98
  68   0.5359     86.810  0.1460    95.420  365.31
  69   0.5390     87.700  0.1377    95.606  370.64
  70   0.4968     87.700  0.1330    95.736  375.98
  71   0.5439     87.820  0.1249    95.960  381.30
  72   0.5230     87.590  0.1238    96.024  386.66
  73   0.5327     87.100  0.1261    95.928  392.05
  74   0.5312     87.250  0.1193    96.230  397.40
  75   0.5034     87.680  0.1204    96.178  402.75
  76   0.5434     87.150  0.1239    96.018  408.07
  77   0.5520     86.400  0.1218    96.118  413.41
  78   0.5340     87.730  0.1248    96.050  418.75
  79   0.5517     86.690  0.1127    96.372  424.16
  80   0.5444     87.210  0.1165    96.252  429.53
  81   0.5534     87.230  0.1041    96.698  434.85
  82   0.5478     87.390  0.1009    96.760  440.23
  83   0.5193     87.740  0.0998    96.800  445.60
  84   0.5340     86.880  0.1040    96.740  450.94
  85   0.5635     86.930  0.1053    96.614  456.32
  86   0.5483     86.850  0.1085    96.550  461.67
  87   0.5197     87.930  0.0994    96.820  467.02
  88   0.5151     87.580  0.1038    96.636  472.38
  89   0.5207     87.630  0.0931    96.976  477.72
  90   0.5480     87.330  0.1006    96.794  483.05
