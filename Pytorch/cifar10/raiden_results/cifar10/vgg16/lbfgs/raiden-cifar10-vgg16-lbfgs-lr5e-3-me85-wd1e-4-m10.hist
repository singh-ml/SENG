Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8721016320 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.3033     10.070  4.2432     9.860  10.74
   2   2.3033     10.070  4.2425    10.338  19.87
   3   2.3033     10.070  4.2430     9.850  29.08
   4   2.3033     10.070  4.2428     9.946  38.18
   5   2.3033     10.070  4.2428    10.084  47.28
   6   2.3033     10.070  4.2430    10.112  56.40
   7   2.3033     10.070  4.2428    10.216  65.62
   8   2.3033     10.070  4.2430     9.640  74.75
   9   2.3033     10.070  4.2432     9.998  83.85
  10   2.3033     10.070  4.2428    10.232  92.94
  11   2.3033     10.070  4.2431     9.754  102.13
  12   2.3033     10.070  4.2432     9.838  111.24
  13   2.3033     10.070  4.2429     9.882  120.36
  14   2.3033     10.070  4.2431     9.974  129.51
  15   2.3033     10.070  4.2430     9.922  138.68
  16   2.3033     10.070  4.2432     9.914  147.82
  17   2.3033     10.070  4.2431     9.932  156.93
  18   2.3033     10.070  4.2428    10.136  166.04
  19   2.3033     10.070  4.2430     9.822  175.22
  20   2.3033     10.070  4.2428     9.884  184.33
  21   2.3033     10.070  4.2426    10.256  193.47
  22   2.3033     10.070  4.2431     9.766  202.67
  23   2.3033     10.070  4.2429    10.006  211.79
  24   2.3033     10.070  4.2431     9.850  220.92
  25   2.3033     10.070  4.2431     9.978  230.04
  26   2.3033     10.070  4.2431     9.686  239.26
  27   2.3033     10.070  4.2429    10.170  248.39
  28   2.3033     10.070  4.2428    10.084  257.52
  29   2.3033     10.070  4.2431     9.902  266.62
  30   2.3033     10.070  4.2430    10.098  275.83
  31   2.3033     10.070  4.2428    10.120  284.95
  32   2.3033     10.070  4.2429    10.160  294.09
  33   2.3033     10.070  4.2428    10.156  303.29
  34   2.3033     10.070  4.2432     9.902  312.39
  35   2.3033     10.070  4.2430     9.952  321.48
  36   2.3033     10.070  4.2427    10.032  330.59
  37   2.3033     10.070  4.2431     9.768  339.81
  38   2.3033     10.070  4.2428    10.156  348.95
  39   2.3033     10.070  4.2429     9.978  358.07
  40   2.3033     10.070  4.2431     9.774  367.19
  41   2.3033     10.070  4.2430     9.828  376.38
  42   2.3033     10.070  4.2430     9.878  385.48
  43   2.3033     10.070  4.2430     9.826  394.59
  44   2.3033     10.070  4.2430     9.912  403.69
  45   2.3033     10.070  4.2430     9.962  412.92
  46   2.3033     10.070  4.2432     9.782  422.05
  47   2.3033     10.070  4.2428    10.142  431.16
  48   2.3033     10.070  4.2429    10.048  440.29
  49   2.3033     10.070  4.2428     9.958  449.52
  50   2.3033     10.070  4.2430    10.088  458.63
  51   2.3033     10.070  4.2428     9.878  467.76
  52   2.3033     10.070  4.2431     9.846  476.89
  53   2.3033     10.070  4.2428    10.128  486.08
  54   2.3033     10.070  4.2432     9.812  495.20
  55   2.3033     10.070  4.2426    10.070  504.32
  56   2.3033     10.070  4.2427    10.200  513.44
  57   2.3033     10.070  4.2430     9.946  522.62
  58   2.3033     10.070  4.2429     9.678  531.74
  59   2.3033     10.070  4.2429     9.850  540.88
  60   2.3033     10.070  4.2426    10.114  550.02
  61   2.3033     10.070  4.2427    10.098  559.23
  62   2.3033     10.070  4.2429     9.884  568.38
  63   2.3033     10.070  4.2427     9.934  577.50
  64   2.3033     10.070  4.2431     9.868  586.65
  65   2.3033     10.070  4.2427    10.152  595.76
  66   2.3033     10.070  4.2428    10.048  604.91
  67   2.3033     10.070  4.2430     9.702  614.05
  68   2.3033     10.070  4.2428     9.984  623.25
  69   2.3033     10.070  4.2429    10.078  632.38
  70   2.3033     10.070  4.2429    10.068  641.49
  71   2.3033     10.070  4.2427    10.136  650.60
  72   2.3033     10.070  4.2429     9.882  659.81
  73   2.3033     10.070  4.2429     9.894  668.93
  74   2.3033     10.070  4.2429    10.094  678.09
  75   2.3033     10.070  4.2429     9.988  687.23
  76   2.3033     10.070  4.2428     9.920  696.45
  77   2.3033     10.070  4.2428    10.152  705.57
  78   2.3033     10.070  4.2431     9.808  714.69
  79   2.3033     10.070  4.2428     9.902  723.78
  80   2.3033     10.070  4.2430    10.028  732.97
  81   2.3033     10.070  4.2428    10.090  742.09
  82   2.3033     10.070  4.2428    10.058  751.20
  83   2.3033     10.070  4.2427    10.110  760.41
  84   2.3033     10.070  4.2428     9.824  769.53
  85   2.3033     10.070  4.2429    10.060  778.63
