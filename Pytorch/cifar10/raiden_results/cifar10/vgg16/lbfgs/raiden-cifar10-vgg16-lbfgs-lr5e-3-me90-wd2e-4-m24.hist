Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8718992384 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.3023     10.120  6.1805     9.996  10.68
   2   2.3023     10.120  6.1804     9.950  19.63
   3   2.3023     10.110  6.1801    10.008  28.64
   4   2.3023     10.110  6.1806     9.882  37.67
   5   2.3023     10.110  6.1804    10.002  46.62
   6   2.3023     10.110  6.1804     9.898  55.56
   7   2.3023     10.110  6.1806    10.028  64.51
   8   2.3023     10.120  6.1804     9.810  73.44
   9   2.3023     10.120  6.1802     9.940  82.32
  10   2.3023     10.120  6.1804    10.132  91.26
  11   2.3023     10.120  6.1804    10.126  100.30
  12   2.3023     10.120  6.1804     9.930  109.28
  13   2.3023     10.120  6.1802     9.932  118.12
  14   2.3023     10.120  6.1806     9.846  127.10
  15   2.3023     10.120  6.1805    10.012  136.22
  16   2.3023     10.120  6.1804     9.898  145.16
  17   2.3023     10.120  6.1803     9.780  154.13
  18   2.3023     10.120  6.1806     9.986  163.16
  19   2.3023     10.120  6.1804    10.094  172.09
  20   2.3023     10.120  6.1802    10.034  181.00
  21   2.3023     10.120  6.1804     9.896  189.86
  22   2.3023     10.120  6.1803     9.696  198.82
  23   2.3023     10.120  6.1806     9.946  207.74
  24   2.3023     10.120  6.1803     9.986  216.66
  25   2.3023     10.120  6.1804     9.904  225.61
  26   2.3023     10.120  6.1804    10.160  234.61
  27   2.3023     10.120  6.1801    10.084  243.50
  28   2.3023     10.120  6.1804     9.982  252.44
  29   2.3023     10.120  6.1805     9.774  261.42
  30   2.3023     10.120  6.1804     9.908  270.44
  31   2.3023     10.120  6.1805     9.940  279.33
  32   2.3023     10.120  6.1802     9.926  288.28
  33   2.3023     10.120  6.1805     9.828  297.19
  34   2.3023     10.120  6.1805     9.820  306.24
  35   2.3023     10.120  6.1800    10.148  315.17
  36   2.3023     10.120  6.1806     9.964  324.09
  37   2.3023     10.120  6.1804     9.948  333.02
  38   2.3023     10.120  6.1802    10.178  342.01
  39   2.3023     10.120  6.1804     9.998  350.98
  40   2.3023     10.120  6.1803    10.082  359.88
  41   2.3023     10.120  6.1802    10.114  368.83
  42   2.3023     10.120  6.1804    10.032  377.84
  43   2.3023     10.120  6.1805     9.892  386.80
  44   2.3023     10.120  6.1804     9.950  395.73
  45   2.3023     10.120  6.1807     9.878  404.75
  46   2.3023     10.120  6.1803    10.068  413.66
  47   2.3023     10.120  6.1804     9.952  422.61
  48   2.3023     10.120  6.1803     9.964  431.59
  49   2.3023     10.120  6.1803     9.902  440.58
  50   2.3023     10.120  6.1805     9.900  449.52
  51   2.3023     10.120  6.1804    10.064  458.44
  52   2.3023     10.120  6.1804     9.916  467.34
  53   2.3023     10.120  6.1805     9.910  476.26
  54   2.3023     10.120  6.1804     9.968  485.32
  55   2.3023     10.120  6.1806    10.154  494.17
  56   2.3023     10.120  6.1806     9.638  503.07
  57   2.3023     10.120  6.1802     9.962  512.09
  58   2.3023     10.120  6.1803     9.890  521.09
  59   2.3023     10.120  6.1803    10.120  529.99
  60   2.3023     10.120  6.1806     9.780  538.91
  61   2.3023     10.120  6.1805    10.110  547.95
  62   2.3023     10.120  6.1805     9.798  556.85
  63   2.3023     10.120  6.1804    10.052  565.76
  64   2.3023     10.120  6.1803    10.094  574.71
  65   2.3023     10.120  6.1804     9.832  583.71
  66   2.3023     10.120  6.1804     9.868  592.61
  67   2.3023     10.120  6.1805     9.882  601.57
  68   2.3023     10.120  6.1803    10.108  610.51
  69   2.3023     10.120  6.1803     9.910  619.53
  70   2.3023     10.120  6.1805     9.874  628.40
  71   2.3023     10.120  6.1804    10.154  637.30
  72   2.3023     10.120  6.1804     9.912  646.40
  73   2.3023     10.120  6.1804     9.764  655.31
  74   2.3023     10.120  6.1803    10.122  664.31
  75   2.3023     10.120  6.1802    10.158  673.19
  76   2.3023     10.120  6.1802    10.064  682.26
  77   2.3023     10.120  6.1802     9.994  691.19
  78   2.3023     10.120  6.1803     9.856  700.11
  79   2.3023     10.120  6.1803    10.004  709.07
  80   2.3023     10.120  6.1804     9.824  718.17
  81   2.3023     10.120  6.1803     9.990  727.12
  82   2.3023     10.120  6.1803    10.020  735.98
  83   2.3023     10.120  6.1803    10.154  744.94
  84   2.3023     10.120  6.1806     9.722  753.96
  85   2.3023     10.120  6.1805    10.022  762.91
  86   2.3023     10.120  6.1802    10.210  771.81
  87   2.3023     10.120  6.1805     9.812  780.77
  88   2.3023     10.120  6.1804    10.002  789.76
  89   2.3023     10.120  6.1804    10.040  798.67
  90   2.3023     10.120  6.1803     9.992  807.59
