Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 13016253440 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.3032      9.820  6.1809    10.096  13.29
   2   2.3032      9.820  6.1811     9.696  24.87
   3   2.3032      9.820  6.1810     9.638  36.45
   4   2.3032      9.820  6.1809     9.848  48.04
   5   2.3032      9.820  6.1812     9.954  59.61
   6   2.3032      9.820  6.1810    10.082  71.19
   7   2.3032      9.820  6.1812     9.810  82.81
   8   2.3032      9.820  6.1811     9.744  95.17
   9   2.3032      9.820  6.1812     9.764  107.56
  10   2.3032      9.820  6.1809     9.912  119.99
  11   2.3032      9.820  6.1811     9.868  132.39
  12   2.3032      9.820  6.1814     9.658  144.78
  13   2.3032      9.820  6.1808     9.846  157.22
  14   2.3032      9.820  6.1808     9.936  169.57
  15   2.3032      9.820  6.1812     9.814  182.00
  16   2.3032      9.820  6.1809     9.850  194.38
  17   2.3032      9.820  6.1809     9.856  206.74
  18   2.3032      9.820  6.1812     9.874  219.18
  19   2.3032      9.820  6.1807     9.770  231.55
  20   2.3032      9.820  6.1812     9.846  243.93
  21   2.3032      9.820  6.1809     9.994  256.35
  22   2.3032      9.820  6.1810     9.704  268.71
  23   2.3032      9.820  6.1807     9.948  281.10
  24   2.3032      9.820  6.1812     9.834  293.53
  25   2.3032      9.820  6.1810     9.854  305.92
  26   2.3032      9.820  6.1811     9.944  318.29
  27   2.3032      9.820  6.1811     9.714  330.73
  28   2.3032      9.820  6.1811     9.798  343.13
  29   2.3032      9.820  6.1808     9.746  355.49
  30   2.3032      9.820  6.1810    10.030  367.93
  31   2.3032      9.820  6.1809     9.888  380.30
  32   2.3032      9.820  6.1811     9.746  392.73
  33   2.3032      9.820  6.1811     9.740  405.10
  34   2.3032      9.820  6.1806    10.030  417.49
  35   2.3032      9.820  6.1810     9.904  429.92
  36   2.3032      9.820  6.1809     9.838  442.31
  37   2.3032      9.820  6.1812     9.826  454.68
  38   2.3032      9.820  6.1809     9.948  467.12
  39   2.3032      9.820  6.1810    10.010  479.47
  40   2.3032      9.820  6.1810     9.846  491.83
  41   2.3032      9.820  6.1810     9.774  504.26
  42   2.3032      9.820  6.1810     9.920  516.66
  43   2.3032      9.820  6.1810     9.678  529.00
  44   2.3032      9.820  6.1809     9.950  541.42
  45   2.3032      9.820  6.1809     9.854  553.82
  46   2.3032      9.820  6.1807     9.992  566.19
  47   2.3032      9.820  6.1811     9.804  578.64
  48   2.3032      9.820  6.1809    10.030  591.02
  49   2.3032      9.820  6.1812     9.616  603.43
  50   2.3032      9.820  6.1811     9.806  615.78
  51   2.3032      9.820  6.1812     9.570  628.17
  52   2.3032      9.820  6.1807    10.082  640.60
  53   2.3032      9.820  6.1810     9.872  652.97
  54   2.3032      9.820  6.1811     9.894  665.36
  55   2.3032      9.820  6.1808     9.986  677.80
  56   2.3032      9.820  6.1809     9.924  690.16
  57   2.3032      9.820  6.1809     9.970  702.55
  58   2.3032      9.820  6.1810     9.706  715.01
  59   2.3032      9.820  6.1809     9.828  727.37
  60   2.3032      9.820  6.1809    10.048  739.75
  61   2.3032      9.820  6.1808    10.080  752.20
  62   2.3032      9.820  6.1811     9.772  764.56
  63   2.3032      9.820  6.1810     9.724  776.93
  64   2.3032      9.820  6.1810     9.854  789.38
  65   2.3032      9.820  6.1811     9.782  801.77
  66   2.3032      9.820  6.1809     9.774  814.15
  67   2.3032      9.820  6.1811     9.840  826.59
  68   2.3032      9.820  6.1810    10.178  838.97
  69   2.3032      9.820  6.1808     9.954  851.40
  70   2.3032      9.820  6.1810     9.854  863.75
  71   2.3032      9.820  6.1812     9.716  876.11
  72   2.3032      9.820  6.1809     9.926  888.57
  73   2.3032      9.820  6.1810     9.996  900.92
  74   2.3032      9.820  6.1809     9.800  913.30
  75   2.3032      9.820  6.1807     9.936  925.75
  76   2.3032      9.820  6.1809     9.988  938.17
  77   2.3032      9.820  6.1810     9.872  950.60
  78   2.3032      9.820  6.1811     9.766  962.98
  79   2.3032      9.820  6.1810     9.900  975.37
  80   2.3032      9.820  6.1810     9.692  987.86
  81   2.3032      9.820  6.1812     9.946  1000.24
  82   2.3032      9.820  6.1808     9.862  1012.62
  83   2.3032      9.820  6.1810    10.076  1025.06
  84   2.3032      9.820  6.1811     9.822  1037.44
  85   2.3032      9.820  6.1813     9.752  1049.78
