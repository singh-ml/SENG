Use GPU: 0 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 8721016320 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.3024     10.630  6.1802     9.828  10.88
   2   2.3024     10.630  6.1802     9.872  20.13
   3   2.3024     10.630  6.1802    10.146  29.28
   4   2.3024     10.630  6.1802     9.974  38.40
   5   2.3024     10.630  6.1803     9.838  47.52
   6   2.3024     10.630  6.1799    10.018  56.72
   7   2.3024     10.630  6.1800     9.916  65.87
   8   2.3024     10.630  6.1802     9.910  75.00
   9   2.3024     10.630  6.1800    10.042  84.15
  10   2.3024     10.630  6.1800     9.918  93.34
  11   2.3024     10.630  6.1798    10.126  102.48
  12   2.3024     10.630  6.1800    10.046  111.63
  13   2.3024     10.630  6.1801     9.912  120.85
  14   2.3024     10.630  6.1799    10.120  129.96
  15   2.3024     10.630  6.1801    10.056  139.13
  16   2.3024     10.630  6.1801    10.190  148.25
  17   2.3024     10.630  6.1800    10.174  157.46
  18   2.3024     10.630  6.1803     9.896  166.61
  19   2.3024     10.630  6.1800     9.980  175.73
  20   2.3024     10.630  6.1800    10.086  184.88
  21   2.3024     10.630  6.1801    10.018  194.06
  22   2.3024     10.630  6.1802     9.946  203.20
  23   2.3024     10.630  6.1798    10.188  212.35
  24   2.3024     10.630  6.1800    10.004  221.51
  25   2.3024     10.630  6.1805     9.836  230.72
  26   2.3024     10.630  6.1799     9.836  239.86
  27   2.3024     10.630  6.1799     9.936  248.97
  28   2.3024     10.630  6.1801    10.094  258.12
  29   2.3024     10.630  6.1801    10.046  267.34
  30   2.3024     10.630  6.1803    10.058  276.46
  31   2.3024     10.630  6.1799     9.928  285.58
  32   2.3024     10.630  6.1801    10.008  294.77
  33   2.3024     10.630  6.1798    10.150  303.91
  34   2.3024     10.630  6.1800    10.008  313.04
  35   2.3024     10.630  6.1800     9.878  322.19
  36   2.3024     10.630  6.1801     9.926  331.40
  37   2.3024     10.630  6.1801    10.054  340.54
  38   2.3024     10.630  6.1801    10.004  349.72
  39   2.3024     10.630  6.1801     9.868  358.85
  40   2.3024     10.630  6.1802     9.898  368.05
  41   2.3024     10.630  6.1800     9.956  377.17
  42   2.3024     10.630  6.1801     9.876  386.32
  43   2.3024     10.630  6.1801    10.052  395.43
  44   2.3024     10.630  6.1801    10.104  404.64
  45   2.3024     10.630  6.1802     9.842  413.76
  46   2.3024     10.630  6.1802     9.826  422.89
  47   2.3024     10.630  6.1800    10.148  432.09
  48   2.3024     10.630  6.1802    10.010  441.23
  49   2.3024     10.630  6.1801     9.948  450.38
  50   2.3024     10.630  6.1801    10.032  459.51
  51   2.3024     10.630  6.1802     9.906  468.74
  52   2.3024     10.630  6.1801     9.962  477.86
  53   2.3024     10.630  6.1799     9.952  486.98
  54   2.3024     10.630  6.1798    10.050  496.13
  55   2.3024     10.630  6.1801    10.062  505.33
  56   2.3024     10.630  6.1799    10.138  514.44
  57   2.3024     10.630  6.1802     9.930  523.56
  58   2.3024     10.630  6.1800    10.064  532.71
  59   2.3024     10.630  6.1803     9.710  541.87
  60   2.3024     10.630  6.1801    10.012  551.01
  61   2.3024     10.630  6.1800     9.960  560.12
  62   2.3024     10.630  6.1802    10.136  569.25
  63   2.3024     10.630  6.1801    10.046  578.44
  64   2.3024     10.630  6.1800     9.906  587.57
  65   2.3024     10.630  6.1800     9.992  596.70
  66   2.3024     10.630  6.1799    10.080  605.81
  67   2.3024     10.630  6.1799    10.058  615.04
  68   2.3024     10.630  6.1800    10.178  624.18
  69   2.3024     10.630  6.1801    10.016  633.34
  70   2.3024     10.630  6.1797    10.178  642.52
  71   2.3024     10.630  6.1803     9.912  651.67
  72   2.3024     10.630  6.1801    10.128  660.85
  73   2.3024     10.630  6.1800     9.926  669.98
  74   2.3024     10.630  6.1801     9.794  679.22
  75   2.3024     10.630  6.1801    10.094  688.34
  76   2.3024     10.630  6.1801    10.032  697.48
  77   2.3024     10.630  6.1805     9.892  706.65
  78   2.3024     10.630  6.1801    10.082  715.87
  79   2.3024     10.630  6.1801     9.922  725.00
  80   2.3024     10.630  6.1802    10.036  734.13
  81   2.3024     10.630  6.1801    10.216  743.33
  82   2.3024     10.630  6.1803     9.898  752.46
  83   2.3024     10.630  6.1804     9.934  761.62
  84   2.3024     10.630  6.1800    10.014  770.75
  85   2.3024     10.630  6.1802    10.064  779.94
