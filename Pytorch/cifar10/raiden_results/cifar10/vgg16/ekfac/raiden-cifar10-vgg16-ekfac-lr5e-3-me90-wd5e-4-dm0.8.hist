Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14133946880 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.6057     38.400  1.9306    25.610  65.08
   2   1.0420     63.270  1.3720    49.118  128.21
   3   0.8929     68.880  0.9802    65.460  191.41
   4   0.7437     75.080  0.7976    72.870  254.61
   5   0.6255     79.490  0.6873    77.120  317.81
   6   0.6081     80.290  0.6067    79.984  380.97
   7   0.5334     82.260  0.5542    81.870  444.22
   8   0.5199     82.610  0.4966    83.608  507.43
   9   0.5211     82.860  0.4608    84.968  570.54
  10   0.5115     83.840  0.4274    86.192  633.83
  11   0.4490     85.260  0.3970    87.132  696.98
  12   0.4625     85.110  0.3728    87.906  760.20
  13   0.4181     86.310  0.3531    88.388  823.36
  14   0.4372     86.220  0.3305    89.290  886.47
  15   0.3977     87.400  0.3125    89.868  949.69
  16   0.3740     87.990  0.2942    90.266  1012.86
  17   0.4188     87.040  0.2794    90.930  1076.11
  18   0.3969     88.070  0.2683    91.150  1139.25
  19   0.4094     87.720  0.2497    91.816  1202.46
  20   0.3865     88.110  0.2349    92.360  1265.70
  21   0.4040     88.050  0.2229    92.660  1328.84
  22   0.3995     88.980  0.2181    92.720  1392.08
  23   0.3927     88.580  0.2050    93.206  1455.18
  24   0.3618     89.050  0.2008    93.448  1518.51
  25   0.3483     89.300  0.1811    94.034  1581.70
  26   0.3740     88.960  0.1676    94.556  1644.87
  27   0.3764     89.070  0.1674    94.468  1708.02
  28   0.3751     89.090  0.1604    94.734  1771.20
  29   0.3740     89.680  0.1529    94.976  1834.34
  30   0.3409     89.810  0.1429    95.284  1897.49
  31   0.4036     89.180  0.1337    95.568  1960.60
  32   0.3645     89.980  0.1277    95.798  2023.79
  33   0.4500     89.140  0.1219    95.900  2086.94
  34   0.3669     90.410  0.1189    96.016  2150.08
  35   0.3908     89.820  0.1099    96.332  2213.28
  36   0.3650     90.380  0.1108    96.386  2276.45
  37   0.3759     90.270  0.0992    96.736  2339.60
  38   0.3875     89.890  0.0979    96.750  2402.99
  39   0.3934     89.550  0.0932    96.876  2466.86
  40   0.3690     90.730  0.0839    97.248  2530.02
  41   0.4221     90.650  0.0839    97.200  2593.16
  42   0.3603     90.470  0.0756    97.494  2656.35
  43   0.4116     90.320  0.0724    97.644  2719.50
  44   0.3916     90.620  0.0714    97.630  2782.66
  45   0.4098     90.430  0.0633    97.872  2845.81
  46   0.4413     90.410  0.0583    98.110  2908.98
  47   0.3991     91.150  0.0560    98.136  2972.29
  48   0.4100     90.620  0.0521    98.274  3035.39
  49   0.4148     90.850  0.0505    98.300  3098.52
  50   0.4187     90.170  0.0447    98.540  3140.89
  51   0.4201     90.690  0.0457    98.492  3203.95
  52   0.4130     90.750  0.0403    98.654  3267.10
  53   0.4145     91.010  0.0388    98.692  3330.27
  54   0.4478     90.350  0.0368    98.778  3393.45
  55   0.4739     90.590  0.0331    98.902  3456.57
  56   0.4485     91.180  0.0322    98.978  3519.72
  57   0.4249     90.850  0.0315    98.926  3583.25
  58   0.4586     91.270  0.0295    98.996  3646.34
  59   0.4745     90.550  0.0276    99.066  3709.52
  60   0.4363     90.960  0.0270    99.126  3772.62
  61   0.4491     91.010  0.0214    99.328  3835.71
  62   0.4563     90.790  0.0216    99.280  3898.94
  63   0.4069     91.580  0.0225    99.244  3962.11
  64   0.4408     91.190  0.0205    99.306  4025.25
  65   0.4654     91.030  0.0192    99.338  4088.45
  66   0.4564     91.440  0.0167    99.462  4151.49
  67      nan     10.000     nan    39.666  4213.99
