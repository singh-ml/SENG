Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14133946880 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.7640     33.320  1.9379    25.024  65.13
   2   1.0696     61.270  1.3786    48.672  128.36
   3   0.8360     70.960  0.9872    65.378  191.57
   4   0.7676     74.290  0.7978    72.868  254.71
   5   0.6401     78.860  0.6788    77.564  317.93
   6   0.5710     81.570  0.5999    80.226  381.08
   7   0.5697     81.390  0.5449    82.180  444.14
   8   0.5640     82.990  0.5014    83.668  507.32
   9   0.4753     84.580  0.4509    85.324  570.38
  10   0.4705     84.670  0.4211    86.450  633.45
  11   0.4271     85.690  0.4009    87.036  696.50
  12   0.4519     84.460  0.3800    87.694  759.53
  13   0.4121     87.130  0.3448    88.792  822.66
  14   0.3845     87.400  0.3326    89.070  885.77
  15   0.4210     86.760  0.3118    89.714  949.05
  16   0.4028     87.670  0.2939    90.316  1012.14
  17   0.3886     87.930  0.2844    90.766  1075.20
  18   0.4184     87.510  0.2576    91.580  1138.29
  19   0.3894     88.420  0.2565    91.580  1201.37
  20   0.3811     88.290  0.2367    92.110  1264.47
  21   0.4063     87.360  0.2320    92.352  1327.51
  22   0.3654     88.770  0.2184    92.822  1390.75
  23   0.3820     88.120  0.2034    93.218  1453.78
  24   0.3833     88.600  0.1980    93.504  1516.89
  25   0.3768     88.030  0.1928    93.632  1579.96
  26   0.3852     89.260  0.1797    94.134  1643.02
  27   0.3801     88.750  0.1702    94.326  1706.15
  28   0.3880     89.170  0.1611    94.752  1769.22
  29   0.3707     89.210  0.1502    94.996  1832.28
  30   0.4080     89.080  0.1425    95.304  1895.31
  31   0.3643     89.550  0.1329    95.642  1958.39
  32   0.4125     89.070  0.1305    95.692  2021.49
  33   0.3949     89.690  0.1230    95.936  2084.55
  34   0.4141     89.960  0.1154    96.148  2147.64
  35   0.3829     89.770  0.1122    96.328  2210.72
  36   0.4204     89.660  0.1062    96.392  2273.95
  37   0.4795     89.300  0.0964    96.816  2337.04
  38   0.4108     89.640  0.0999    96.760  2400.14
  39   0.3975     89.920  0.0934    96.896  2463.18
  40   0.4203     89.990  0.0857    97.296  2526.33
  41   0.4209     90.380  0.0819    97.280  2589.46
  42   0.3814     90.810  0.0771    97.522  2652.56
  43   0.4190     90.280  0.0752    97.540  2715.59
  44   0.3791     90.510  0.0724    97.620  2778.79
  45   0.4171     90.220  0.0637    97.882  2841.98
  46   0.4378     90.160  0.0625    97.960  2905.05
  47   0.4235     90.730  0.0601    98.074  2968.20
  48   0.4581     90.420  0.0554    98.122  3031.26
  49   0.4424     90.660  0.0493    98.352  3094.34
  50   0.4442     90.870  0.0467    98.492  3136.56
  51   0.4232     90.480  0.0442    98.616  3199.64
  52   0.4388     90.710  0.0408    98.684  3262.85
  53   0.4437     90.680  0.0425    98.616  3325.88
  54   0.4725     90.820  0.0360    98.792  3388.96
  55   0.4608     90.770  0.0344    98.830  3452.15
  56   0.4923     90.350  0.0318    98.964  3515.23
  57   0.5059     90.580  0.0320    98.984  3578.31
  58   0.4756     90.810  0.0309    98.982  3641.38
  59   0.4443     91.120  0.0292    99.054  3704.46
  60   0.4576     90.940  0.0244    99.204  3767.54
  61   0.4821     91.280  0.0260    99.138  3830.62
  62   0.4632     90.880  0.0236    99.206  3893.70
  63      nan     10.000     nan    32.466  3956.25
