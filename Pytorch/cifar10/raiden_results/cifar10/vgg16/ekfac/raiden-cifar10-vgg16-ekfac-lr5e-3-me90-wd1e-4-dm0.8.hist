Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '90', '--arch', 'vgg16', '--lr-decay-epoch', '90', '--damping', '0.8', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14133946880 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.6383     35.140  1.9460    24.698  65.00
   2   1.1529     57.730  1.3950    47.128  128.10
   3   0.8982     69.400  1.0098    64.096  191.27
   4   0.7254     75.650  0.8125    72.212  254.29
   5   0.6451     78.730  0.6986    76.396  317.25
   6   0.5743     80.900  0.6145    79.590  380.36
   7   0.5816     81.010  0.5496    81.840  443.39
   8   0.4969     83.530  0.5044    83.376  506.40
   9   0.5473     82.580  0.4624    84.704  569.45
  10   0.5107     83.160  0.4332    85.852  632.47
  11   0.4616     84.750  0.3959    87.066  695.46
  12   0.4255     86.670  0.3787    87.628  758.51
  13   0.4426     86.360  0.3601    88.248  821.53
  14   0.4378     86.700  0.3343    89.052  884.53
  15   0.4251     86.990  0.3169    89.548  947.55
  16   0.4218     87.280  0.3027    89.930  1010.56
  17   0.4191     87.050  0.2829    90.618  1073.61
  18   0.3854     88.050  0.2676    91.238  1136.75
  19   0.4549     87.480  0.2468    91.886  1199.74
  20   0.4030     87.930  0.2377    92.322  1262.78
  21   0.3962     88.310  0.2321    92.332  1325.78
  22   0.4216     87.560  0.2250    92.684  1388.83
  23   0.3884     88.060  0.2098    92.980  1451.84
  24   0.3909     88.570  0.2126    93.082  1514.90
  25   0.3761     89.020  0.1904    93.820  1577.96
  26   0.4343     88.590  0.1700    94.360  1641.02
  27   0.3601     89.650  0.1697    94.356  1704.08
  28   0.3892     89.050  0.1565    94.838  1767.11
  29   0.4201     88.270  0.1574    94.888  1830.13
  30   0.4096     88.950  0.1510    95.036  1893.16
  31   0.3600     89.200  0.1421    95.292  1956.23
  32   0.3907     89.400  0.1322    95.572  2019.24
  33   0.3751     89.850  0.1245    95.908  2082.27
  34   0.3733     89.050  0.1154    96.296  2145.27
  35   0.3992     89.120  0.1146    96.268  2208.27
  36   0.4058     89.640  0.1070    96.452  2271.32
  37   0.3704     90.270  0.1052    96.616  2334.35
  38   0.3777     90.310  0.0924    96.912  2397.44
  39   0.3831     90.360  0.0944    96.876  2460.46
  40   0.3916     90.050  0.0892    97.140  2523.51
  41   0.4266     90.110  0.0818    97.304  2586.51
  42   0.4157     90.020  0.0811    97.316  2649.53
  43   0.4423     89.770  0.0760    97.478  2712.54
  44   0.3933     89.750  0.0705    97.708  2775.48
  45   0.4203     89.900  0.0653    97.890  2838.71
  46   0.4780     90.190  0.0635    97.876  2901.73
  47   0.4278     90.150  0.0623    98.004  2964.76
  48   0.4277     90.530  0.0551    98.168  3027.78
  49      nan     10.000     nan    97.252  3090.78
  50      nan     10.000     nan    10.000  3132.24
