Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '85', '--arch', 'vgg16', '--lr-decay-epoch', '85', '--damping', '1.2', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 14133946880 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5105     40.210  1.9215    24.484  66.09
   2   1.0592     63.120  1.3951    47.306  128.85
   3   0.9313     68.250  0.9941    64.800  191.59
   4   0.6957     76.450  0.8060    72.292  254.30
   5   0.6490     77.820  0.6865    77.032  317.00
   6   0.5985     80.570  0.6203    79.390  379.74
   7   0.5387     82.650  0.5475    81.964  442.64
   8   0.5299     82.780  0.5025    83.584  505.40
   9   0.4811     84.150  0.4739    84.340  568.11
  10   0.4861     84.520  0.4278    86.128  630.82
  11   0.4689     84.880  0.3919    87.004  693.56
  12   0.5616     84.110  0.3710    88.032  756.34
  13   0.4475     85.300  0.3458    88.516  819.20
  14   0.4133     87.380  0.3319    89.118  881.96
  15   0.4265     86.980  0.3065    89.974  944.71
  16   0.4113     87.150  0.2963    90.328  1007.51
  17   0.3904     87.780  0.2720    90.880  1070.33
  18   0.4200     87.920  0.2641    91.388  1133.08
  19   0.3873     88.260  0.2465    91.946  1195.83
  20   0.3993     88.090  0.2372    92.164  1258.59
  21   0.3956     88.030  0.2269    92.526  1321.38
  22   0.4196     87.330  0.2162    93.066  1384.23
  23   0.3778     88.650  0.2075    93.288  1446.93
  24   0.3763     88.940  0.1950    93.538  1509.87
  25   0.3983     88.630  0.1851    93.902  1572.69
  26   0.3793     88.800  0.1800    93.992  1635.44
  27   0.4117     88.820  0.1644    94.594  1698.22
  28   0.3875     88.920  0.1537    95.012  1760.98
  29   0.3751     88.590  0.1533    94.988  1823.71
  30   0.3988     89.590  0.1457    95.204  1886.57
  31   0.3699     89.430  0.1372    95.558  1949.36
  32   0.4088     89.450  0.1244    95.926  2012.19
  33   0.4069     89.550  0.1220    96.026  2075.10
  34   0.3910     89.220  0.1154    96.264  2137.94
  35   0.4202     89.790  0.1104    96.414  2200.71
  36   0.3831     89.980  0.1042    96.524  2263.54
  37   0.3742     90.030  0.0972    96.662  2326.30
  38   0.4280     90.060  0.0939    96.926  2389.10
  39   0.4119     90.090  0.0932    96.930  2451.82
  40   0.3961     90.270  0.0820    97.248  2514.55
  41   0.4291     90.330  0.0803    97.372  2577.33
  42   0.4031     90.380  0.0732    97.588  2640.13
  43   0.4213     90.260  0.0713    97.646  2702.85
  44   0.4238     90.610  0.0641    97.906  2765.66
  45   0.3898     90.690  0.0660    97.864  2828.47
  46   0.4065     90.250  0.0612    98.026  2891.24
  47   0.4549     90.740  0.0546    98.272  2954.03
  48   0.4440     90.630  0.0501    98.328  3017.00
  49   0.4057     90.740  0.0470    98.514  3079.78
  50   0.4255     90.360  0.0461    98.496  3121.68
  51   0.4636     90.450  0.0436    98.604  3184.44
  52   0.4295     90.640  0.0397    98.674  3247.20
  53   0.4314     90.960  0.0390    98.756  3309.91
  54   0.4714     90.380  0.0361    98.816  3372.60
  55   0.4509     90.820  0.0303    98.962  3435.37
  56   0.5066     90.650  0.0284    99.062  3498.07
  57   0.4620     90.920  0.0306    99.064  3560.82
  58   0.5198     90.680  0.0268    99.136  3623.64
  59   0.4560     91.150  0.0261    99.142  3686.42
  60   0.4701     90.900  0.0236    99.222  3749.11
  61      nan     10.000     nan    28.672  3811.44
