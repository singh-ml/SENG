Use GPU: 0 for training
==> Running with ['main_sgd.py', '--epoch', '85', '--arch', 'resnet50', '--lr-decay-epoch', '85', '--trainset', 'cifar10', '--datadir', '/data/ghighdim/singh/data/', '--lr', '1e-1', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 17946863104 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.2839     12.200  4.0751    11.964  25.32
   2   2.0835     19.380  2.1412    17.996  48.25
   3   1.9038     24.810  1.9506    22.734  71.17
   4   1.8463     26.380  1.8937    24.734  94.12
   5   1.7910     33.180  1.8279    28.388  117.03
   6   1.6101     39.910  1.7135    34.582  139.93
   7   1.5250     42.980  1.6045    40.080  162.86
   8   1.4503     47.090  1.4952    44.464  185.80
   9   1.3662     50.090  1.3907    48.444  208.70
  10   1.4540     50.480  1.2957    52.506  231.61
  11   1.2592     55.940  1.2138    55.370  254.54
  12   1.0489     62.170  1.1236    59.170  277.46
  13   1.0000     64.640  1.0479    62.062  300.35
  14   0.9590     66.090  0.9839    64.578  323.28
  15   0.9171     67.370  0.9240    66.848  346.17
  16   0.8793     69.650  0.8742    68.782  369.07
  17   0.8801     69.370  0.8164    70.922  391.98
  18   0.7903     72.400  0.7780    72.220  414.89
  19   0.8789     71.140  0.7458    73.538  437.83
  20   0.7692     73.630  0.7143    74.836  460.73
  21   0.7088     75.790  0.6789    76.076  483.66
  22   0.7355     75.050  0.6455    77.054  506.61
  23   0.6435     77.910  0.6216    77.900  529.51
  24   0.6290     77.690  0.5935    79.188  552.46
  25   0.6129     79.460  0.5701    79.988  575.37
  26   0.6100     79.620  0.5497    80.928  598.28
  27   0.5527     80.760  0.5304    81.430  621.23
  28   0.6320     80.110  0.5056    82.514  644.14
  29   0.5771     80.610  0.4833    83.060  667.06
  30   0.5917     80.980  0.4650    83.784  689.97
  31   0.5240     83.000  0.4525    84.018  712.90
  32   0.5187     82.480  0.4327    84.712  735.81
  33   0.5034     83.320  0.4086    85.842  758.74
  34   0.5402     82.020  0.3981    86.172  781.67
  35   0.4662     83.880  0.3818    86.576  804.57
  36   0.4832     83.830  0.3757    86.676  827.49
  37   0.4820     84.210  0.3558    87.646  850.40
  38   0.5031     83.930  0.3555    87.512  873.29
  39   0.5132     83.970  0.3327    88.234  896.24
  40   0.4763     84.780  0.3231    88.528  919.18
  41   0.4759     84.590  0.3079    89.146  942.09
  42   0.4880     84.980  0.2978    89.484  964.99
  43   0.4752     84.970  0.2804    90.074  987.89
  44   0.4411     86.390  0.2711    90.396  1010.81
  45   0.4547     85.840  0.2623    90.670  1033.69
  46   0.4213     86.830  0.2542    90.966  1056.63
  47   0.4742     85.550  0.2409    91.542  1079.57
  48   0.4135     87.070  0.2309    91.780  1102.50
  49   0.5044     84.810  0.2258    92.056  1125.42
  50   0.4293     86.980  0.2087    92.578  1148.35
  51   0.4737     86.700  0.2053    92.778  1171.25
  52   0.4474     87.070  0.1923    93.246  1194.21
  53   0.4436     87.400  0.1817    93.592  1217.16
  54   0.4274     87.400  0.1704    93.852  1240.07
  55   0.4380     87.370  0.1630    94.048  1263.01
  56   0.4289     87.570  0.1524    94.524  1285.96
  57   0.4356     87.670  0.1426    94.962  1308.88
  58   0.4714     86.920  0.1344    95.156  1331.77
  59   0.4799     87.380  0.1271    95.496  1354.69
  60   0.4593     87.860  0.1184    95.852  1377.64
  61   0.4834     87.750  0.1086    96.120  1400.54
  62   0.4406     88.820  0.1015    96.476  1423.45
  63   0.4817     88.450  0.0948    96.644  1446.37
  64   0.4384     88.620  0.0911    96.730  1469.29
  65   0.4751     88.310  0.0771    97.304  1492.23
  66   0.5061     88.280  0.0726    97.434  1515.16
  67   0.4859     88.480  0.0711    97.514  1538.05
  68   0.4655     89.290  0.0605    97.936  1560.95
  69   0.4911     88.670  0.0545    98.160  1583.88
  70   0.4756     88.950  0.0525    98.194  1606.79
  71   0.4820     89.390  0.0472    98.384  1629.73
  72   0.5036     89.110  0.0404    98.658  1652.63
  73   0.4899     89.330  0.0376    98.720  1675.57
  74   0.5077     88.940  0.0334    98.932  1698.46
  75   0.5427     88.970  0.0314    98.952  1721.37
  76   0.5184     89.100  0.0293    99.028  1744.27
  77   0.4876     89.270  0.0268    99.138  1767.21
  78   0.5360     89.210  0.0260    99.146  1790.11
  79   0.6143     88.500  0.0241    99.242  1813.05
  80   0.5401     88.640  0.0223    99.348  1835.94
  81   0.5677     88.560  0.0222    99.296  1858.86
  82   0.6070     88.920  0.0222    99.336  1881.80
  83   0.5435     89.370  0.0203    99.400  1904.73
  84   0.5399     88.840  0.0218    99.320  1927.67
  85   0.5246     89.660  0.0195    99.432  1950.62
