Use GPU: 1 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '2', '--trainset', 'cifar100', '--datadir', '/data/singh/data/', '--lr', '1e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '1']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6351025152 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.1252      7.660  4.3786     4.568  71.05
   2   3.8241     11.730  3.9763    10.106  139.04
   3   3.6479     14.070  3.7462    13.014  206.99
   4   3.5120     16.630  3.5934    15.128  274.98
   5   3.4015     18.340  3.4647    17.306  342.97
   6   3.3146     19.840  3.3584    19.192  410.82
   7   3.2008     21.970  3.2531    21.322  478.64
   8   3.1052     24.280  3.1522    23.090  546.54
   9   3.0229     25.670  3.0514    25.186  614.55
  10   2.8980     27.550  2.9484    27.136  682.52
  11   2.8314     28.940  2.8532    28.620  750.48
  12   2.7446     30.540  2.7619    30.142  818.39
  13   2.7024     31.310  2.6821    31.724  886.29
  14   2.6115     33.270  2.5993    33.062  954.22
  15   2.5224     34.830  2.5330    34.846  1022.15
  16   2.5135     34.990  2.4647    36.146  1090.13
  17   2.4776     36.060  2.3984    37.590  1158.06
  18   2.4124     36.850  2.3413    38.596  1226.05
  19   2.4355     36.600  2.2786    40.178  1294.11
  20   2.3052     39.450  2.2318    40.888  1362.03
  21   2.2884     40.430  2.1715    42.344  1429.91
  22   2.2472     40.440  2.1306    43.444  1498.01
  23   2.2523     40.530  2.0762    44.534  1565.97
  24   2.1542     42.770  2.0232    45.808  1633.90
  25   2.1089     43.750  1.9812    46.508  1701.81
  26   2.0680     44.940  1.9381    47.720  1769.90
  27   2.0606     44.610  1.8982    48.476  1837.96
  28   2.0832     44.900  1.8529    49.598  1906.02
  29   1.9666     47.350  1.8145    50.602  1974.07
  30   1.9817     46.960  1.7800    51.092  2041.99
  31   2.0316     46.380  1.7493    51.796  2109.97
  32   1.9588     47.550  1.7105    52.792  2178.00
  33   1.9939     47.290  1.6723    53.870  2245.97
  34   1.9745     47.740  1.6358    54.638  2313.85
  35   1.9248     48.600  1.6041    55.290  2381.70
  36   1.9078     48.770  1.5814    55.870  2449.66
  37   1.8800     49.820  1.5437    56.910  2517.74
  38   1.8652     50.520  1.5169    57.558  2585.72
  39   1.8350     51.010  1.4869    58.282  2653.65
  40   1.8582     50.390  1.4647    58.722  2721.65
  41   1.7917     52.430  1.4282    59.672  2789.66
  42   1.8490     50.560  1.4050    60.328  2857.69
  43   1.7786     52.170  1.3774    61.008  2925.77
  44   1.7972     52.150  1.3533    61.558  2993.79
  45   1.7608     53.240  1.3277    62.132  3061.73
  46   1.7610     52.850  1.2994    62.766  3129.73
  47   1.7908     52.530  1.2805    63.524  3197.67
  48   1.7336     54.210  1.2518    64.136  3265.66
  49   1.7483     53.490  1.2295    64.746  3333.61
  50   1.7794     53.590  1.1977    65.562  3401.62
  51   1.7359     54.170  1.1779    66.032  3469.72
  52   1.6906     55.140  1.1501    66.830  3537.70
  53   1.7405     54.630  1.1337    67.232  3605.75
  54   1.6851     55.190  1.1084    67.844  3673.70
  55   1.7639     54.540  1.0852    68.600  3741.72
  56   1.6907     55.500  1.0612    69.078  3809.75
  57   1.7392     55.030  1.0404    69.464  3877.78
  58   1.7336     54.780  1.0232    70.062  3945.86
  59   1.7338     54.960  1.0056    70.644  4013.94
  60   1.8136     54.620  0.9785    71.380  4081.97
  61   1.6901     56.060  0.9563    72.084  4149.97
  62   1.6694     56.250  0.9414    72.524  4217.87
  63   1.7003     56.730  0.9162    73.224  4285.99
  64   1.7323     55.920  0.8944    73.624  4354.08
  65   1.6392     56.800  0.8743    74.228  4422.15
  66   1.6722     56.630  0.8610    74.498  4490.11
  67   1.7270     55.760  0.8392    75.210  4558.06
