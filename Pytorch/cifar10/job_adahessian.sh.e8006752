/etc/shinit_v2: line 3: sed: command not found
/etc/shinit_v2: line 8: hostname: command not found
/etc/shinit_v2: line 42: dpkg: command not found
/uge_mnt/var/spool/uge/dl-gtn01/job_scripts/8006752: line 8: bash: command not found
/uge_mnt/home/dinesh/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:154: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/torch/csrc/autograd/engine.cpp:976.)
  Variable._execution_engine.run_backward(
Traceback (most recent call last):
  File "/uge_mnt/home/dinesh/SENG/Pytorch/cifar10/main_adahessian.py", line 352, in <module>
    main()
  File "/uge_mnt/home/dinesh/SENG/Pytorch/cifar10/main_adahessian.py", line 130, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "/uge_mnt/home/dinesh/SENG/Pytorch/cifar10/main_adahessian.py", line 334, in main_worker
    train_loss, train_acc = train(epoch)
  File "/uge_mnt/home/dinesh/SENG/Pytorch/cifar10/main_adahessian.py", line 291, in train
    optimizer.step()
  File "/uge_mnt/home/dinesh/anaconda3/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/uge_mnt/home/dinesh/SENG/Pytorch/cifar10/adahessian.py", line 126, in step
    hut_traces = self.get_trace(params, grads)
  File "/uge_mnt/home/dinesh/SENG/Pytorch/cifar10/adahessian.py", line 70, in get_trace
    hvs = torch.autograd.grad(
  File "/uge_mnt/home/dinesh/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py", line 234, in grad
    return Variable._execution_engine.run_backward(
RuntimeError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 39.59 GiB total capacity; 37.11 GiB already allocated; 34.19 MiB free; 37.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
