Use GPU: 0 for training
==> Running with ['main_ekfac.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--damping', '1.0', '--trainset', 'cifar100', '--datadir', '/data/singh/data/', '--lr', '5e-2', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '0']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 5018745856 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.3367     19.520  3.7515    12.590  37.01
   2   2.9887     27.920  3.0527    23.922  70.13
   3   2.5688     35.570  2.5752    33.310  103.72
   4   2.2976     40.750  2.2118    41.056  136.74
   5   2.2766     41.700  1.9599    46.850  170.95
   6   2.1445     45.580  1.7453    51.748  204.86
   7   1.9861     48.620  1.5941    55.240  238.38
   8   1.8640     51.560  1.4590    58.680  271.87
   9   1.6774     56.080  1.3288    61.830  306.02
  10   1.6063     57.530  1.2265    64.542  339.24
  11   1.5484     58.990  1.1413    66.650  373.40
  12   1.5210     60.240  1.0524    68.828  407.18
  13   1.5272     60.460  0.9765    70.764  441.41
  14   1.4556     61.760  0.9101    72.664  475.31
  15   1.6490     60.520  0.8436    74.456  509.22
  16   1.4273     63.470  0.7725    76.206  542.79
  17   1.4804     62.670  0.7205    77.628  576.54
  18   1.5487     62.310  0.6644    79.352  610.53
  19   1.4166     65.080  0.6087    80.868  644.22
  20   1.5151     63.730  0.5599    82.300  677.91
  21   1.4779     65.230  0.5104    83.584  711.65
  22   1.7399     62.630  0.4658    84.880  745.44
  23   1.5299     65.430  0.4237    86.182  779.75
  24   1.6198     64.490  0.3772    87.726  813.63
  25   1.5003     66.830  0.3492    88.500  847.31
  26   1.6426     66.200  0.3112    89.846  881.20
  27   1.6752     65.780  0.2790    90.762  915.44
  28   1.6488     66.610  0.2516    91.706  949.44
  29   1.6302     67.420  0.2259    92.578  983.27
  30   1.6394     68.080  0.1965    93.566  1017.49
  31   1.7491     66.650  0.1752    94.278  1051.29
  32   1.6978     67.960  0.1489    95.278  1085.18
  33   1.7550     68.760  0.1318    95.716  1119.10
  34   1.7304     68.190  0.1156    96.358  1152.88
  35   1.7235     69.100  0.1059    96.596  1186.90
  36   1.6814     68.370  0.0901    97.150  1221.02
  37   1.7223     69.250  0.0796    97.554  1255.19
  38   1.6993     69.720  0.0668    98.040  1289.25
  39   1.6538     70.240  0.0575    98.338  1323.30
  40   1.7184     70.350  0.0467    98.680  1357.26
  41   1.6724     70.880  0.0441    98.794  1391.07
  42   1.6819     70.780  0.0347    99.092  1424.90
  43   1.7117     71.030  0.0268    99.378  1458.64
  44   1.6552     71.610  0.0231    99.472  1492.34
  45   1.6638     71.790  0.0230    99.438  1526.18
  46   1.6538     71.520  0.0208    99.556  1560.36
  47   1.6637     71.820  0.0169    99.664  1594.06
  48   1.6507     71.710  0.0135    99.730  1627.81
  49   1.6410     72.190  0.0119    99.804  1661.70
  50   1.6411     71.940  0.0100    99.844  1687.62
  51   1.6348     72.350  0.0096    99.846  1721.73
  52   1.6244     72.410  0.0088    99.868  1755.50
  53   1.6314     72.620  0.0079    99.882  1789.23
  54   1.6030     72.770  0.0063    99.920  1822.91
  55   1.5969     72.950  0.0060    99.910  1856.79
  56   1.5981     72.670  0.0067    99.926  1890.67
  57   1.6040     72.870  0.0058    99.916  1924.80
  58   1.5885     73.050  0.0050    99.946  1959.02
  59   1.5926     73.150  0.0049    99.950  1993.40
  60   1.5996     72.880  0.0043    99.944  2027.30
  61   1.5978     73.040  0.0042    99.950  2061.08
