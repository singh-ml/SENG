Use GPU: 2 for training
==> Running with ['main_nsgd.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '5', '--trainset', 'cifar10', '--datadir', '/data/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '2']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6339986432 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   2.0450     20.360  2.5037    16.096  70.88
   2   1.8564     29.250  1.9235    24.082  138.85
   3   1.6090     40.510  1.6988    35.072  206.81
   4   1.4611     45.210  1.5209    43.272  274.76
   5   1.5672     46.320  1.3619    50.046  342.78
   6   1.2966     53.200  1.2476    54.724  410.86
   7   1.1330     59.090  1.1394    59.000  478.81
   8   1.1534     60.600  1.0350    63.002  546.68
   9   0.9431     66.780  0.9394    66.610  614.60
  10   0.8880     69.540  0.8514    69.598  682.51
  11   0.7830     72.440  0.7761    72.502  750.35
  12   0.8801     70.820  0.7089    74.988  818.21
  13   0.7907     73.580  0.6511    77.122  886.17
  14   0.7587     74.530  0.5834    79.658  954.04
  15   0.5862     80.180  0.5483    80.780  1022.01
  16   0.6751     77.600  0.5025    82.450  1089.95
  17   0.7795     75.450  0.4744    83.500  1157.76
  18   0.5038     82.840  0.4405    84.760  1225.65
  19   0.4685     84.060  0.4189    85.608  1293.49
  20   0.4935     83.490  0.3941    86.384  1361.61
  21   0.5409     82.270  0.3681    87.234  1429.56
  22   0.5131     83.640  0.3458    88.048  1497.44
  23   0.4608     85.180  0.3304    88.498  1565.30
  24   0.4640     84.870  0.3087    89.310  1633.08
  25   0.4416     85.640  0.2998    89.576  1700.98
  26   0.4703     85.200  0.2693    90.692  1768.91
  27   0.4455     85.920  0.2654    90.758  1836.74
  28   0.4782     85.020  0.2539    91.176  1904.58
  29   0.4284     86.120  0.2346    91.822  1972.49
  30   0.4224     86.590  0.2213    92.400  2040.40
  31   0.4731     86.250  0.2116    92.448  2108.22
  32   0.4002     88.180  0.1971    93.028  2176.12
  33   0.3844     87.720  0.1894    93.314  2244.03
  34   0.3966     88.200  0.1726    93.834  2311.99
  35   0.4745     86.390  0.1698    94.016  2379.75
  36   0.4263     87.030  0.1581    94.382  2447.62
  37   0.4340     87.210  0.1450    94.896  2515.56
  38   0.4424     87.710  0.1392    95.116  2583.51
  39   0.4185     88.010  0.1317    95.372  2651.64
  40   0.4394     87.650  0.1256    95.546  2719.62
  41   0.4057     88.660  0.1176    95.774  2787.61
  42   0.4022     89.010  0.1081    96.206  2855.57
  43   0.4152     88.760  0.0970    96.666  2923.45
  44   0.3910     89.590  0.0979    96.656  2991.31
  45   0.4319     88.840  0.0882    96.892  3059.30
  46   0.4169     89.730  0.0832    97.096  3127.28
  47   0.4143     89.690  0.0777    97.222  3195.17
  48   0.4370     89.630  0.0662    97.692  3263.11
  49   0.4343     89.400  0.0623    97.804  3331.13
  50   0.4222     90.090  0.0582    97.992  3399.12
  51   0.4795     89.290  0.0544    98.092  3467.07
  52   0.4246     89.700  0.0476    98.348  3535.02
  53   0.4564     89.380  0.0455    98.434  3602.97
  54   0.4256     90.390  0.0382    98.692  3670.88
  55   0.4177     90.800  0.0380    98.722  3738.86
  56   0.4434     90.750  0.0301    98.952  3806.81
  57   0.4724     90.000  0.0280    99.092  3874.70
