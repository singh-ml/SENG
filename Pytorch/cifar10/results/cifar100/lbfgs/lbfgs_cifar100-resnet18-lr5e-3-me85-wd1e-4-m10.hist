Use GPU: 2 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/singh/data/', '--lr', '5e-3', '--weight-decay', '1e-4', '--lr-scheme', 'cosine', '--gpu', '2']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239565312 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7373      0.820  5.9917     0.888  24.39
   2   4.7386      0.840  5.9910     0.838  46.51
   3   4.7375      0.800  5.9917     0.918  68.93
   4   4.7380      0.850  5.9919     0.876  91.45
   5   4.7369      0.850  5.9922     0.838  113.98
   6   4.7374      0.840  5.9918     0.820  136.18
   7   4.7363      0.870  5.9912     0.848  158.70
   8   4.7375      0.850  5.9920     0.892  181.10
   9   4.7379      0.810  5.9914     0.890  203.46
  10   4.7368      0.850  5.9918     0.916  225.72
  11   4.7388      0.830  5.9914     0.912  248.14
  12   4.7358      0.830  5.9925     0.896  270.64
  13   4.7387      0.800  5.9923     0.884  293.18
  14   4.7386      0.840  5.9922     0.874  315.66
  15   4.7389      0.840  5.9914     0.880  338.10
  16   4.7387      0.860  5.9912     0.798  360.30
  17   4.7384      0.860  5.9916     0.792  382.70
  18   4.7373      0.850  5.9915     0.836  404.93
  19   4.7383      0.800  5.9923     0.928  427.25
  20   4.7386      0.840  5.9917     0.872  449.60
  21   4.7364      0.840  5.9915     0.798  471.85
  22   4.7380      0.840  5.9918     0.878  494.16
  23   4.7368      0.870  5.9918     0.908  516.46
  24   4.7403      0.830  5.9915     0.898  538.82
  25   4.7400      0.840  5.9917     0.910  561.42
  26   4.7384      0.810  5.9909     0.926  583.70
  27   4.7386      0.850  5.9913     0.942  605.85
  28   4.7393      0.850  5.9921     0.888  628.34
  29   4.7374      0.850  5.9921     0.908  650.56
  30   4.7370      0.850  5.9914     0.846  672.87
  31   4.7363      0.820  5.9918     0.866  695.53
  32   4.7386      0.830  5.9919     0.826  718.01
  33   4.7381      0.870  5.9910     0.854  740.12
  34   4.7360      0.780  5.9907     0.914  762.45
  35   4.7371      0.850  5.9920     0.960  784.84
  36   4.7366      0.830  5.9921     0.916  807.01
  37   4.7359      0.830  5.9923     0.884  829.30
  38   4.7377      0.830  5.9919     0.874  851.72
  39   4.7373      0.810  5.9920     0.872  874.01
  40   4.7372      0.830  5.9913     0.850  896.04
  41   4.7374      0.820  5.9910     0.894  918.25
  42   4.7393      0.820  5.9922     0.834  940.36
  43   4.7389      0.820  5.9912     0.886  962.68
  44   4.7371      0.820  5.9903     0.876  985.02
  45   4.7350      0.840  5.9922     0.870  1007.16
  46   4.7366      0.860  5.9918     0.856  1029.53
  47   4.7396      0.830  5.9921     0.884  1051.96
  48   4.7395      0.840  5.9923     0.884  1074.37
  49   4.7368      0.830  5.9913     0.834  1096.50
  50   4.7373      0.820  5.9915     0.852  1118.82
  51   4.7354      0.810  5.9920     0.838  1141.20
  52   4.7384      0.820  5.9921     0.786  1163.37
  53   4.7381      0.820  5.9910     0.874  1185.81
  54   4.7378      0.810  5.9915     0.824  1208.08
  55   4.7380      0.790  5.9919     0.874  1230.31
  56   4.7377      0.850  5.9921     0.838  1252.42
  57   4.7396      0.860  5.9918     0.882  1274.59
  58   4.7381      0.840  5.9917     0.836  1296.70
  59   4.7353      0.840  5.9920     0.924  1318.85
  60   4.7394      0.820  5.9921     0.852  1341.28
  61   4.7381      0.830  5.9918     0.810  1363.43
  62   4.7378      0.830  5.9921     0.868  1385.59
  63   4.7359      0.840  5.9921     0.836  1407.93
  64   4.7357      0.870  5.9921     0.888  1430.08
  65   4.7365      0.870  5.9912     0.926  1452.26
  66   4.7381      0.810  5.9927     0.876  1474.78
  67   4.7389      0.830  5.9913     0.950  1497.24
  68   4.7392      0.780  5.9916     0.876  1519.36
  69   4.7383      0.840  5.9918     0.896  1541.55
  70   4.7364      0.830  5.9924     0.924  1564.32
  71   4.7373      0.830  5.9916     0.880  1586.52
  72   4.7374      0.820  5.9905     0.886  1608.82
  73   4.7387      0.860  5.9924     0.866  1631.14
  74   4.7371      0.840  5.9916     0.882  1653.36
  75   4.7386      0.830  5.9927     0.858  1675.92
  76   4.7378      0.780  5.9916     0.820  1698.15
  77   4.7363      0.800  5.9921     0.882  1720.29
  78   4.7376      0.840  5.9917     0.892  1742.48
  79   4.7354      0.830  5.9918     0.924  1764.91
  80   4.7389      0.830  5.9914     0.924  1787.19
  81   4.7378      0.840  5.9910     0.864  1809.44
  82   4.7410      0.870  5.9923     0.898  1831.59
  83   4.7405      0.830  5.9923     0.864  1853.91
  84   4.7375      0.810  5.9919     0.862  1876.44
  85   4.7370      0.840  5.9922     0.912  1898.89
