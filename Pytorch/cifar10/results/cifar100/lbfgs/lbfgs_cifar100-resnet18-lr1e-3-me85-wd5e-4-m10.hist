Use GPU: 2 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '10', '--trainset', 'cifar100', '--datadir', '/data/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '2']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239565312 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7103      0.950  10.9783     0.956  24.12
   2   4.7070      0.920  10.9780     0.938  46.25
   3   4.7084      0.940  10.9779     0.910  67.92
   4   4.7109      0.950  10.9788     0.946  89.59
   5   4.7095      0.950  10.9783     0.960  111.18
   6   4.7105      0.970  10.9778     0.962  132.71
   7   4.7092      0.940  10.9779     0.978  154.34
   8   4.7086      0.950  10.9780     0.908  175.83
   9   4.7094      0.950  10.9784     0.948  197.40
  10   4.7109      0.960  10.9777     0.974  219.08
  11   4.7103      0.940  10.9777     0.950  240.61
  12   4.7116      0.960  10.9777     0.956  262.18
  13   4.7108      0.950  10.9774     0.962  283.94
  14   4.7090      0.950  10.9782     0.944  305.51
  15   4.7115      0.960  10.9784     0.976  327.06
  16   4.7114      0.960  10.9783     0.946  348.72
  17   4.7069      0.960  10.9778     0.952  370.33
  18   4.7092      0.940  10.9767     0.942  391.91
  19   4.7107      0.950  10.9777     0.940  413.57
  20   4.7102      0.950  10.9785     0.916  435.13
  21   4.7094      0.950  10.9776     0.970  456.76
  22   4.7109      0.960  10.9787     0.956  478.34
  23   4.7099      0.950  10.9791     0.954  500.00
  24   4.7107      0.960  10.9783     0.974  521.62
  25   4.7078      0.940  10.9780     0.970  543.18
  26   4.7101      0.950  10.9774     0.956  564.76
  27   4.7077      0.940  10.9781     0.982  586.20
  28   4.7097      0.940  10.9780     0.940  607.66
  29   4.7090      0.950  10.9773     0.954  629.23
  30   4.7093      0.950  10.9778     0.952  650.74
  31   4.7081      0.960  10.9776     0.952  672.28
  32   4.7097      0.940  10.9788     0.950  693.90
  33   4.7097      0.960  10.9783     0.956  715.55
  34   4.7099      0.940  10.9788     0.978  737.09
  35   4.7089      0.940  10.9781     0.966  758.80
  36   4.7088      0.950  10.9777     0.940  780.35
  37   4.7084      0.940  10.9779     0.932  801.93
  38   4.7106      0.940  10.9771     0.946  823.57
  39   4.7098      0.960  10.9781     0.954  845.16
  40   4.7102      0.940  10.9765     0.966  866.69
  41   4.7098      0.950  10.9777     0.938  888.33
  42   4.7089      0.940  10.9783     0.946  909.91
  43   4.7092      0.940  10.9773     0.968  931.54
  44   4.7091      0.950  10.9785     0.980  953.21
  45   4.7091      0.960  10.9783     0.950  974.76
  46   4.7092      0.940  10.9779     0.948  996.33
  47   4.7105      0.950  10.9785     0.910  1018.06
  48   4.7113      0.950  10.9773     0.982  1039.78
  49   4.7096      0.930  10.9782     0.956  1061.32
  50   4.7098      0.940  10.9775     0.930  1082.89
  51   4.7127      0.960  10.9776     0.950  1104.79
  52   4.7114      0.960  10.9781     0.940  1126.47
  53   4.7080      0.950  10.9779     0.946  1148.10
  54   4.7088      0.950  10.9780     0.946  1169.83
  55   4.7093      0.970  10.9776     0.958  1191.44
  56   4.7086      0.940  10.9782     0.940  1212.95
  57   4.7087      0.930  10.9781     0.952  1234.57
  58   4.7086      0.950  10.9785     0.962  1256.11
  59   4.7089      0.950  10.9777     0.970  1277.76
  60   4.7101      0.950  10.9785     0.960  1299.26
  61   4.7099      0.950  10.9791     0.940  1320.82
  62   4.7104      0.950  10.9787     0.964  1342.41
  63   4.7088      0.950  10.9771     0.974  1363.88
  64   4.7102      0.940  10.9785     0.980  1385.52
  65   4.7102      0.950  10.9780     0.962  1407.22
  66   4.7101      0.950  10.9791     0.946  1428.96
  67   4.7100      0.960  10.9771     0.966  1450.68
  68   4.7088      0.930  10.9777     0.930  1472.36
  69   4.7078      0.950  10.9786     0.954  1493.87
  70   4.7097      0.960  10.9786     0.988  1515.45
  71   4.7087      0.950  10.9781     0.928  1537.11
  72   4.7120      0.960  10.9774     0.974  1558.60
  73   4.7108      0.940  10.9784     0.940  1580.23
  74   4.7084      0.920  10.9791     0.948  1601.74
  75   4.7094      0.940  10.9779     0.936  1623.27
  76   4.7102      0.950  10.9791     0.944  1644.86
  77   4.7086      0.960  10.9793     0.968  1666.36
  78   4.7081      0.940  10.9783     0.936  1687.96
  79   4.7077      0.960  10.9779     0.944  1709.58
  80   4.7119      0.950  10.9791     0.942  1731.09
  81   4.7116      0.950  10.9778     0.940  1752.66
  82   4.7121      0.950  10.9773     0.974  1774.37
  83   4.7092      0.950  10.9783     0.914  1796.06
  84   4.7095      0.940  10.9778     0.926  1817.59
  85   4.7117      0.950  10.9761     0.932  1839.18
