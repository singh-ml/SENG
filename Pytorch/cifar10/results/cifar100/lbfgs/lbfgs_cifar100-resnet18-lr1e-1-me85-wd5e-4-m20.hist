Use GPU: 2 for training
==> Running with ['main_lbfgs.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '-m', '20', '--trainset', 'cifar100', '--datadir', '/data/singh/data/', '--lr', '1e-1', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '2']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 5437247488 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.5821      1.900  10.9175     1.416  28.43
   2   4.5266      2.430  10.8192     1.970  54.70
   3   4.4162      4.010  10.7678     3.078  80.99
   4   4.3260      5.410  10.6113     4.590  106.93
   5   6.3676      1.000  32.0079     4.622  133.10
   6   4.7621      3.080  12.0020     2.010  159.68
   7   4.6994      1.000  55.6462     2.516  185.94
   8   4.8623      1.530  10.1316     1.116  212.32
   9   4.6845      1.710  10.9342     1.656  238.62
  10   4.6724      2.810  8.9713     2.472  265.10
  11   4.6548      2.800  8.7552     2.940  291.16
  12   2279223334.4000      1.000  963777071127.5793     2.144  316.96
  13   4158292.0625      1.030  534669162.1224     1.050  342.87
  14   144911505.2000      1.000  126170390416.0000     1.120  369.17
  15   8384074.7062      1.080  527057284.9796     1.094  395.90
  16   32815631.2500      1.000  43406711097.3878     1.064  422.28
  17   14740067.0723      1.290  983763979.1837     1.162  448.10
  18   5126123.1531      1.280  205025054.2041     1.186  474.56
  19   11692778.9097      1.190  146544694.0000     1.056  500.62
  20   15850187.8515      1.010  128155249.7959     1.060  526.95
  21   9197453.9873      0.900  125612267.5102     0.972  553.33
  22   13976872.2110      0.920  125605160.0000     0.904  579.12
  23   1172714.7688      1.010  136747040.8163     1.128  604.93
  24   6351267.2109      0.490  120679876.6122     1.144  630.57
  25   14058191.8207      0.890  111059538.3265     0.704  656.71
  26   9578378.5939      0.910  110332901.3061     0.976  682.80
  27   5662028.1541      1.000  110712141.7959     0.982  709.31
  28   10094904.0699      0.880  110156593.0204     0.990  735.81
  29   8750093.3830      0.950  110549219.1020     1.034  762.14
  30   8817922.3402      1.370  109955802.6122     1.060  788.59
  31   3678279.1338      1.380  110623687.7551     1.166  814.50
  32   6634459.0619      1.300  110820562.3265     1.124  840.59
  33   3664692.3929      1.300  110826919.7959     1.084  865.76
  34   9859252.0080      1.180  111035707.2653     1.192  891.14
  35   8318767.6666      1.290  110363060.4082     1.182  916.66
  36   9228031.6061      1.190  110326012.0816     1.120  942.48
  37   9127331.0875      1.280  109935296.4490     1.158  968.12
  38   4208506.4271      1.320  111251038.0000     1.148  993.24
  39   5303599.3408      1.370  110384154.5714     1.132  1018.84
  40   7198975.1367      1.270  110174454.2449     1.088  1044.16
  41   6013870.6746      1.330  110657821.4286     1.086  1069.75
  42   12393722.0170      1.110  110817080.8571     1.184  1095.27
  43   10314261.4836      1.300  110663718.6939     1.042  1120.74
  44   6098566.9831      1.320  110358147.5510     1.052  1146.34
  45   9107997.3730      1.360  111088916.6531     1.052  1171.95
  46   8648860.3227      1.410  110223004.0816     1.126  1197.29
  47   4819620.7677      1.350  110757405.4694     1.126  1222.73
  48   6025126.7273      1.390  110128340.3673     1.114  1248.20
  49   10033004.6547      1.320  110301242.8980     1.128  1273.50
  50   4749298.0215      1.400  110317181.4286     1.082  1298.90
  51   9994425.8404      1.230  110900962.8980     1.000  1324.59
  52   7272865.4256      1.210  109972652.5714     1.124  1350.19
  53   5978124.7568      1.270  110320255.9184     1.134  1375.74
  54   6039840.7553      1.270  111057121.7551     1.118  1400.89
  55   9484898.8309      1.100  110497486.1633     1.214  1426.17
  56   5413203.1471      1.290  111217757.0612     1.114  1451.77
  57   10572401.0621      1.360  110514369.9592     1.070  1477.10
  58   5176887.2842      1.100  110356625.5918     1.134  1503.01
  59   6448966.4162      1.350  110520612.7755     1.182  1528.72
  60   11638394.7996      1.290  109983508.8163     1.190  1554.46
  61   10047431.2361      1.160  110771735.4286     1.026  1579.84
  62   10907793.0477      1.260  110722178.8163     1.174  1605.60
  63   10661178.4891      1.330  109599878.6939     1.164  1631.00
  64   5231680.4406      1.370  110208767.3878     1.070  1656.56
  65   9873383.3223      1.200  110620072.7755     1.058  1682.17
  66   8780236.6271      1.340  110182393.5918     1.096  1707.85
  67   8569437.3221      1.290  109900212.7755     1.108  1733.14
  68   15227874.1059      1.260  111134493.5510     1.134  1758.50
  69   8848293.5768      1.230  110494049.4694     1.168  1784.04
  70   6187338.4980      1.340  110241880.7347     1.110  1809.82
  71   7156721.9713      1.330  110353958.7347     1.114  1835.17
  72   3656508.4839      1.240  110817670.4898     1.122  1860.91
  73   8262996.7727      1.400  110761526.2449     1.152  1886.43
  74   9753497.3410      1.300  110615238.2857     1.142  1912.67
  75   8777485.0273      1.250  110108387.5510     1.102  1938.29
  76   9420872.3705      1.200  110364879.0612     1.196  1963.85
  77   8801427.7150      1.310  110174406.5714     1.098  1989.89
  78   3810815.2667      1.340  109888516.2041     1.028  2015.73
  79   4050538.3818      1.420  110661042.6122     1.108  2041.51
  80   5343565.5043      1.350  109985307.8776     1.106  2066.84
  81   5390658.3088      1.440  110317811.7143     1.120  2092.37
  82   11668542.9215      1.080  110367387.8367     1.092  2118.08
  83   10309922.7900      1.350  110294884.1633     1.118  2143.31
  84   9264917.2029      1.370  110258121.7551     1.102  2168.51
  85   12320163.7264      1.260  110494075.3061     1.084  2194.24
