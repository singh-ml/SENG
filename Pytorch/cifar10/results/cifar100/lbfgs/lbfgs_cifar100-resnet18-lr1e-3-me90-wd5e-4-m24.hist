Use GPU: 2 for training
==> Running with ['main_lbfgs.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '-m', '24', '--trainset', 'cifar100', '--datadir', '/data/singh/data/', '--lr', '1e-3', '--weight-decay', '5e-4', '--lr-scheme', 'cosine', '--gpu', '2']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 4239565312 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   4.7000      0.790  10.9689     0.818  24.14
   2   4.6991      0.770  10.9686     0.796  45.69
   3   4.6990      0.780  10.9698     0.852  67.08
   4   4.6990      0.730  10.9698     0.834  88.67
   5   4.7005      0.800  10.9695     0.838  110.18
   6   4.6997      0.790  10.9694     0.868  131.81
   7   4.6980      0.840  10.9688     0.822  153.33
   8   4.6992      0.760  10.9693     0.834  175.03
   9   4.6985      0.780  10.9694     0.842  196.70
  10   4.6994      0.790  10.9707     0.848  218.23
  11   4.7001      0.780  10.9691     0.830  240.00
  12   4.6981      0.810  10.9696     0.820  261.54
  13   4.6967      0.820  10.9697     0.864  283.03
  14   4.6988      0.840  10.9699     0.852  304.58
  15   4.6976      0.800  10.9696     0.888  326.26
  16   4.6983      0.780  10.9705     0.818  347.70
  17   4.6982      0.760  10.9696     0.862  369.39
  18   4.6991      0.830  10.9702     0.798  391.11
  19   4.6983      0.840  10.9698     0.878  412.76
  20   4.6977      0.810  10.9706     0.850  434.31
  21   4.6981      0.810  10.9688     0.836  456.16
  22   4.6992      0.790  10.9695     0.772  477.78
  23   4.6997      0.810  10.9701     0.836  499.36
  24   4.6992      0.760  10.9680     0.890  520.99
  25   4.6986      0.800  10.9691     0.792  542.53
  26   4.6997      0.790  10.9693     0.818  564.17
  27   4.6971      0.800  10.9697     0.874  585.70
  28   4.6970      0.800  10.9703     0.798  607.23
  29   4.6974      0.810  10.9687     0.902  628.83
  30   4.6980      0.820  10.9687     0.814  650.42
  31   4.6979      0.820  10.9700     0.846  671.96
  32   4.6985      0.830  10.9696     0.880  693.56
  33   4.6997      0.760  10.9700     0.828  715.11
  34   4.6998      0.780  10.9697     0.864  736.72
  35   4.6975      0.810  10.9697     0.792  758.32
  36   4.6987      0.760  10.9700     0.856  779.82
  37   4.6984      0.800  10.9707     0.842  801.37
  38   4.6995      0.760  10.9705     0.848  823.06
  39   4.6977      0.750  10.9700     0.770  844.73
  40   4.7009      0.780  10.9697     0.856  866.33
  41   4.6984      0.790  10.9688     0.856  887.98
  42   4.6974      0.830  10.9685     0.878  909.64
  43   4.6995      0.800  10.9706     0.830  931.18
  44   4.6979      0.810  10.9703     0.802  952.86
  45   4.6976      0.800  10.9699     0.804  974.55
  46   4.6979      0.840  10.9688     0.800  996.09
  47   4.6975      0.820  10.9704     0.824  1017.70
  48   4.6996      0.800  10.9701     0.886  1039.12
  49   4.7005      0.750  10.9684     0.846  1060.71
  50   4.6984      0.810  10.9696     0.824  1082.37
  51   4.6990      0.780  10.9697     0.834  1104.01
  52   4.6988      0.780  10.9689     0.862  1125.53
  53   4.6978      0.820  10.9686     0.858  1147.12
  54   4.7007      0.800  10.9702     0.820  1168.63
  55   4.7003      0.750  10.9691     0.824  1190.21
  56   4.6976      0.810  10.9699     0.812  1211.77
  57   4.6979      0.810  10.9703     0.814  1233.42
  58   4.6995      0.810  10.9677     0.846  1255.12
  59   4.6986      0.800  10.9700     0.916  1276.87
  60   4.6983      0.820  10.9700     0.888  1298.56
  61   4.6977      0.800  10.9692     0.890  1320.06
  62   4.6997      0.770  10.9697     0.852  1341.72
  63   4.6985      0.780  10.9682     0.866  1363.30
  64   4.6986      0.800  10.9702     0.838  1385.03
  65   4.6985      0.790  10.9698     0.898  1406.71
  66   4.6979      0.810  10.9687     0.824  1428.56
  67   4.6982      0.820  10.9691     0.874  1450.10
  68   4.6987      0.860  10.9700     0.816  1471.62
  69   4.6984      0.850  10.9693     0.814  1493.14
  70   4.6983      0.820  10.9699     0.830  1514.67
  71   4.6994      0.730  10.9685     0.874  1536.22
  72   4.7000      0.750  10.9691     0.882  1557.79
  73   4.7000      0.770  10.9708     0.858  1579.52
  74   4.6984      0.810  10.9701     0.858  1601.12
  75   4.6996      0.860  10.9693     0.872  1622.75
  76   4.6996      0.820  10.9698     0.832  1644.31
  77   4.6984      0.820  10.9690     0.846  1665.88
  78   4.6997      0.810  10.9693     0.856  1687.42
  79   4.6981      0.810  10.9689     0.864  1708.95
  80   4.6982      0.750  10.9691     0.792  1730.50
  81   4.6994      0.820  10.9706     0.850  1752.02
  82   4.6988      0.780  10.9697     0.822  1773.74
  83   4.6993      0.760  10.9685     0.854  1795.31
  84   4.6968      0.790  10.9684     0.828  1816.86
  85   4.6994      0.760  10.9699     0.858  1838.51
  86   4.6987      0.790  10.9696     0.880  1860.13
  87   4.6976      0.850  10.9694     0.886  1881.67
  88   4.6995      0.790  10.9699     0.798  1903.22
  89   4.6971      0.710  10.9701     0.816  1924.74
  90   4.6977      0.810  10.9691     0.848  1946.41
