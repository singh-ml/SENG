Use GPU: 1 for training
==> Running with ['main_nsgd.py', '--epoch', '85', '--arch', 'resnet18', '--lr-decay-epoch', '85', '--bh', '32', '--irho', '2', '--trainset', 'cifar100', '--datadir', '/data/singh/data/', '--lr', '5e-3', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '1']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6351025152 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   3.7641     12.030  4.0353     8.216  71.50
   2   3.2584     20.600  3.4469    17.108  139.65
   3   3.0661     24.420  3.1144    23.118  207.65
   4   2.9243     28.090  2.8383    28.184  275.69
   5   2.5919     33.290  2.5948    33.208  343.63
   6   2.4783     36.780  2.3747    37.478  411.65
   7   2.2315     41.930  2.1971    41.488  479.71
   8   2.1515     43.730  2.0354    45.028  547.72
   9   2.1586     43.490  1.8954    48.138  615.56
  10   2.0200     47.710  1.7781    51.214  683.63
  11   1.9997     48.080  1.6716    53.476  751.60
  12   1.8742     50.230  1.5696    56.012  819.45
  13   1.8150     51.010  1.4891    57.916  887.42
  14   1.7311     53.440  1.4084    59.800  955.40
  15   1.8161     52.390  1.3338    61.798  1023.23
  16   1.7904     53.370  1.2642    63.682  1091.12
  17   1.6257     56.320  1.2019    65.098  1159.02
  18   1.5917     56.830  1.1397    66.696  1227.02
  19   1.6520     56.730  1.0830    68.464  1294.96
  20   1.5430     58.480  1.0276    69.780  1362.93
  21   1.5395     58.850  0.9759    71.000  1430.80
  22   1.5581     59.870  0.9320    72.372  1498.75
  23   1.5414     59.330  0.8800    73.678  1566.73
  24   1.5214     60.360  0.8367    74.900  1634.68
  25   1.5330     60.390  0.7907    76.390  1702.66
  26   1.6025     59.580  0.7460    77.586  1770.61
  27   1.4688     62.240  0.7056    78.684  1838.57
  28   1.5731     60.580  0.6647    79.782  1906.53
  29   1.5395     61.410  0.6278    80.754  1974.41
  30   1.4626     62.620  0.5905    82.100  2042.33
  31   1.4769     62.760  0.5530    82.954  2110.33
  32   1.5294     62.810  0.5255    83.948  2178.26
  33   1.5262     62.640  0.4898    85.014  2246.16
  34   1.5560     62.560  0.4557    86.048  2314.09
  35   1.5000     63.430  0.4227    87.130  2382.03
  36   1.5211     64.130  0.3924    88.204  2449.93
  37   1.5504     63.900  0.3666    88.878  2517.83
  38   1.5781     63.470  0.3485    89.440  2585.74
  39   1.5402     64.160  0.3104    90.714  2653.62
  40   1.6291     63.160  0.2844    91.544  2721.53
  41   1.5986     64.330  0.2650    92.396  2789.44
  42   1.5882     64.720  0.2482    92.792  2857.33
  43   1.5896     64.840  0.2268    93.600  2925.29
  44   1.5836     64.840  0.2093    94.130  2993.15
  45   1.6059     64.950  0.1897    94.868  3061.27
  46   1.5543     65.570  0.1741    95.516  3129.26
  47   1.5789     65.260  0.1613    95.882  3197.24
  48   1.6177     64.910  0.1530    96.070  3265.05
  49   1.5961     65.650  0.1396    96.636  3332.97
  50   1.5825     65.940  0.1281    96.906  3400.86
  51   1.6547     65.580  0.1170    97.328  3468.82
  52   1.5839     66.430  0.1068    97.618  3536.83
  53   1.6262     66.310  0.0982    97.964  3604.82
  54   1.6256     66.070  0.0929    98.010  3672.85
  55   1.6155     65.890  0.0842    98.440  3740.82
  56   1.6082     66.640  0.0798    98.520  3808.85
  57   1.6000     66.360  0.0753    98.558  3876.93
  58   1.6159     66.700  0.0699    98.732  3944.90
  59   1.6252     66.700  0.0646    98.860  4012.86
  60   1.6132     66.970  0.0618    98.992  4080.82
