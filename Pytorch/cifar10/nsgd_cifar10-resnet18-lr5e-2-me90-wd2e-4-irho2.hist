Use GPU: 2 for training
==> Running with ['main_nsgd.py', '--epoch', '90', '--arch', 'resnet18', '--lr-decay-epoch', '90', '--bh', '32', '--irho', '2', '--trainset', 'cifar10', '--datadir', '/data/singh/data/', '--lr', '5e-2', '--weight-decay', '2e-4', '--lr-scheme', 'cosine', '--gpu', '2']
==> Building model..
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
Memory peak: 6339986432 Bytes
Epoch  testloss  testacc  trainloss  trainacc  time
   1   1.5675     42.620  2.0148    29.706  71.22
   2   1.3579     51.640  1.4184    47.848  139.10
   3   0.9996     64.490  1.1487    58.456  206.95
   4   0.9644     66.070  0.9469    66.516  274.80
   5   0.8385     70.390  0.8059    71.580  342.64
   6   0.7055     76.060  0.6910    75.704  410.58
   7   0.6845     76.730  0.6033    79.108  478.63
   8   0.5788     80.130  0.5340    81.326  546.42
   9   0.5597     81.040  0.4825    83.148  614.30
  10   0.5662     81.570  0.4331    84.900  682.05
  11   0.5166     81.950  0.4051    85.970  749.87
  12   0.6078     80.430  0.3754    86.930  817.87
  13   0.5033     83.270  0.3387    88.260  885.71
  14   0.4719     84.250  0.3172    88.986  953.45
  15   0.4395     85.740  0.2943    89.822  1021.24
  16   0.4362     86.260  0.2792    90.260  1089.09
  17   0.4936     84.170  0.2560    90.988  1157.01
  18   0.3951     87.390  0.2436    91.394  1224.84
  19   0.3862     87.550  0.2244    92.174  1292.63
  20   0.4544     86.420  0.2114    92.676  1360.50
  21   0.3681     88.580  0.1941    93.140  1428.36
  22   0.3651     89.080  0.1911    93.258  1496.25
  23   0.3514     89.650  0.1734    93.952  1564.16
  24   0.3986     87.810  0.1604    94.266  1632.04
  25   0.3651     89.100  0.1541    94.566  1699.90
  26   0.3375     89.520  0.1497    94.736  1767.81
  27   0.3740     89.100  0.1361    95.112  1835.62
  28   0.3608     89.960  0.1279    95.468  1903.53
  29   0.3847     89.040  0.1256    95.518  1971.42
  30   0.3563     89.410  0.1088    96.130  2039.25
  31   0.4166     89.170  0.1048    96.338  2107.18
  32   0.3802     89.510  0.0975    96.592  2175.21
  33   0.3482     90.100  0.0943    96.750  2243.12
  34   0.3646     89.700  0.0876    96.820  2311.12
  35   0.4029     89.230  0.0814    97.174  2379.09
  36   0.3346     91.040  0.0790    97.314  2446.96
  37   0.3678     90.540  0.0734    97.442  2514.84
  38   0.3446     91.190  0.0672    97.644  2582.78
  39   0.3555     91.170  0.0635    97.752  2650.70
  40   0.4447     89.470  0.0565    98.078  2718.56
  41   0.3753     90.650  0.0595    97.934  2786.45
  42   0.3754     90.650  0.0532    98.162  2854.33
  43   0.3378     91.430  0.0474    98.272  2922.17
  44   0.3543     91.280  0.0397    98.604  2990.11
  45   0.3576     91.290  0.0343    98.838  3058.91
  46   0.3681     91.140  0.0339    98.838  3126.94
  47   0.3519     91.700  0.0283    99.016  3196.04
  48   0.3466     91.720  0.0299    99.004  3264.08
  49   0.3400     91.910  0.0260    99.130  3333.44
  50   0.3472     91.930  0.0255    99.124  3401.37
  51   0.3490     92.230  0.0208    99.350  3469.41
  52   0.3462     92.430  0.0178    99.448  3550.39
  53   0.3655     92.050  0.0163    99.458  3649.25
